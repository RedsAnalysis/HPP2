{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d33a4648-2943-40a8-8af7-ae50570d8d81",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Libraries imported successfully.\n",
      "Raw data loaded successfully.\n",
      "Setup complete.\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# BLOCK 1: SETUP, IMPORTS, AND DATA LOADING\n",
    "# =============================================================================\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "import time\n",
    "import os\n",
    "# --- Library Imports ---\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import gc\n",
    "from sklearn.model_selection import StratifiedKFold, train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import xgboost as xgb\n",
    "import catboost as cb\n",
    "import optuna\n",
    "from scipy.optimize import minimize\n",
    "print(\"Libraries imported successfully.\")\n",
    "# --- Helper Function for Winkler Score ---\n",
    "def winkler_score(y_true, lower, upper, alpha=0.1, return_coverage=False):\n",
    "    width = upper - lower\n",
    "    penalty_lower = np.where(y_true < lower, (2 / alpha) * (lower - y_true), 0)\n",
    "    penalty_upper = np.where(y_true > upper, (2 / alpha) * (y_true - upper), 0)\n",
    "    score = width + penalty_lower + penalty_upper\n",
    "    if return_coverage:\n",
    "        coverage = np.mean((y_true >= lower) & (y_true <= upper))\n",
    "        return np.mean(score), coverage\n",
    "    return np.mean(score)\n",
    "# --- Global Constants ---\n",
    "N_SPLITS = 5\n",
    "RANDOM_STATE = 42\n",
    "DATA_PATH = './'\n",
    "N_OPTUNA_TRIALS = 30 # A strong number for a comprehensive search\n",
    "COMPETITION_ALPHA = 0.1\n",
    "\n",
    "# --- Load Raw Data ---\n",
    "try:\n",
    "    # We drop the low-variance columns they identified right away\n",
    "    drop_cols=['id', 'golf', 'view_rainier', 'view_skyline', 'view_lakesamm','view_otherwater', 'view_other']\n",
    "    df_train = pd.read_csv(DATA_PATH + 'dataset.csv').drop(columns=drop_cols)\n",
    "    df_test = pd.read_csv(DATA_PATH + 'test.csv').drop(columns=drop_cols)\n",
    "    print(\"Raw data loaded successfully.\")\n",
    "except FileNotFoundError:\n",
    "    print(\"ERROR: Could not find 'dataset.csv' or 'test.csv'.\")\n",
    "    exit()\n",
    "# --- Prepare Target Variable ---\n",
    "y_true = df_train['sale_price'].copy()\n",
    "grade_for_stratify = df_train['grade'].copy()\n",
    "# The mean-error model works best when predicting the raw price directly\n",
    "# So, we will NOT log-transform the target this time.\n",
    "# df_train.drop('sale_price', axis=1, inplace=True) # We keep sale_price for FE\n",
    "print(\"Setup complete.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3a94b99e-758e-4790-9a36-e704094fb922",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Starting Block 2.5: Executing Feature Engineering Pipeline ---\n",
      "--- Starting Comprehensive Feature Engineering ---\n",
      "Step 1: Creating brute-force numerical interaction features...\n",
      "Step 2: Creating date features...\n",
      "Step 3: Creating TF-IDF features for text columns...\n",
      "Step 4: Creating group-by aggregation features...\n",
      "Step 5: Creating ratio features...\n",
      "Step 6: Creating geospatial clustering features...\n",
      "Step 7: Finalizing feature set...\n",
      "\n",
      "Comprehensive FE complete. Total features: 233\n",
      "Feature engineering complete. X shape: (200000, 233), X_test shape: (200000, 233)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Make sure to have these libraries installed\n",
    "# pip install pandas numpy scikit-learn\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.cluster import KMeans\n",
    "import gc\n",
    "\n",
    "# Define a random state for reproducibility\n",
    "RANDOM_STATE = 42\n",
    "\n",
    "def create_comprehensive_features(df_train, df_test):\n",
    "    \"\"\"\n",
    "    Combines original and new advanced feature engineering steps into a single pipeline.\n",
    "    \"\"\"\n",
    "    print(\"--- Starting Comprehensive Feature Engineering ---\")\n",
    "\n",
    "    # Store original indices and target variable\n",
    "    train_ids = df_train.index\n",
    "    test_ids = df_test.index\n",
    "    y_train = df_train['sale_price'].copy() # Keep the target separate\n",
    "\n",
    "    # Combine for consistent processing\n",
    "    df_train_temp = df_train.drop(columns=['sale_price'])\n",
    "    all_data = pd.concat([df_train_temp, df_test], axis=0, ignore_index=True)\n",
    "\n",
    "    # --- Original Feature Engineering ---\n",
    "\n",
    "    # A) Brute-Force Numerical Interactions\n",
    "    print(\"Step 1: Creating brute-force numerical interaction features...\")\n",
    "    NUMS = ['area', 'land_val', 'imp_val', 'sqft_lot', 'sqft', 'sqft_1', 'grade', 'year_built']\n",
    "    # Ensure all columns exist and are numeric, fill missing with 0 for safety\n",
    "    for col in NUMS:\n",
    "        if col not in all_data.columns:\n",
    "            all_data[col] = 0\n",
    "        else:\n",
    "            all_data[col] = pd.to_numeric(all_data[col], errors='coerce').fillna(0)\n",
    "            \n",
    "    for i in range(len(NUMS)):\n",
    "        for j in range(i + 1, len(NUMS)):\n",
    "            all_data[f'{NUMS[i]}_x_{NUMS[j]}'] = all_data[NUMS[i]] * all_data[NUMS[j]]\n",
    "\n",
    "    # B) Date Features\n",
    "    print(\"Step 2: Creating date features...\")\n",
    "    all_data['sale_date'] = pd.to_datetime(all_data['sale_date'])\n",
    "    all_data['sale_year'] = all_data['sale_date'].dt.year\n",
    "    all_data['sale_month'] = all_data['sale_date'].dt.month\n",
    "    all_data['sale_dayofyear'] = all_data['sale_date'].dt.dayofyear\n",
    "    all_data['age_at_sale'] = all_data['sale_year'] - all_data['year_built']\n",
    "\n",
    "    # C) TF-IDF Text Features\n",
    "    print(\"Step 3: Creating TF-IDF features for text columns...\")\n",
    "    text_cols = ['subdivision', 'zoning', 'city', 'sale_warning', 'join_status', 'submarket']\n",
    "    all_data[text_cols] = all_data[text_cols].fillna('missing').astype(str)\n",
    "    \n",
    "    for col in text_cols:\n",
    "        tfidf = TfidfVectorizer(analyzer='char', ngram_range=(3, 5), max_features=128, binary=True)\n",
    "        svd = TruncatedSVD(n_components=8, random_state=RANDOM_STATE)\n",
    "        \n",
    "        tfidf_matrix = tfidf.fit_transform(all_data[col])\n",
    "        tfidf_svd = svd.fit_transform(tfidf_matrix)\n",
    "        \n",
    "        tfidf_df = pd.DataFrame(tfidf_svd, columns=[f'{col}_tfidf_svd_{i}' for i in range(8)])\n",
    "        all_data = pd.concat([all_data, tfidf_df], axis=1)\n",
    "\n",
    "    # D) Log transform some interaction features\n",
    "    for c in ['land_val_x_imp_val', 'land_val_x_sqft', 'imp_val_x_sqft']:\n",
    "        if c in all_data.columns:\n",
    "            all_data[c] = np.log1p(all_data[c].fillna(0))\n",
    "\n",
    "    # --- New Feature Engineering Ideas ---\n",
    "\n",
    "    # F) Group-By Aggregation Features\n",
    "    print(\"Step 4: Creating group-by aggregation features...\")\n",
    "    group_cols = ['submarket', 'city', 'zoning']\n",
    "    num_cols_for_agg = ['grade', 'sqft', 'imp_val', 'land_val', 'age_at_sale']\n",
    "\n",
    "    for group_col in group_cols:\n",
    "        for num_col in num_cols_for_agg:\n",
    "            agg_stats = all_data.groupby(group_col)[num_col].agg(['mean', 'std', 'max', 'min']).reset_index()\n",
    "            agg_stats.columns = [group_col] + [f'{group_col}_{num_col}_{stat}' for stat in ['mean', 'std', 'max', 'min']]\n",
    "            all_data = pd.merge(all_data, agg_stats, on=group_col, how='left')\n",
    "            all_data[f'{num_col}_minus_{group_col}_mean'] = all_data[num_col] - all_data[f'{group_col}_{num_col}_mean']\n",
    "\n",
    "    # G) Ratio Features\n",
    "    print(\"Step 5: Creating ratio features...\")\n",
    "    # Add a small epsilon to prevent division by zero\n",
    "    epsilon = 1e-6 \n",
    "    all_data['total_val'] = all_data['imp_val'] + all_data['land_val']\n",
    "    all_data['imp_val_to_land_val_ratio'] = all_data['imp_val'] / (all_data['land_val'] + epsilon)\n",
    "    all_data['land_val_ratio'] = all_data['land_val'] / (all_data['total_val'] + epsilon)\n",
    "    all_data['sqft_to_lot_ratio'] = all_data['sqft'] / (all_data['sqft_lot'] + epsilon)\n",
    "    all_data['was_renovated'] = (all_data['year_reno'] > 0).astype(int)\n",
    "    all_data['reno_age_at_sale'] = np.where(all_data['was_renovated'] == 1, all_data['sale_year'] - all_data['year_reno'], -1)\n",
    "\n",
    "    # H) Geospatial Clustering Features\n",
    "    print(\"Step 6: Creating geospatial clustering features...\")\n",
    "    coords = all_data[['latitude', 'longitude']].copy()\n",
    "    coords.fillna(coords.median(), inplace=True) # Simple imputation\n",
    "\n",
    "    # KMeans is sensitive to feature scaling, but for lat/lon it's often okay without it.\n",
    "    kmeans = KMeans(n_clusters=20, random_state=RANDOM_STATE, n_init=10) \n",
    "    all_data['location_cluster'] = kmeans.fit_predict(coords)\n",
    "    \n",
    "    # Calculate distance to each cluster center\n",
    "    cluster_centers = kmeans.cluster_centers_\n",
    "    for i in range(len(cluster_centers)):\n",
    "        center = cluster_centers[i]\n",
    "        all_data[f'dist_to_cluster_{i}'] = np.sqrt((coords['latitude'] - center[0])**2 + (coords['longitude'] - center[1])**2)\n",
    "\n",
    "    # --- Final Cleanup ---\n",
    "    print(\"Step 7: Finalizing feature set...\")\n",
    "    cols_to_drop = ['sale_date', 'subdivision', 'zoning', 'city', 'sale_warning', 'join_status', 'submarket']\n",
    "    all_data = all_data.drop(columns=cols_to_drop)\n",
    "\n",
    "    # One-hot encode the new cluster feature\n",
    "    all_data = pd.get_dummies(all_data, columns=['location_cluster'], prefix='loc_cluster')\n",
    "    \n",
    "    # Final check for any remaining object columns to be safe (besides index)\n",
    "    object_cols = all_data.select_dtypes(include='object').columns\n",
    "    if len(object_cols) > 0:\n",
    "        print(f\"Warning: Found unexpected object columns: {object_cols}. Dropping them.\")\n",
    "        all_data = all_data.drop(columns=object_cols)\n",
    "        \n",
    "    all_data.fillna(0, inplace=True)\n",
    "\n",
    "    # Separate back into train and test sets\n",
    "    train_len = len(train_ids)\n",
    "    X = all_data.iloc[:train_len].copy()\n",
    "    X_test = all_data.iloc[train_len:].copy()\n",
    "    \n",
    "    # Restore original indices\n",
    "    X.index = train_ids\n",
    "    X_test.index = test_ids\n",
    "    \n",
    "    # Align columns - crucial for model prediction\n",
    "    X_test = X_test[X.columns]\n",
    "    \n",
    "    print(f\"\\nComprehensive FE complete. Total features: {X.shape[1]}\")\n",
    "    gc.collect()\n",
    "    \n",
    "    return X, X_test, y_train\n",
    "# =============================================================================\n",
    "# BLOCK 2.5: EXECUTE FEATURE ENGINEERING\n",
    "# =============================================================================\n",
    "print(\"\\n--- Starting Block 2.5: Executing Feature Engineering Pipeline ---\")\n",
    "\n",
    "# This is the crucial step that was missing.\n",
    "# We call the function to create our training and testing dataframes.\n",
    "X, X_test, y_train = create_comprehensive_features(df_train, df_test)\n",
    "\n",
    "# Let's verify the output\n",
    "print(f\"Feature engineering complete. X shape: {X.shape}, X_test shape: {X_test.shape}\")\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1fc25c52-2f59-4ab9-9878-c98a2b97e3de",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Loading all base model predictions from saved .npy files... ---\n",
      "All MEAN AND ERROR models predictions loaded successfully.\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# BLOCK 3: LOAD ALL PRE-TRAINED MODEL PREDICTIONS\n",
    "# =============================================================================\n",
    "\n",
    "# Define paths to your saved prediction files\n",
    "PREDS_SAVE_PATH = './mean_models_v1/' # For XGB and CatBoost preds\n",
    "NN_PREDS_PATH = './NN_model_predictions/' # For NN preds\n",
    "ERR_PATH = './error_models/' # For error preds\n",
    "\n",
    "print(\"--- Loading all base model predictions from saved .npy files... ---\")\n",
    "try:\n",
    "    # Load Mean Model OOF (Out-of-Fold) Predictions\n",
    "    oof_xgb_preds = np.load(os.path.join(PREDS_SAVE_PATH, 'oof_xgb_preds.npy'))\n",
    "    oof_cb_preds = np.load(os.path.join(PREDS_SAVE_PATH, 'oof_cb_preds.npy'))\n",
    "    oof_lgbm_preds = np.load(os.path.join(PREDS_SAVE_PATH, 'oof_lgbm_preds.npy'))\n",
    "    oof_nn_preds = np.load(os.path.join(NN_PREDS_PATH, 'oof_nn_preds.npy'))\n",
    "\n",
    "    oof_error_preds_cb = np.load(os.path.join(ERR_PATH, 'oof_error_preds_cb.npy'))\n",
    "    oof_error_preds_lgbm = np.load(os.path.join(ERR_PATH, 'oof_error_preds_lgbm.npy'))\n",
    "    oof_error_preds_xgb = np.load(os.path.join(ERR_PATH, 'oof_error_preds_xgb.npy'))\n",
    "    \n",
    "    # Load Mean Model Test Predictions\n",
    "    test_xgb_preds = np.load(os.path.join(PREDS_SAVE_PATH, 'test_xgb_preds.npy'))\n",
    "    test_cb_preds = np.load(os.path.join(PREDS_SAVE_PATH, 'test_cb_preds.npy'))\n",
    "    test_lgbm_preds = np.load(os.path.join(PREDS_SAVE_PATH, 'test_lgbm_preds.npy'))\n",
    "    test_nn_preds = np.load(os.path.join(NN_PREDS_PATH, 'test_nn_preds.npy'))\n",
    "\n",
    "    test_error_preds_cb = np.load(os.path.join(ERR_PATH, 'test_error_preds_cb.npy'))\n",
    "    test_error_preds_lgbm = np.load(os.path.join(ERR_PATH, 'test_error_preds_lgbm.npy'))\n",
    "    test_error_preds_xgb = np.load(os.path.join(ERR_PATH, 'test_error_preds_xgb.npy'))\n",
    "\n",
    "     \n",
    "    \n",
    "    print(\"All MEAN AND ERROR models predictions loaded successfully.\")\n",
    "    \n",
    "except FileNotFoundError as e:\n",
    "    print(f\"\\nERROR: Could not find a required prediction file. {e}\")\n",
    "    print(\"Please ensure you have run all training notebooks and saved their predictions first.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e8c81f2b-cce5-4d7b-b3a4-f09551d44b2d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Training a simple model to determine feature importance... ---\n",
      "Feature importance ranking created.\n",
      "\n",
      "--- Building an ELITE feature set with reduced features and stacked predictions ---\n",
      "Selected the top 25 raw features to reduce noise.\n",
      "\n",
      "--- Engineering and adding VOLATILITY features (leakage-proof method) ---\n",
      "\n",
      "Elite feature set for quantile models created successfully.\n",
      "Final Shape: (200000, 34)\n",
      "Total features include: 25 raw + 7 stacked preds + 2 volatility features = 34\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1243"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# BLOCK 5.5 (Corrected): ELITE FEATURE SET WITH VOLATILITY FEATURES\n",
    "# =============================================================================\n",
    "# This block creates the definitive feature set for the quantile models.\n",
    "#\n",
    "# NEW IMPROVEMENT: We are now engineering features specifically designed to\n",
    "# capture price VOLATILITY. This gives the quantile models a direct signal\n",
    "# about which groups of houses have a wider or narrower price distribution,\n",
    "# which is exactly what they need to predict the tails accurately.\n",
    "#\n",
    "# We also drastically reduce the number of raw features to N=25 to combat\n",
    "# overfitting, forcing the model to rely on these powerful new signals.\n",
    "\n",
    "# --- Step 1: Generate Feature Importance Ranking (Same as before) ---\n",
    "print(\"\\n--- Training a simple model to determine feature importance... ---\")\n",
    "dtrain_importance = xgb.DMatrix(X, label=y_true)\n",
    "params = {'objective': 'reg:squarederror', 'eval_metric': 'rmse', 'seed': RANDOM_STATE, 'n_jobs': -1}\n",
    "bst_for_importance = xgb.train(params, dtrain_importance, num_boost_round=500, verbose_eval=False)\n",
    "importance_scores = bst_for_importance.get_score(importance_type='gain')\n",
    "feature_importance = pd.DataFrame({\n",
    "    'Feature': importance_scores.keys(),\n",
    "    'Importance': importance_scores.values()\n",
    "}).sort_values(by='Importance', ascending=False).reset_index(drop=True)\n",
    "print(\"Feature importance ranking created.\")\n",
    "\n",
    "# --- Step 2: Build the ELITE Feature Set ---\n",
    "print(\"\\n--- Building an ELITE feature set with reduced features and stacked predictions ---\")\n",
    "\n",
    "# Drastically reduce feature count to combat overfitting.\n",
    "N_TOP_FEATURES = 25\n",
    "elite_raw_features = feature_importance['Feature'].head(N_TOP_FEATURES).tolist()\n",
    "print(f\"Selected the top {N_TOP_FEATURES} raw features to reduce noise.\")\n",
    "\n",
    "# Create the base dataframes\n",
    "X_for_quantile = X[elite_raw_features].copy()\n",
    "X_test_for_quantile = X_test[elite_raw_features].copy()\n",
    "\n",
    "# Add the stacked predictions (mean and error models)\n",
    "# These are still the most powerful features.\n",
    "# (Code to add oof_mean_*, oof_error_*, test_mean_*, test_error_* preds)\n",
    "for pred_name, oof_pred, test_pred in [\n",
    "    ('oof_mean_xgb', oof_xgb_preds, test_xgb_preds), ('oof_mean_cb', oof_cb_preds, test_cb_preds),\n",
    "    ('oof_mean_lgbm', oof_lgbm_preds, test_lgbm_preds), ('oof_mean_nn', oof_nn_preds, test_nn_preds),\n",
    "    ('oof_error_xgb', oof_error_preds_xgb, test_error_preds_xgb), ('oof_error_cb', oof_error_preds_cb, test_error_preds_cb),\n",
    "    ('oof_error_lgbm', oof_error_preds_lgbm, test_error_preds_lgbm)\n",
    "]:\n",
    "    X_for_quantile[pred_name] = oof_pred\n",
    "    X_test_for_quantile[pred_name] = test_pred\n",
    "\n",
    "# --- Step 3: Create and Add Volatility Features (The Leakage-Proof Way) ---\n",
    "# This is the key improvement. We must create these features inside a CV loop.\n",
    "print(\"\\n--- Engineering and adding VOLATILITY features (leakage-proof method) ---\")\n",
    "\n",
    "# We will create OOF features for the train set and a single set for the test set.\n",
    "# Initialize new feature columns with NaNs\n",
    "X_for_quantile['price_std_by_submarket'] = np.nan\n",
    "X_for_quantile['price_range_by_grade'] = np.nan\n",
    "X_test_for_quantile['price_std_by_submarket'] = np.nan\n",
    "X_test_for_quantile['price_range_by_grade'] = np.nan\n",
    "\n",
    "# We will average the test set calculations over the folds\n",
    "test_std_agg = np.zeros(len(X_test_for_quantile))\n",
    "test_range_agg = np.zeros(len(X_test_for_quantile))\n",
    "\n",
    "skf = StratifiedKFold(n_splits=N_SPLITS, shuffle=True, random_state=RANDOM_STATE)\n",
    "\n",
    "for fold, (train_idx, val_idx) in enumerate(skf.split(X, grade_for_stratify)):\n",
    "    # Get the training data for THIS FOLD ONLY\n",
    "    X_train_fold, y_train_fold = df_train.iloc[train_idx], y_true.iloc[train_idx]\n",
    "    \n",
    "    # --- Feature 1: Price Standard Deviation by Submarket ---\n",
    "    # Calculate the std only on the fold's training data\n",
    "    std_map = y_train_fold.groupby(X_train_fold['submarket']).std()\n",
    "    # Map these values to the validation set for this fold\n",
    "    val_stds = df_train.iloc[val_idx]['submarket'].map(std_map)\n",
    "    X_for_quantile.loc[X_for_quantile.index[val_idx], 'price_std_by_submarket'] = val_stds\n",
    "    # Map to the test set and add to the aggregate\n",
    "    test_std_agg += df_test['submarket'].map(std_map) / N_SPLITS\n",
    "\n",
    "    # --- Feature 2: Price Range by Grade ---\n",
    "    # Calculate the range (max-min) only on the fold's training data\n",
    "    range_map = y_train_fold.groupby(X_train_fold['grade']).apply(lambda x: x.max() - x.min())\n",
    "    # Map these values to the validation set for this fold\n",
    "    val_ranges = df_train.iloc[val_idx]['grade'].map(range_map)\n",
    "    X_for_quantile.loc[X_for_quantile.index[val_idx], 'price_range_by_grade'] = val_ranges\n",
    "    # Map to the test set and add to the aggregate\n",
    "    test_range_agg += df_test['grade'].map(range_map) / N_SPLITS\n",
    "\n",
    "# Assign the averaged features to the test set\n",
    "X_test_for_quantile['price_std_by_submarket'] = test_std_agg\n",
    "X_test_for_quantile['price_range_by_grade'] = test_range_agg\n",
    "\n",
    "# --- Step 4: Final Cleanup and Report ---\n",
    "# Fill any NaNs that might have occurred if a category in val/test was not in train\n",
    "# (e.g., using the global median as a fallback)\n",
    "global_std_median = X_for_quantile['price_std_by_submarket'].median()\n",
    "global_range_median = X_for_quantile['price_range_by_grade'].median()\n",
    "\n",
    "X_for_quantile.fillna({\n",
    "    'price_std_by_submarket': global_std_median,\n",
    "    'price_range_by_grade': global_range_median\n",
    "}, inplace=True)\n",
    "X_test_for_quantile.fillna({\n",
    "    'price_std_by_submarket': global_std_median,\n",
    "    'price_range_by_grade': global_range_median\n",
    "}, inplace=True)\n",
    "\n",
    "# Final alignment check\n",
    "X_test_for_quantile = X_test_for_quantile[X_for_quantile.columns]\n",
    "\n",
    "total_features = X_for_quantile.shape[1]\n",
    "print(f\"\\nElite feature set for quantile models created successfully.\")\n",
    "print(f\"Final Shape: {X_for_quantile.shape}\")\n",
    "print(f\"Total features include: {N_TOP_FEATURES} raw + 7 stacked preds + 2 volatility features = {total_features}\")\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "038310b2-0d19-4196-a824-3fbd15d30b9f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-07-25 19:38:40,748] A new study created in memory with name: no-name-c02667e7-78c3-437c-8014-392048230280\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Preparing data for Optuna tuning ---\n",
      "Data split for tuning: 160000 train, 40000 validation samples.\n",
      "\n",
      "--- Tuning the XGBoost Lower-Bound Model (alpha=0.05)... ---\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2e32fdf528c5496dada5dce798db70e9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/50 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-07-25 19:38:48,639] Trial 0 finished with value: 6870.290654460782 and parameters: {'eta': 0.057970079583532425, 'max_depth': 6, 'subsample': 0.7577973532493475, 'colsample_bytree': 0.624271877564692, 'lambda': 83.14652712195401, 'alpha': 48.845046642256136, 'min_child_weight': 16}. Best is trial 0 with value: 6870.290654460782.\n",
      "[I 2025-07-25 19:38:54,532] Trial 1 finished with value: 6876.1039571235415 and parameters: {'eta': 0.024796089161968602, 'max_depth': 8, 'subsample': 0.6893402596863362, 'colsample_bytree': 0.6556373561581808, 'lambda': 0.5750763896207203, 'alpha': 0.11041057928325171, 'min_child_weight': 1}. Best is trial 0 with value: 6870.290654460782.\n",
      "[I 2025-07-25 19:38:59,731] Trial 2 finished with value: 6879.449708960122 and parameters: {'eta': 0.038716278735454024, 'max_depth': 8, 'subsample': 0.8908824932144612, 'colsample_bytree': 0.6231163179512852, 'lambda': 14.784432457891823, 'alpha': 2.83797347548028, 'min_child_weight': 2}. Best is trial 0 with value: 6870.290654460782.\n",
      "[I 2025-07-25 19:39:07,839] Trial 3 finished with value: 6835.294216948008 and parameters: {'eta': 0.05935802636657266, 'max_depth': 3, 'subsample': 0.6560793364000727, 'colsample_bytree': 0.7179214721096039, 'lambda': 0.015483299990147577, 'alpha': 1.0460616656283397, 'min_child_weight': 12}. Best is trial 3 with value: 6835.294216948008.\n",
      "[I 2025-07-25 19:39:14,192] Trial 4 finished with value: 6890.608742987092 and parameters: {'eta': 0.09518612766479265, 'max_depth': 7, 'subsample': 0.8055811655728671, 'colsample_bytree': 0.6983731100071058, 'lambda': 0.15617874435906184, 'alpha': 46.48232633725607, 'min_child_weight': 2}. Best is trial 3 with value: 6835.294216948008.\n",
      "[I 2025-07-25 19:39:22,168] Trial 5 finished with value: 6822.333165678152 and parameters: {'eta': 0.027107505162735857, 'max_depth': 4, 'subsample': 0.7047881425807698, 'colsample_bytree': 0.7681590794730471, 'lambda': 0.017538443293748544, 'alpha': 0.19519694287184736, 'min_child_weight': 13}. Best is trial 5 with value: 6822.333165678152.\n",
      "[I 2025-07-25 19:39:34,053] Trial 6 finished with value: 6829.393288586456 and parameters: {'eta': 0.017415095616369038, 'max_depth': 6, 'subsample': 0.8530081499632706, 'colsample_bytree': 0.6193278712624569, 'lambda': 1.092994877495344, 'alpha': 1.5126235456509742, 'min_child_weight': 20}. Best is trial 5 with value: 6822.333165678152.\n",
      "[I 2025-07-25 19:39:37,257] Trial 7 finished with value: 6870.530528609767 and parameters: {'eta': 0.059675569960153954, 'max_depth': 7, 'subsample': 0.7709833597669908, 'colsample_bytree': 0.6311452166904364, 'lambda': 0.11149104558712794, 'alpha': 4.0183608644003135, 'min_child_weight': 5}. Best is trial 5 with value: 6822.333165678152.\n",
      "[I 2025-07-25 19:39:44,304] Trial 8 finished with value: 6871.888766281074 and parameters: {'eta': 0.08320355098306201, 'max_depth': 3, 'subsample': 0.7546748405588773, 'colsample_bytree': 0.9098579132529286, 'lambda': 0.029886666137427575, 'alpha': 12.118792037708271, 'min_child_weight': 10}. Best is trial 5 with value: 6822.333165678152.\n",
      "[I 2025-07-25 19:39:55,868] Trial 9 finished with value: 6841.10004858953 and parameters: {'eta': 0.013307201557129033, 'max_depth': 7, 'subsample': 0.804285429468681, 'colsample_bytree': 0.6753625027546776, 'lambda': 46.234494367704315, 'alpha': 11.32871979046103, 'min_child_weight': 16}. Best is trial 5 with value: 6822.333165678152.\n",
      "[I 2025-07-25 19:40:05,503] Trial 10 finished with value: 6821.197900134258 and parameters: {'eta': 0.028584211954640112, 'max_depth': 4, 'subsample': 0.6073146079254123, 'colsample_bytree': 0.8191782956253383, 'lambda': 3.676090101162704, 'alpha': 0.023595928850853255, 'min_child_weight': 9}. Best is trial 10 with value: 6821.197900134258.\n",
      "[I 2025-07-25 19:40:12,437] Trial 11 finished with value: 6841.0682531515195 and parameters: {'eta': 0.028154958014745053, 'max_depth': 4, 'subsample': 0.6291524889324134, 'colsample_bytree': 0.8276513100514068, 'lambda': 2.9516297463951937, 'alpha': 0.010989449468843839, 'min_child_weight': 8}. Best is trial 10 with value: 6821.197900134258.\n",
      "[I 2025-07-25 19:40:23,097] Trial 12 finished with value: 6814.248199886301 and parameters: {'eta': 0.02143416243251684, 'max_depth': 4, 'subsample': 0.6063896283311488, 'colsample_bytree': 0.7964142688352593, 'lambda': 5.827907983331321, 'alpha': 0.12097790666333506, 'min_child_weight': 13}. Best is trial 12 with value: 6814.248199886301.\n",
      "[I 2025-07-25 19:40:36,695] Trial 13 finished with value: 6818.967844458911 and parameters: {'eta': 0.01848706166178655, 'max_depth': 4, 'subsample': 0.621232131845999, 'colsample_bytree': 0.8528845121080583, 'lambda': 3.8191438249193226, 'alpha': 0.016416482660729065, 'min_child_weight': 7}. Best is trial 12 with value: 6814.248199886301.\n",
      "[I 2025-07-25 19:40:52,803] Trial 14 finished with value: 6826.240645706418 and parameters: {'eta': 0.011532427727437367, 'max_depth': 5, 'subsample': 0.6041912614327138, 'colsample_bytree': 0.9317737813971406, 'lambda': 10.003451097195834, 'alpha': 0.10298487862695434, 'min_child_weight': 6}. Best is trial 12 with value: 6814.248199886301.\n",
      "[I 2025-07-25 19:41:04,046] Trial 15 finished with value: 6817.759230771491 and parameters: {'eta': 0.018175793653686483, 'max_depth': 5, 'subsample': 0.6759047719085846, 'colsample_bytree': 0.8731820950224456, 'lambda': 12.250602164353456, 'alpha': 0.31314246480631563, 'min_child_weight': 14}. Best is trial 12 with value: 6814.248199886301.\n",
      "[I 2025-07-25 19:41:14,285] Trial 16 finished with value: 6812.824800944483 and parameters: {'eta': 0.018747936275896302, 'max_depth': 5, 'subsample': 0.6944359915647627, 'colsample_bytree': 0.8836311785544233, 'lambda': 19.4922678738083, 'alpha': 0.3156907034903922, 'min_child_weight': 16}. Best is trial 16 with value: 6812.824800944483.\n",
      "[I 2025-07-25 19:41:20,072] Trial 17 finished with value: 6831.707318530989 and parameters: {'eta': 0.040332212705134624, 'max_depth': 5, 'subsample': 0.7186993742429402, 'colsample_bytree': 0.7809912029476114, 'lambda': 31.343931368781814, 'alpha': 0.3383738259869065, 'min_child_weight': 18}. Best is trial 16 with value: 6812.824800944483.\n",
      "[I 2025-07-25 19:41:53,070] Trial 18 finished with value: 6848.905871296163 and parameters: {'eta': 0.015130351859530915, 'max_depth': 3, 'subsample': 0.9445183450630574, 'colsample_bytree': 0.7592141751245438, 'lambda': 0.7367584253545729, 'alpha': 0.033835020312986554, 'min_child_weight': 15}. Best is trial 16 with value: 6812.824800944483.\n",
      "[I 2025-07-25 19:42:10,281] Trial 19 finished with value: 6823.605696474452 and parameters: {'eta': 0.010369630297580134, 'max_depth': 6, 'subsample': 0.6573321850086548, 'colsample_bytree': 0.8944186754745642, 'lambda': 26.298770579974942, 'alpha': 0.05640710723986963, 'min_child_weight': 18}. Best is trial 16 with value: 6812.824800944483.\n",
      "[I 2025-07-25 19:42:18,875] Trial 20 finished with value: 6825.140611232427 and parameters: {'eta': 0.021432643447608004, 'max_depth': 5, 'subsample': 0.7241006118809693, 'colsample_bytree': 0.9472805018038153, 'lambda': 7.440429291045097, 'alpha': 0.2940117360592969, 'min_child_weight': 11}. Best is trial 16 with value: 6812.824800944483.\n",
      "[I 2025-07-25 19:42:26,846] Trial 21 finished with value: 6814.904033310786 and parameters: {'eta': 0.020691651868275675, 'max_depth': 5, 'subsample': 0.6703660169324718, 'colsample_bytree': 0.8701237276895867, 'lambda': 14.354149010904774, 'alpha': 0.5719663622739446, 'min_child_weight': 14}. Best is trial 16 with value: 6812.824800944483.\n",
      "[I 2025-07-25 19:42:37,802] Trial 22 finished with value: 6827.219983286503 and parameters: {'eta': 0.022673906748990166, 'max_depth': 4, 'subsample': 0.64873633174386, 'colsample_bytree': 0.8095469266468697, 'lambda': 1.7858002992119022, 'alpha': 0.5991973676380723, 'min_child_weight': 13}. Best is trial 16 with value: 6812.824800944483.\n",
      "[I 2025-07-25 19:42:46,139] Trial 23 finished with value: 6838.474330718878 and parameters: {'eta': 0.03504898075237194, 'max_depth': 5, 'subsample': 0.6747806986130186, 'colsample_bytree': 0.8562566027239563, 'lambda': 86.75373748661785, 'alpha': 0.6615638050435947, 'min_child_weight': 18}. Best is trial 16 with value: 6812.824800944483.\n",
      "[I 2025-07-25 19:43:00,760] Trial 24 finished with value: 6834.701725110446 and parameters: {'eta': 0.014387837718584639, 'max_depth': 6, 'subsample': 0.640041327464045, 'colsample_bytree': 0.9048445877427314, 'lambda': 6.468532274494892, 'alpha': 0.09981847687118418, 'min_child_weight': 15}. Best is trial 16 with value: 6812.824800944483.\n",
      "[I 2025-07-25 19:43:14,752] Trial 25 finished with value: 6830.178403516566 and parameters: {'eta': 0.02077248956903957, 'max_depth': 4, 'subsample': 0.7358509166681586, 'colsample_bytree': 0.8721920404233328, 'lambda': 22.49358881063921, 'alpha': 2.0874070970402783, 'min_child_weight': 20}. Best is trial 16 with value: 6812.824800944483.\n",
      "[I 2025-07-25 19:43:26,539] Trial 26 finished with value: 6813.704509931986 and parameters: {'eta': 0.01626230205266222, 'max_depth': 5, 'subsample': 0.6937847466812366, 'colsample_bytree': 0.794382804777155, 'lambda': 0.29805149823641974, 'alpha': 0.05527341560286741, 'min_child_weight': 11}. Best is trial 16 with value: 6812.824800944483.\n",
      "[I 2025-07-25 19:43:55,305] Trial 27 finished with value: 6834.743490505494 and parameters: {'eta': 0.015973708382297936, 'max_depth': 3, 'subsample': 0.7940457103403147, 'colsample_bytree': 0.7323410721632122, 'lambda': 0.3100231240323856, 'alpha': 0.04888868163346798, 'min_child_weight': 11}. Best is trial 16 with value: 6812.824800944483.\n",
      "[I 2025-07-25 19:44:17,452] Trial 28 finished with value: 6810.6867049341545 and parameters: {'eta': 0.01262701763251539, 'max_depth': 4, 'subsample': 0.6998800758505612, 'colsample_bytree': 0.7904960107771442, 'lambda': 0.04535729555486087, 'alpha': 0.18059976328847985, 'min_child_weight': 9}. Best is trial 28 with value: 6810.6867049341545.\n",
      "[I 2025-07-25 19:44:31,471] Trial 29 finished with value: 6830.615116118592 and parameters: {'eta': 0.010965502609193371, 'max_depth': 6, 'subsample': 0.7050214098118514, 'colsample_bytree': 0.740686175120244, 'lambda': 0.047757323353968144, 'alpha': 0.04622621155028316, 'min_child_weight': 4}. Best is trial 28 with value: 6810.6867049341545.\n",
      "[I 2025-07-25 19:44:55,645] Trial 30 finished with value: 6813.735322922185 and parameters: {'eta': 0.012688274585483501, 'max_depth': 5, 'subsample': 0.7431459881149461, 'colsample_bytree': 0.8380109157699046, 'lambda': 0.09219351594039195, 'alpha': 0.18870730567081162, 'min_child_weight': 9}. Best is trial 28 with value: 6810.6867049341545.\n",
      "[I 2025-07-25 19:45:11,327] Trial 31 finished with value: 6820.440724990079 and parameters: {'eta': 0.013005553697490136, 'max_depth': 5, 'subsample': 0.7424675711228246, 'colsample_bytree': 0.8415108560969036, 'lambda': 0.07100449630523233, 'alpha': 0.19243424035744597, 'min_child_weight': 9}. Best is trial 28 with value: 6810.6867049341545.\n",
      "[I 2025-07-25 19:45:22,917] Trial 32 finished with value: 6834.561088121888 and parameters: {'eta': 0.01251753644380156, 'max_depth': 6, 'subsample': 0.69114954314741, 'colsample_bytree': 0.7981245758628468, 'lambda': 0.22489522547519833, 'alpha': 0.07487853474750011, 'min_child_weight': 8}. Best is trial 28 with value: 6810.6867049341545.\n",
      "[I 2025-07-25 19:45:45,828] Trial 33 finished with value: 6819.401954797813 and parameters: {'eta': 0.01022756110594699, 'max_depth': 5, 'subsample': 0.774981122534974, 'colsample_bytree': 0.8371430775533534, 'lambda': 0.3766083218557202, 'alpha': 0.23466320219618447, 'min_child_weight': 10}. Best is trial 28 with value: 6810.6867049341545.\n",
      "[I 2025-07-25 19:46:04,081] Trial 34 finished with value: 6829.683388655772 and parameters: {'eta': 0.01590640414786196, 'max_depth': 4, 'subsample': 0.7025068602350367, 'colsample_bytree': 0.7838828301236561, 'lambda': 0.07111988358016114, 'alpha': 0.14040034721896216, 'min_child_weight': 7}. Best is trial 28 with value: 6810.6867049341545.\n",
      "[I 2025-07-25 19:46:17,687] Trial 35 finished with value: 6821.124353533116 and parameters: {'eta': 0.012012247257766642, 'max_depth': 5, 'subsample': 0.7494790854136515, 'colsample_bytree': 0.7563716539083887, 'lambda': 0.010998766216526743, 'alpha': 0.03097150058428923, 'min_child_weight': 12}. Best is trial 28 with value: 6810.6867049341545.\n",
      "[I 2025-07-25 19:46:27,818] Trial 36 finished with value: 6891.745238693947 and parameters: {'eta': 0.01374175104520114, 'max_depth': 8, 'subsample': 0.8313690120678198, 'colsample_bytree': 0.8048408841161984, 'lambda': 0.03236167342932253, 'alpha': 0.48462562660287534, 'min_child_weight': 3}. Best is trial 28 with value: 6810.6867049341545.\n",
      "[I 2025-07-25 19:46:45,963] Trial 37 finished with value: 6827.041520840445 and parameters: {'eta': 0.016611647314193626, 'max_depth': 4, 'subsample': 0.724264159733479, 'colsample_bytree': 0.886938255349343, 'lambda': 0.1124567545083378, 'alpha': 1.0276236220517043, 'min_child_weight': 6}. Best is trial 28 with value: 6810.6867049341545.\n",
      "[I 2025-07-25 19:46:50,213] Trial 38 finished with value: 6841.727465803287 and parameters: {'eta': 0.04501511684095452, 'max_depth': 6, 'subsample': 0.6892883332616861, 'colsample_bytree': 0.6895837565069053, 'lambda': 0.4365984427231146, 'alpha': 0.16211479455600156, 'min_child_weight': 9}. Best is trial 28 with value: 6810.6867049341545.\n",
      "[I 2025-07-25 19:47:23,228] Trial 39 finished with value: 6931.508382110915 and parameters: {'eta': 0.024745596032845874, 'max_depth': 7, 'subsample': 0.7594387139521341, 'colsample_bytree': 0.7203050734470905, 'lambda': 0.198172162017183, 'alpha': 95.93861351301858, 'min_child_weight': 12}. Best is trial 28 with value: 6810.6867049341545.\n",
      "[I 2025-07-25 19:47:45,982] Trial 40 finished with value: 6834.585261880736 and parameters: {'eta': 0.01882400223420431, 'max_depth': 3, 'subsample': 0.7834582521383349, 'colsample_bytree': 0.8236004419380516, 'lambda': 0.022828194949293827, 'alpha': 0.08002927913679045, 'min_child_weight': 17}. Best is trial 28 with value: 6810.6867049341545.\n",
      "[I 2025-07-25 19:47:54,200] Trial 41 finished with value: 6828.7471379843 and parameters: {'eta': 0.024391873496013964, 'max_depth': 4, 'subsample': 0.6591111836716346, 'colsample_bytree': 0.7917914339239464, 'lambda': 0.8624876628771548, 'alpha': 0.13506933402185742, 'min_child_weight': 13}. Best is trial 28 with value: 6810.6867049341545.\n",
      "[I 2025-07-25 19:48:13,650] Trial 42 finished with value: 6825.892403949986 and parameters: {'eta': 0.01472890673740836, 'max_depth': 4, 'subsample': 0.7085411185215331, 'colsample_bytree': 0.7601543133109468, 'lambda': 56.429971741736, 'alpha': 0.37052252531851987, 'min_child_weight': 10}. Best is trial 28 with value: 6810.6867049341545.\n",
      "[I 2025-07-25 19:48:20,152] Trial 43 finished with value: 6830.153671558067 and parameters: {'eta': 0.031776945987030215, 'max_depth': 5, 'subsample': 0.8252620754851285, 'colsample_bytree': 0.7759700644417825, 'lambda': 1.5421235157365776, 'alpha': 0.06435846815714553, 'min_child_weight': 12}. Best is trial 28 with value: 6810.6867049341545.\n",
      "[I 2025-07-25 19:48:31,522] Trial 44 finished with value: 6819.333594849679 and parameters: {'eta': 0.019909323677746567, 'max_depth': 4, 'subsample': 0.6288146794658377, 'colsample_bytree': 0.8152332934273718, 'lambda': 0.10341740028734392, 'alpha': 0.21461957379799002, 'min_child_weight': 16}. Best is trial 28 with value: 6810.6867049341545.\n",
      "[I 2025-07-25 19:48:56,346] Trial 45 finished with value: 6845.863466006549 and parameters: {'eta': 0.01680437752662752, 'max_depth': 3, 'subsample': 0.876884939047152, 'colsample_bytree': 0.6000537994315309, 'lambda': 0.041114883733274425, 'alpha': 0.8394001253527328, 'min_child_weight': 8}. Best is trial 28 with value: 6810.6867049341545.\n",
      "[I 2025-07-25 19:49:16,161] Trial 46 finished with value: 6811.819893690877 and parameters: {'eta': 0.011536983301902783, 'max_depth': 5, 'subsample': 0.6902882409798305, 'colsample_bytree': 0.8401571669993185, 'lambda': 0.5625702228534818, 'alpha': 0.01919213002930925, 'min_child_weight': 11}. Best is trial 28 with value: 6810.6867049341545.\n",
      "[I 2025-07-25 19:49:30,392] Trial 47 finished with value: 6821.567805549235 and parameters: {'eta': 0.011642175687395223, 'max_depth': 6, 'subsample': 0.6894533925521753, 'colsample_bytree': 0.8533724587320836, 'lambda': 0.6209555180966287, 'alpha': 0.017719137995771954, 'min_child_weight': 11}. Best is trial 28 with value: 6810.6867049341545.\n",
      "[I 2025-07-25 19:49:53,978] Trial 48 finished with value: 6816.555410208331 and parameters: {'eta': 0.012492663093659684, 'max_depth': 5, 'subsample': 0.7293039336803497, 'colsample_bytree': 0.8455163292734927, 'lambda': 0.2556933772620122, 'alpha': 0.010593957438016858, 'min_child_weight': 7}. Best is trial 28 with value: 6810.6867049341545.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-07-25 19:50:08,271] A new study created in memory with name: no-name-33d8e5fd-1ba5-4955-bc8b-492d1e652188\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-07-25 19:50:08,267] Trial 49 finished with value: 6821.570076007746 and parameters: {'eta': 0.013875012454619884, 'max_depth': 5, 'subsample': 0.7656751480273651, 'colsample_bytree': 0.8239448453442719, 'lambda': 0.14779033285446974, 'alpha': 0.0253047428184783, 'min_child_weight': 9}. Best is trial 28 with value: 6810.6867049341545.\n",
      "\n",
      "Lower-Bound Model Tuning Complete.\n",
      "Best Validation Score (Pinball Loss): 6,810.6867\n",
      "Best Parameters Found: {'eta': 0.01262701763251539, 'max_depth': 4, 'subsample': 0.6998800758505612, 'colsample_bytree': 0.7904960107771442, 'lambda': 0.04535729555486087, 'alpha': 0.18059976328847985, 'min_child_weight': 9, 'n_estimators': 1680}\n",
      "\n",
      "--- Tuning the XGBoost Upper-Bound Model (alpha=0.95)... ---\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f5e03550ee8f4e6594939542437f10c3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/50 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-07-25 19:50:17,150] Trial 0 finished with value: 7866.627735773041 and parameters: {'eta': 0.02259648069763278, 'max_depth': 6, 'subsample': 0.7426364997144287, 'colsample_bytree': 0.6877151408915609, 'lambda': 0.03888545030675885, 'alpha': 0.06458208136736243, 'min_child_weight': 1}. Best is trial 0 with value: 7866.627735773041.\n",
      "[I 2025-07-25 19:50:22,628] Trial 1 finished with value: 7871.151835758187 and parameters: {'eta': 0.041573920656684575, 'max_depth': 7, 'subsample': 0.730446736232013, 'colsample_bytree': 0.8389441984720529, 'lambda': 92.8852846914973, 'alpha': 34.82656730971083, 'min_child_weight': 20}. Best is trial 0 with value: 7866.627735773041.\n",
      "[I 2025-07-25 19:50:25,728] Trial 2 finished with value: 7875.780349157314 and parameters: {'eta': 0.08401760616923908, 'max_depth': 5, 'subsample': 0.7431994070292813, 'colsample_bytree': 0.9470967973824076, 'lambda': 0.10740313852214445, 'alpha': 17.79435332401801, 'min_child_weight': 11}. Best is trial 0 with value: 7866.627735773041.\n",
      "[I 2025-07-25 19:50:29,170] Trial 3 finished with value: 7895.23798227541 and parameters: {'eta': 0.06354112355752256, 'max_depth': 7, 'subsample': 0.7133437459289191, 'colsample_bytree': 0.6267428388998865, 'lambda': 12.812325629424297, 'alpha': 3.107025947232804, 'min_child_weight': 17}. Best is trial 0 with value: 7866.627735773041.\n",
      "[I 2025-07-25 19:50:33,567] Trial 4 finished with value: 7920.578249622402 and parameters: {'eta': 0.07114122779411731, 'max_depth': 7, 'subsample': 0.8998219348417917, 'colsample_bytree': 0.6368656369165093, 'lambda': 87.944799203523, 'alpha': 0.08728592492428397, 'min_child_weight': 15}. Best is trial 0 with value: 7866.627735773041.\n",
      "[I 2025-07-25 19:50:53,088] Trial 5 finished with value: 7863.859184927139 and parameters: {'eta': 0.010629121995312768, 'max_depth': 6, 'subsample': 0.8088577995732718, 'colsample_bytree': 0.7045339948979025, 'lambda': 0.02325916758102795, 'alpha': 0.3604668128506505, 'min_child_weight': 8}. Best is trial 5 with value: 7863.859184927139.\n",
      "[I 2025-07-25 19:50:56,888] Trial 6 finished with value: 8022.779393299765 and parameters: {'eta': 0.06543803526749112, 'max_depth': 8, 'subsample': 0.8812483274326338, 'colsample_bytree': 0.7927980307435285, 'lambda': 6.775687918356792, 'alpha': 1.9191669667920292, 'min_child_weight': 6}. Best is trial 5 with value: 7863.859184927139.\n",
      "[I 2025-07-25 19:51:00,954] Trial 7 finished with value: 7894.652156412882 and parameters: {'eta': 0.05544017368411433, 'max_depth': 6, 'subsample': 0.726453280383353, 'colsample_bytree': 0.8189550018033549, 'lambda': 13.21870721507844, 'alpha': 0.4322561662842409, 'min_child_weight': 10}. Best is trial 5 with value: 7863.859184927139.\n",
      "[I 2025-07-25 19:51:33,195] Trial 8 finished with value: 7841.888600019479 and parameters: {'eta': 0.012000949206151681, 'max_depth': 3, 'subsample': 0.901732283494165, 'colsample_bytree': 0.7170141903832212, 'lambda': 0.06099794741929315, 'alpha': 1.8130180146675603, 'min_child_weight': 1}. Best is trial 8 with value: 7841.888600019479.\n",
      "[I 2025-07-25 19:51:40,231] Trial 9 finished with value: 7864.165735461338 and parameters: {'eta': 0.04635108343041296, 'max_depth': 5, 'subsample': 0.851120876218903, 'colsample_bytree': 0.9493998175046905, 'lambda': 6.10557185563422, 'alpha': 15.38593400728722, 'min_child_weight': 11}. Best is trial 8 with value: 7841.888600019479.\n",
      "[I 2025-07-25 19:52:07,651] Trial 10 finished with value: 7828.375076458654 and parameters: {'eta': 0.011182495456636089, 'max_depth': 3, 'subsample': 0.6269141865851323, 'colsample_bytree': 0.7172315504738234, 'lambda': 0.37001752701062285, 'alpha': 0.016634184536068238, 'min_child_weight': 1}. Best is trial 10 with value: 7828.375076458654.\n",
      "[I 2025-07-25 19:52:30,390] Trial 11 finished with value: 7826.052930425279 and parameters: {'eta': 0.010063366723578422, 'max_depth': 3, 'subsample': 0.610919169117335, 'colsample_bytree': 0.728851048093688, 'lambda': 0.2849772690345848, 'alpha': 0.024068870046322056, 'min_child_weight': 2}. Best is trial 11 with value: 7826.052930425279.\n",
      "[I 2025-07-25 19:52:41,985] Trial 12 finished with value: 7827.557926044991 and parameters: {'eta': 0.019645757456151606, 'max_depth': 3, 'subsample': 0.6027182007942707, 'colsample_bytree': 0.7412462139834518, 'lambda': 0.3753266840968507, 'alpha': 0.012391292354879554, 'min_child_weight': 4}. Best is trial 11 with value: 7826.052930425279.\n",
      "[I 2025-07-25 19:52:52,436] Trial 13 finished with value: 7813.693146805647 and parameters: {'eta': 0.018384106190997986, 'max_depth': 4, 'subsample': 0.6026451332081345, 'colsample_bytree': 0.7659055684196558, 'lambda': 0.6656565876484267, 'alpha': 0.011930811532185772, 'min_child_weight': 5}. Best is trial 13 with value: 7813.693146805647.\n",
      "[I 2025-07-25 19:53:03,732] Trial 14 finished with value: 7818.938780366871 and parameters: {'eta': 0.017964925093694968, 'max_depth': 4, 'subsample': 0.6624257659951086, 'colsample_bytree': 0.8689155252223117, 'lambda': 0.6777667018891185, 'alpha': 0.051437811710493796, 'min_child_weight': 4}. Best is trial 13 with value: 7813.693146805647.\n",
      "[I 2025-07-25 19:53:15,536] Trial 15 finished with value: 7839.909419182245 and parameters: {'eta': 0.017755027468916912, 'max_depth': 4, 'subsample': 0.6714175634834616, 'colsample_bytree': 0.8849094909014118, 'lambda': 1.4250796450130994, 'alpha': 0.08274595072231783, 'min_child_weight': 5}. Best is trial 13 with value: 7813.693146805647.\n",
      "[I 2025-07-25 19:53:23,006] Trial 16 finished with value: 7830.512213603727 and parameters: {'eta': 0.027833862488852897, 'max_depth': 4, 'subsample': 0.6638482633165373, 'colsample_bytree': 0.87769148013039, 'lambda': 1.2227862336668964, 'alpha': 0.2445536690923201, 'min_child_weight': 8}. Best is trial 13 with value: 7813.693146805647.\n",
      "[I 2025-07-25 19:53:37,817] Trial 17 finished with value: 7818.099787065783 and parameters: {'eta': 0.014868409761200033, 'max_depth': 4, 'subsample': 0.6701032913818091, 'colsample_bytree': 0.7786611527691161, 'lambda': 3.6688075231699213, 'alpha': 0.037210982490685364, 'min_child_weight': 4}. Best is trial 13 with value: 7813.693146805647.\n",
      "[I 2025-07-25 19:53:58,239] Trial 18 finished with value: 7830.405316552785 and parameters: {'eta': 0.014437617080601681, 'max_depth': 4, 'subsample': 0.9426510578738915, 'colsample_bytree': 0.765723233686194, 'lambda': 2.5086297336482564, 'alpha': 0.028839412725240824, 'min_child_weight': 7}. Best is trial 13 with value: 7813.693146805647.\n",
      "[I 2025-07-25 19:54:05,202] Trial 19 finished with value: 7849.416399369773 and parameters: {'eta': 0.02918625560175873, 'max_depth': 5, 'subsample': 0.787848920087106, 'colsample_bytree': 0.6698935658290165, 'lambda': 26.61042087930943, 'alpha': 0.010038560344771246, 'min_child_weight': 3}. Best is trial 13 with value: 7813.693146805647.\n",
      "[I 2025-07-25 19:54:21,578] Trial 20 finished with value: 7818.780933982797 and parameters: {'eta': 0.015166259907071796, 'max_depth': 4, 'subsample': 0.6846120280846442, 'colsample_bytree': 0.7852648720405825, 'lambda': 0.011049265542675954, 'alpha': 0.1576144767171325, 'min_child_weight': 13}. Best is trial 13 with value: 7813.693146805647.\n",
      "[I 2025-07-25 19:54:35,684] Trial 21 finished with value: 7823.680960452038 and parameters: {'eta': 0.013792060071750928, 'max_depth': 4, 'subsample': 0.6888379987840556, 'colsample_bytree': 0.7733839697947559, 'lambda': 3.1809544823964173, 'alpha': 0.1629322689549656, 'min_child_weight': 14}. Best is trial 13 with value: 7813.693146805647.\n",
      "[I 2025-07-25 19:54:47,486] Trial 22 finished with value: 7821.657687153693 and parameters: {'eta': 0.015492997783881069, 'max_depth': 5, 'subsample': 0.6371775036712793, 'colsample_bytree': 0.8106059770367231, 'lambda': 0.01035911945285551, 'alpha': 0.033782363816032855, 'min_child_weight': 13}. Best is trial 13 with value: 7813.693146805647.\n",
      "[I 2025-07-25 19:54:55,773] Trial 23 finished with value: 7825.404847474904 and parameters: {'eta': 0.023357874435376378, 'max_depth': 4, 'subsample': 0.6452152935934841, 'colsample_bytree': 0.7509055875476773, 'lambda': 0.09648346684687535, 'alpha': 0.14617527321723764, 'min_child_weight': 9}. Best is trial 13 with value: 7813.693146805647.\n",
      "[I 2025-07-25 19:55:02,920] Trial 24 finished with value: 7830.88004782977 and parameters: {'eta': 0.036461308396212, 'max_depth': 3, 'subsample': 0.6959318677457734, 'colsample_bytree': 0.7946951801563295, 'lambda': 0.1639574416559008, 'alpha': 0.9019477780332974, 'min_child_weight': 17}. Best is trial 13 with value: 7813.693146805647.\n",
      "[I 2025-07-25 19:55:12,336] Trial 25 finished with value: 7849.642128409554 and parameters: {'eta': 0.02387143237915894, 'max_depth': 5, 'subsample': 0.7673100528849174, 'colsample_bytree': 0.8413338492760941, 'lambda': 0.022031674934307938, 'alpha': 0.039493075650631135, 'min_child_weight': 12}. Best is trial 13 with value: 7813.693146805647.\n",
      "[I 2025-07-25 19:55:30,061] Trial 26 finished with value: 7806.842258494016 and parameters: {'eta': 0.01381657397769597, 'max_depth': 4, 'subsample': 0.602995575613357, 'colsample_bytree': 0.7605086548585408, 'lambda': 36.82161123879866, 'alpha': 0.7529655932485861, 'min_child_weight': 6}. Best is trial 26 with value: 7806.842258494016.\n",
      "[I 2025-07-25 19:55:45,966] Trial 27 finished with value: 7823.4199762563285 and parameters: {'eta': 0.013130553260509666, 'max_depth': 4, 'subsample': 0.6034653947970389, 'colsample_bytree': 0.6642018262602287, 'lambda': 29.306773231042506, 'alpha': 4.498766911642695, 'min_child_weight': 6}. Best is trial 26 with value: 7806.842258494016.\n",
      "[I 2025-07-25 19:56:06,932] Trial 28 finished with value: 7886.712359537865 and parameters: {'eta': 0.0185627596750055, 'max_depth': 5, 'subsample': 0.6305945193584637, 'colsample_bytree': 0.6009448700602769, 'lambda': 2.8034920467379063, 'alpha': 90.104512475257, 'min_child_weight': 3}. Best is trial 26 with value: 7806.842258494016.\n",
      "[I 2025-07-25 19:56:15,167] Trial 29 finished with value: 7844.225797940394 and parameters: {'eta': 0.022028041975543283, 'max_depth': 6, 'subsample': 0.6467940522329557, 'colsample_bytree': 0.6875440654131603, 'lambda': 39.79731468178969, 'alpha': 0.5395924576408189, 'min_child_weight': 5}. Best is trial 26 with value: 7806.842258494016.\n",
      "[I 2025-07-25 19:56:31,591] Trial 30 finished with value: 7826.19255332737 and parameters: {'eta': 0.01659807952672228, 'max_depth': 3, 'subsample': 0.6167628235593724, 'colsample_bytree': 0.745505864016194, 'lambda': 0.6735709202740833, 'alpha': 6.147309730725215, 'min_child_weight': 7}. Best is trial 26 with value: 7806.842258494016.\n",
      "[I 2025-07-25 19:56:50,754] Trial 31 finished with value: 7815.331071253359 and parameters: {'eta': 0.012582962643626539, 'max_depth': 4, 'subsample': 0.6864311153248616, 'colsample_bytree': 0.7751810856157927, 'lambda': 6.269015986568339, 'alpha': 0.11009167645175723, 'min_child_weight': 5}. Best is trial 26 with value: 7806.842258494016.\n",
      "[I 2025-07-25 19:57:10,494] Trial 32 finished with value: 7812.687878906185 and parameters: {'eta': 0.011925769782311786, 'max_depth': 4, 'subsample': 0.6581767445299818, 'colsample_bytree': 0.7654477900888844, 'lambda': 6.559887866800209, 'alpha': 0.06927800370343838, 'min_child_weight': 3}. Best is trial 26 with value: 7806.842258494016.\n",
      "[I 2025-07-25 19:57:28,069] Trial 33 finished with value: 7828.4285288931205 and parameters: {'eta': 0.011967992903813356, 'max_depth': 4, 'subsample': 0.6481229889633402, 'colsample_bytree': 0.8312381610334533, 'lambda': 15.592566285230765, 'alpha': 0.9127886279857376, 'min_child_weight': 3}. Best is trial 26 with value: 7806.842258494016.\n",
      "[I 2025-07-25 19:57:41,678] Trial 34 finished with value: 7812.551114893833 and parameters: {'eta': 0.01277860082976884, 'max_depth': 5, 'subsample': 0.7123014412547681, 'colsample_bytree': 0.7535298015849697, 'lambda': 7.355610543103312, 'alpha': 0.07436309205271695, 'min_child_weight': 6}. Best is trial 26 with value: 7806.842258494016.\n",
      "[I 2025-07-25 19:57:52,924] Trial 35 finished with value: 7826.998781765638 and parameters: {'eta': 0.019921614735525946, 'max_depth': 5, 'subsample': 0.714177836371607, 'colsample_bytree': 0.7543968381891195, 'lambda': 72.65334778250842, 'alpha': 0.0646588729965903, 'min_child_weight': 7}. Best is trial 26 with value: 7806.842258494016.\n",
      "[I 2025-07-25 19:58:12,539] Trial 36 finished with value: 7848.611000431338 and parameters: {'eta': 0.010224764327612856, 'max_depth': 6, 'subsample': 0.7474160600570862, 'colsample_bytree': 0.8070091470633501, 'lambda': 50.952594905232665, 'alpha': 0.01912881512709825, 'min_child_weight': 9}. Best is trial 26 with value: 7806.842258494016.\n",
      "[I 2025-07-25 19:58:14,780] Trial 37 finished with value: 7897.943562201169 and parameters: {'eta': 0.0951994660285057, 'max_depth': 5, 'subsample': 0.622438401361336, 'colsample_bytree': 0.6986196544124561, 'lambda': 10.2658777263061, 'alpha': 0.2582985581085092, 'min_child_weight': 2}. Best is trial 26 with value: 7806.842258494016.\n",
      "[I 2025-07-25 19:58:27,640] Trial 38 finished with value: 7999.273677379483 and parameters: {'eta': 0.01348484303842402, 'max_depth': 8, 'subsample': 0.7084496952529473, 'colsample_bytree': 0.8510094238177068, 'lambda': 20.807355040839653, 'alpha': 0.55974401899743, 'min_child_weight': 6}. Best is trial 26 with value: 7806.842258494016.\n",
      "[I 2025-07-25 19:58:43,706] Trial 39 finished with value: 7850.662040030087 and parameters: {'eta': 0.011885163772200212, 'max_depth': 6, 'subsample': 0.7442260172527531, 'colsample_bytree': 0.7288708942500194, 'lambda': 1.7778994561072576, 'alpha': 1.590951804541685, 'min_child_weight': 8}. Best is trial 26 with value: 7806.842258494016.\n",
      "[I 2025-07-25 19:58:56,054] Trial 40 finished with value: 7918.60656873523 and parameters: {'eta': 0.016433693779164083, 'max_depth': 7, 'subsample': 0.8294675260699349, 'colsample_bytree': 0.7603471257744611, 'lambda': 9.240655879355117, 'alpha': 0.05883445329749751, 'min_child_weight': 10}. Best is trial 26 with value: 7806.842258494016.\n",
      "[I 2025-07-25 19:59:14,833] Trial 41 finished with value: 7821.271146857849 and parameters: {'eta': 0.012863175327136532, 'max_depth': 4, 'subsample': 0.6519276493586474, 'colsample_bytree': 0.8003365417514012, 'lambda': 7.214662781104422, 'alpha': 0.11308237072208335, 'min_child_weight': 5}. Best is trial 26 with value: 7806.842258494016.\n",
      "[I 2025-07-25 19:59:38,744] Trial 42 finished with value: 7832.136896274373 and parameters: {'eta': 0.012414373519340442, 'max_depth': 3, 'subsample': 0.6868931223688626, 'colsample_bytree': 0.7746945452736116, 'lambda': 16.4671679367532, 'alpha': 0.24817052302297438, 'min_child_weight': 5}. Best is trial 26 with value: 7806.842258494016.\n",
      "[I 2025-07-25 19:59:55,948] Trial 43 finished with value: 7823.05910613684 and parameters: {'eta': 0.011400997866944524, 'max_depth': 4, 'subsample': 0.7278711780638133, 'colsample_bytree': 0.7314380057848326, 'lambda': 5.719082099999987, 'alpha': 0.09526513859861728, 'min_child_weight': 2}. Best is trial 26 with value: 7806.842258494016.\n",
      "[I 2025-07-25 20:00:13,416] Trial 44 finished with value: 7834.481582210225 and parameters: {'eta': 0.010775214229439832, 'max_depth': 5, 'subsample': 0.7015793788417359, 'colsample_bytree': 0.9177850651558157, 'lambda': 0.8267898290620419, 'alpha': 0.016763232818159796, 'min_child_weight': 20}. Best is trial 26 with value: 7806.842258494016.\n",
      "[I 2025-07-25 20:00:28,129] Trial 45 finished with value: 7837.03993149535 and parameters: {'eta': 0.016375284375507202, 'max_depth': 3, 'subsample': 0.6208590482833862, 'colsample_bytree': 0.8220558877284506, 'lambda': 4.830081203496132, 'alpha': 0.3566213670340425, 'min_child_weight': 6}. Best is trial 26 with value: 7806.842258494016.\n",
      "[I 2025-07-25 20:00:44,591] Trial 46 finished with value: 7805.85259831284 and parameters: {'eta': 0.013754516793566996, 'max_depth': 4, 'subsample': 0.6778913078940807, 'colsample_bytree': 0.7153150169289751, 'lambda': 1.8625484929300882, 'alpha': 0.054189191364477446, 'min_child_weight': 1}. Best is trial 46 with value: 7805.85259831284.\n",
      "[I 2025-07-25 20:00:56,005] Trial 47 finished with value: 7825.803491795433 and parameters: {'eta': 0.021141572200881063, 'max_depth': 5, 'subsample': 0.6590310090383719, 'colsample_bytree': 0.709468682019949, 'lambda': 0.5105719378509157, 'alpha': 11.613235478778952, 'min_child_weight': 1}. Best is trial 46 with value: 7805.85259831284.\n",
      "[I 2025-07-25 20:01:06,314] Trial 48 finished with value: 7814.50877823767 and parameters: {'eta': 0.025655210018195544, 'max_depth': 4, 'subsample': 0.6013460323477187, 'colsample_bytree': 0.6970257221724999, 'lambda': 1.7226489701872114, 'alpha': 0.02353818417551968, 'min_child_weight': 1}. Best is trial 46 with value: 7805.85259831284.\n",
      "[I 2025-07-25 20:01:29,853] Trial 49 finished with value: 7835.153623435772 and parameters: {'eta': 0.014305669839074522, 'max_depth': 3, 'subsample': 0.6379906042803974, 'colsample_bytree': 0.7338978257543132, 'lambda': 0.234095820346429, 'alpha': 0.0487911915681196, 'min_child_weight': 3}. Best is trial 46 with value: 7805.85259831284.\n",
      "\n",
      "Upper-Bound Model Tuning Complete.\n",
      "Best Validation Score (Pinball Loss): 7,805.8526\n",
      "Best Parameters Found: {'eta': 0.013754516793566996, 'max_depth': 4, 'subsample': 0.6778913078940807, 'colsample_bytree': 0.7153150169289751, 'lambda': 1.8625484929300882, 'alpha': 0.054189191364477446, 'min_child_weight': 1, 'n_estimators': 1228}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "60"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# BLOCK 6: HYPERPARAMETER TUNING FOR QUANTILE MODELS WITH OPTUNA\n",
    "# =============================================================================\n",
    "# We will now use Optuna to find the best hyperparameters for our two specialist\n",
    "# quantile models. We tune them independently because the optimal parameters for\n",
    "# predicting the lower tail of the distribution might differ from those for the\n",
    "# upper tail.\n",
    "#\n",
    "# We use the native XGBoost API (`xgb.train`) because it allows us to use\n",
    "# callbacks like `early_stopping`, which significantly speeds up the tuning\n",
    "# process by not training for a fixed, excessive number of rounds.\n",
    "\n",
    "# --- Step 1: Create a Holdout Set for Tuning ---\n",
    "# For hyperparameter tuning, we need a single, consistent validation set to\n",
    "# evaluate each trial's performance. We'll split off 20% of our elite\n",
    "# training data for this purpose.\n",
    "print(\"\\n--- Preparing data for Optuna tuning ---\")\n",
    "X_train_opt, X_val_opt, y_train_opt, y_val_opt = train_test_split(\n",
    "    X_for_quantile,\n",
    "    y_true,\n",
    "    test_size=0.20,\n",
    "    random_state=RANDOM_STATE\n",
    ")\n",
    "\n",
    "# The native XGBoost API is most efficient with its internal DMatrix format.\n",
    "dtrain_opt = xgb.DMatrix(X_train_opt, label=y_train_opt)\n",
    "dval_opt = xgb.DMatrix(X_val_opt, label=y_val_opt)\n",
    "\n",
    "print(f\"Data split for tuning: {len(X_train_opt)} train, {len(X_val_opt)} validation samples.\")\n",
    "\n",
    "# --- Step 2: Tune the Lower-Bound Model (alpha=0.05) ---\n",
    "\n",
    "def objective_lower(trial):\n",
    "    \"\"\"Optuna objective function for the lower-bound quantile model.\"\"\"\n",
    "    params = {\n",
    "        'objective': 'reg:quantileerror',\n",
    "        'quantile_alpha': 0.05,\n",
    "        # The 'quantile' eval_metric is the pinball loss, which is exactly\n",
    "        # what we want to minimize for this objective.\n",
    "        'eval_metric': 'quantile',\n",
    "        'tree_method': 'hist',\n",
    "        'seed': RANDOM_STATE,\n",
    "        'n_jobs': -1,\n",
    "\n",
    "        # --- Hyperparameters to Tune ---\n",
    "        'eta': trial.suggest_float('eta', 0.01, 0.1, log=True),\n",
    "        'max_depth': trial.suggest_int('max_depth', 3, 8),\n",
    "        'subsample': trial.suggest_float('subsample', 0.6, 0.95),\n",
    "        'colsample_bytree': trial.suggest_float('colsample_bytree', 0.6, 0.95),\n",
    "        'lambda': trial.suggest_float('lambda', 1e-2, 100.0, log=True), # L2 Reg\n",
    "        'alpha': trial.suggest_float('alpha', 1e-2, 100.0, log=True),   # L1 Reg\n",
    "        'min_child_weight': trial.suggest_int('min_child_weight', 1, 20),\n",
    "    }\n",
    "\n",
    "    # Train the model with early stopping\n",
    "    bst = xgb.train(\n",
    "        params,\n",
    "        dtrain_opt,\n",
    "        num_boost_round=2000,  # High number, early stopping will find the best\n",
    "        evals=[(dval_opt, 'validation')],\n",
    "        early_stopping_rounds=50,\n",
    "        verbose_eval=False\n",
    "    )\n",
    "\n",
    "    # Store the best number of boosting rounds in the trial attributes\n",
    "    trial.set_user_attr('best_iteration', bst.best_iteration)\n",
    "\n",
    "    # Return the score to be minimized (pinball loss on the validation set)\n",
    "    return bst.best_score\n",
    "\n",
    "print(\"\\n--- Tuning the XGBoost Lower-Bound Model (alpha=0.05)... ---\")\n",
    "N_TRIALS = 50 # A good number for a solid search\n",
    "study_lower = optuna.create_study(direction='minimize')\n",
    "study_lower.optimize(objective_lower, n_trials=N_TRIALS, show_progress_bar=True)\n",
    "\n",
    "# Extract best parameters and add the optimal number of estimators\n",
    "best_params_lower = study_lower.best_params\n",
    "best_params_lower['n_estimators'] = study_lower.best_trial.user_attrs['best_iteration']\n",
    "\n",
    "print(f\"\\nLower-Bound Model Tuning Complete.\")\n",
    "print(f\"Best Validation Score (Pinball Loss): {study_lower.best_value:,.4f}\")\n",
    "print(f\"Best Parameters Found: {best_params_lower}\")\n",
    "\n",
    "# --- Step 3: Tune the Upper-Bound Model (alpha=0.95) ---\n",
    "\n",
    "def objective_upper(trial):\n",
    "    \"\"\"Optuna objective function for the upper-bound quantile model.\"\"\"\n",
    "    # The parameter space is identical, only quantile_alpha changes.\n",
    "    params = {\n",
    "        'objective': 'reg:quantileerror',\n",
    "        'quantile_alpha': 0.95, # The only key difference from the lower model\n",
    "        'eval_metric': 'quantile',\n",
    "        'tree_method': 'hist',\n",
    "        'seed': RANDOM_STATE,\n",
    "        'n_jobs': -1,\n",
    "\n",
    "        # --- Hyperparameters to Tune ---\n",
    "        'eta': trial.suggest_float('eta', 0.01, 0.1, log=True),\n",
    "        'max_depth': trial.suggest_int('max_depth', 3, 8),\n",
    "        'subsample': trial.suggest_float('subsample', 0.6, 0.95),\n",
    "        'colsample_bytree': trial.suggest_float('colsample_bytree', 0.6, 0.95),\n",
    "        'lambda': trial.suggest_float('lambda', 1e-2, 100.0, log=True),\n",
    "        'alpha': trial.suggest_float('alpha', 1e-2, 100.0, log=True),\n",
    "        'min_child_weight': trial.suggest_int('min_child_weight', 1, 20),\n",
    "    }\n",
    "\n",
    "    bst = xgb.train(\n",
    "        params,\n",
    "        dtrain_opt,\n",
    "        num_boost_round=2000,\n",
    "        evals=[(dval_opt, 'validation')],\n",
    "        early_stopping_rounds=50,\n",
    "        verbose_eval=False\n",
    "    )\n",
    "    trial.set_user_attr('best_iteration', bst.best_iteration)\n",
    "    return bst.best_score\n",
    "\n",
    "print(\"\\n--- Tuning the XGBoost Upper-Bound Model (alpha=0.95)... ---\")\n",
    "study_upper = optuna.create_study(direction='minimize')\n",
    "study_upper.optimize(objective_upper, n_trials=N_TRIALS, show_progress_bar=True)\n",
    "\n",
    "# Extract best parameters\n",
    "best_params_upper = study_upper.best_params\n",
    "best_params_upper['n_estimators'] = study_upper.best_trial.user_attrs['best_iteration']\n",
    "\n",
    "print(f\"\\nUpper-Bound Model Tuning Complete.\")\n",
    "print(f\"Best Validation Score (Pinball Loss): {study_upper.best_value:,.4f}\")\n",
    "print(f\"Best Parameters Found: {best_params_upper}\")\n",
    "\n",
    "# Clean up to free memory\n",
    "del dtrain_opt, dval_opt, X_train_opt, X_val_opt, y_train_opt, y_val_opt\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "59e26397-9fc6-4e8c-82aa-7cbefe906e4f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Initializing K-Fold training for tuned quantile models ---\n",
      "Prediction artifacts will be saved to: './meta_quantile_models/'\n",
      "\n",
      "===== FOLD 1/5 =====\n",
      "  -> Training lower-bound model (alpha=0.05)...\n",
      "  -> Training upper-bound model (alpha=0.95)...\n",
      "\n",
      "===== FOLD 2/5 =====\n",
      "  -> Training lower-bound model (alpha=0.05)...\n",
      "  -> Training upper-bound model (alpha=0.95)...\n",
      "\n",
      "===== FOLD 3/5 =====\n",
      "  -> Training lower-bound model (alpha=0.05)...\n",
      "  -> Training upper-bound model (alpha=0.95)...\n",
      "\n",
      "===== FOLD 4/5 =====\n",
      "  -> Training lower-bound model (alpha=0.05)...\n",
      "  -> Training upper-bound model (alpha=0.95)...\n",
      "\n",
      "===== FOLD 5/5 =====\n",
      "  -> Training lower-bound model (alpha=0.05)...\n",
      "  -> Training upper-bound model (alpha=0.95)...\n",
      "\n",
      "--- K-Fold training complete. ---\n",
      "\n",
      "--- Saving quantile prediction arrays to disk... ---\n",
      "Saved 'oof_lower_preds.npy' successfully.\n",
      "Saved 'test_lower_preds.npy' successfully.\n",
      "Saved 'oof_upper_preds.npy' successfully.\n",
      "Saved 'test_upper_preds.npy' successfully.\n",
      "\n",
      "All prediction artifacts are now ready for the final calibration and submission step.\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# BLOCK 7: K-FOLD TRAINING & PREDICTION FOR QUANTILE MODELS (Corrected)\n",
    "# =============================================================================\n",
    "# With our tuned hyperparameters, we now perform the final model training using\n",
    "# a 5-Fold cross-validation strategy.\n",
    "#\n",
    "# THIS IS THE CORRECTED VERSION using the native `xgb.train` API to properly\n",
    "# support early stopping during the final K-Fold training loop.\n",
    "\n",
    "# --- Step 1: Setup and Initialization ---\n",
    "print(\"\\n--- Initializing K-Fold training for tuned quantile models ---\")\n",
    "\n",
    "# Define and create the directory to save our new prediction artifacts\n",
    "META_QUANTILE_PATH = './meta_quantile_models/'\n",
    "os.makedirs(META_QUANTILE_PATH, exist_ok=True)\n",
    "print(f\"Prediction artifacts will be saved to: '{META_QUANTILE_PATH}'\")\n",
    "\n",
    "# Initialize arrays to store the predictions\n",
    "oof_lower_preds = np.zeros(len(X_for_quantile))\n",
    "oof_upper_preds = np.zeros(len(X_for_quantile))\n",
    "test_lower_preds = np.zeros(len(X_test_for_quantile))\n",
    "test_upper_preds = np.zeros(len(X_test_for_quantile))\n",
    "\n",
    "# Initialize the consistent StratifiedKFold splitter\n",
    "skf = StratifiedKFold(n_splits=N_SPLITS, shuffle=True, random_state=RANDOM_STATE)\n",
    "\n",
    "# Prepare the full test data as a DMatrix once, for efficiency\n",
    "dtest = xgb.DMatrix(X_test_for_quantile)\n",
    "\n",
    "# --- Step 2: K-Fold Training Loop with Native API ---\n",
    "for fold, (train_idx, val_idx) in enumerate(skf.split(X_for_quantile, grade_for_stratify)):\n",
    "    print(f\"\\n===== FOLD {fold+1}/{N_SPLITS} =====\")\n",
    "\n",
    "    # Split the data for this fold\n",
    "    X_train, X_val = X_for_quantile.iloc[train_idx], X_for_quantile.iloc[val_idx]\n",
    "    y_train, y_val = y_true.iloc[train_idx], y_true.iloc[val_idx]\n",
    "\n",
    "    # Convert fold data to DMatrix format\n",
    "    dtrain = xgb.DMatrix(X_train, label=y_train)\n",
    "    dval = xgb.DMatrix(X_val, label=y_val)\n",
    "\n",
    "    # --- Train and Predict Lower-Bound Model ---\n",
    "    print(\"  -> Training lower-bound model (alpha=0.05)...\")\n",
    "    # We remove 'n_estimators' as it's not a valid param for xgb.train\n",
    "    # It's controlled by num_boost_round instead.\n",
    "    num_boost_round_lower = best_params_lower.pop('n_estimators')\n",
    "    \n",
    "    lower_model = xgb.train(\n",
    "        params=best_params_lower,\n",
    "        dtrain=dtrain,\n",
    "        num_boost_round=num_boost_round_lower,\n",
    "        evals=[(dval, 'validation')],\n",
    "        early_stopping_rounds=50,\n",
    "        verbose_eval=False\n",
    "    )\n",
    "    \n",
    "    # Restore n_estimators for the next fold to avoid a pop error\n",
    "    best_params_lower['n_estimators'] = num_boost_round_lower\n",
    "    \n",
    "    # Predict on the validation set to generate OOF predictions\n",
    "    oof_lower_preds[val_idx] = lower_model.predict(dval, iteration_range=(0, lower_model.best_iteration))\n",
    "    # Predict on the test set and add to the running average\n",
    "    test_lower_preds += lower_model.predict(dtest, iteration_range=(0, lower_model.best_iteration)) / N_SPLITS\n",
    "\n",
    "    # --- Train and Predict Upper-Bound Model ---\n",
    "    print(\"  -> Training upper-bound model (alpha=0.95)...\")\n",
    "    num_boost_round_upper = best_params_upper.pop('n_estimators')\n",
    "\n",
    "    upper_model = xgb.train(\n",
    "        params=best_params_upper,\n",
    "        dtrain=dtrain,\n",
    "        num_boost_round=num_boost_round_upper,\n",
    "        evals=[(dval, 'validation')],\n",
    "        early_stopping_rounds=50,\n",
    "        verbose_eval=False\n",
    "    )\n",
    "\n",
    "    best_params_upper['n_estimators'] = num_boost_round_upper\n",
    "    \n",
    "    # Predict on the validation set for OOF\n",
    "    oof_upper_preds[val_idx] = upper_model.predict(dval, iteration_range=(0, upper_model.best_iteration))\n",
    "    # Predict on the test set and add to the running average\n",
    "    test_upper_preds += upper_model.predict(dtest, iteration_range=(0, upper_model.best_iteration)) / N_SPLITS\n",
    "\n",
    "    gc.collect()\n",
    "\n",
    "print(\"\\n--- K-Fold training complete. ---\")\n",
    "\n",
    "# --- Step 3: Save the Prediction Artifacts ---\n",
    "print(\"\\n--- Saving quantile prediction arrays to disk... ---\")\n",
    "\n",
    "np.save(os.path.join(META_QUANTILE_PATH, 'oof_lower_preds.npy'), oof_lower_preds)\n",
    "print(f\"Saved 'oof_lower_preds.npy' successfully.\")\n",
    "\n",
    "np.save(os.path.join(META_QUANTILE_PATH, 'test_lower_preds.npy'), test_lower_preds)\n",
    "print(f\"Saved 'test_lower_preds.npy' successfully.\")\n",
    "\n",
    "np.save(os.path.join(META_QUANTILE_PATH, 'oof_upper_preds.npy'), oof_upper_preds)\n",
    "print(f\"Saved 'oof_upper_preds.npy' successfully.\")\n",
    "\n",
    "np.save(os.path.join(META_QUANTILE_PATH, 'test_upper_preds.npy'), test_upper_preds)\n",
    "print(f\"Saved 'test_upper_preds.npy' successfully.\")\n",
    "\n",
    "print(\"\\nAll prediction artifacts are now ready for the final calibration and submission step.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e6a1b2ed-e304-4a00-ab91-fa2d3bb84334",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Loading all necessary prediction files ---\n",
      "All prediction artifacts loaded successfully.\n",
      "\n",
      "--- Recreating OOF interval bounds from base predictions ---\n",
      "\n",
      "--- Optimizing the blend weight for the two interval models ---\n",
      "\n",
      "============================================================\n",
      "FINAL ENSEMBLE RESULTS\n",
      "============================================================\n",
      "Original Error Model OOF Score:     $292,680.00\n",
      "Direct Quantile Model OOF Score:  $349,061.10\n",
      "Final BLENDED OOF Winkler Score:    $294,662.90\n",
      "------------------------------------------------------------\n",
      "Optimal Blend Weights:\n",
      "  -> Mean+Error Model:      93.15%\n",
      "  -> Direct Quantile Model: 6.85%\n",
      "\n",
      "--- Creating final blended submission file... ---\n",
      "\n",
      "'submission_FINAL_BLEND_294662.csv' created successfully! This is your best shot!\n",
      "\n",
      "Final Submission Head:\n",
      "       id       pi_lower      pi_upper\n",
      "0  200000  812841.025447  1.016490e+06\n",
      "1  200001  576953.492400  7.999085e+05\n",
      "2  200002  450015.580139  6.540748e+05\n",
      "3  200003  294411.784474  4.243998e+05\n",
      "4  200004  354569.920838  7.901194e+05\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# BLOCK 9 (Corrected): FINAL ENSEMBLE OF INTERVALS (Self-Contained)\n",
    "# =============================================================================\n",
    "# This is the definitive final step. We will blend the bounds from our two\n",
    "# best pipelines to create a final, robust submission.\n",
    "#\n",
    "# CORRECTION: This version now explicitly loads all required OOF prediction\n",
    "# arrays from disk to ensure it can run independently without NameErrors.\n",
    "\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy.optimize import minimize\n",
    "\n",
    "# --- Helper Function for Winkler Score (to make block self-contained) ---\n",
    "def winkler_score(y_true, lower, upper, alpha=0.1):\n",
    "    width = upper - lower\n",
    "    penalty_lower = np.where(y_true < lower, (2 / alpha) * (lower - y_true), 0)\n",
    "    penalty_upper = np.where(y_true > upper, (2 / alpha) * (y_true - upper), 0)\n",
    "    score = width + penalty_lower + penalty_upper\n",
    "    return np.mean(score)\n",
    "\n",
    "# --- Step 1: Load All Required Predictions from Disk ---\n",
    "print(\"\\n--- Loading all necessary prediction files ---\")\n",
    "\n",
    "# --- Define Paths ---\n",
    "DATA_PATH = './'\n",
    "PREDS_SAVE_PATH = './mean_models_v1/'\n",
    "NN_PREDS_PATH = './NN_model_predictions/'\n",
    "ERROR_MODELS_PATH = './error_models/'\n",
    "META_QUANTILE_PATH = './meta_quantile_models/'\n",
    "ERROR_MODEL_SUB_FILE = 'submission_final_OptimalEoE_292680.csv'\n",
    "QUANTILE_MODEL_SUB_FILE = 'submission_direct_quantile_robust_349061.csv'\n",
    "\n",
    "try:\n",
    "    # --- Load Test Set Bounds (from submission files) ---\n",
    "    df_error_model = pd.read_csv(ERROR_MODEL_SUB_FILE)\n",
    "    df_quantile_model = pd.read_csv(QUANTILE_MODEL_SUB_FILE)\n",
    "    test_lower_error = df_error_model['pi_lower'].values\n",
    "    test_upper_error = df_error_model['pi_upper'].values\n",
    "    test_lower_quantile = df_quantile_model['pi_lower'].values\n",
    "    test_upper_quantile = df_quantile_model['pi_upper'].values\n",
    "\n",
    "    # --- Load All OOF Predictions Needed for Recreation ---\n",
    "    # True labels\n",
    "    df_train = pd.read_csv(os.path.join(DATA_PATH, 'dataset.csv'))\n",
    "    y_true = df_train['sale_price'].values\n",
    "\n",
    "    # OOF Mean model predictions\n",
    "    oof_xgb_preds = np.load(os.path.join(PREDS_SAVE_PATH, 'oof_xgb_preds.npy'))\n",
    "    oof_cb_preds = np.load(os.path.join(PREDS_SAVE_PATH, 'oof_cb_preds.npy'))\n",
    "    oof_lgbm_preds = np.load(os.path.join(PREDS_SAVE_PATH, 'oof_lgbm_preds.npy'))\n",
    "    oof_nn_preds = np.load(os.path.join(NN_PREDS_PATH, 'oof_nn_preds.npy'))\n",
    "\n",
    "    # OOF Error model predictions\n",
    "    oof_error_preds_xgb = np.load(os.path.join(ERROR_MODELS_PATH, 'oof_error_preds_xgb.npy'))\n",
    "    oof_error_preds_cb = np.load(os.path.join(ERROR_MODELS_PATH, 'oof_error_preds_cb.npy'))\n",
    "\n",
    "    # OOF Quantile model predictions\n",
    "    oof_lower_preds = np.load(os.path.join(META_QUANTILE_PATH, 'oof_lower_preds.npy'))\n",
    "    oof_upper_preds = np.load(os.path.join(META_QUANTILE_PATH, 'oof_upper_preds.npy'))\n",
    "\n",
    "    print(\"All prediction artifacts loaded successfully.\")\n",
    "\n",
    "except FileNotFoundError as e:\n",
    "    print(f\"\\nERROR: Could not find a required file. {e}\")\n",
    "    print(\"Please ensure all previous training notebooks have been run successfully.\")\n",
    "    # exit()\n",
    "\n",
    "\n",
    "# --- Step 2: Recreate OOF Bounds for Both Pipelines ---\n",
    "print(\"\\n--- Recreating OOF interval bounds from base predictions ---\")\n",
    "\n",
    "# --- Pipeline 1: Mean+Error Model OOF Bounds ---\n",
    "oof_ensemble_mean = (oof_xgb_preds + oof_cb_preds + oof_lgbm_preds + oof_nn_preds) / 4\n",
    "best_a_error_model, best_b_error_model = 1.9799, 2.1755\n",
    "# Using the 60/40 blend weights from the original notebook\n",
    "oof_error_final = np.clip((oof_error_preds_xgb * 0.60 + oof_error_preds_cb * 0.40), 0, None)\n",
    "oof_lower_error = oof_ensemble_mean - oof_error_final * best_a_error_model\n",
    "oof_upper_error = oof_ensemble_mean + oof_error_final * best_b_error_model\n",
    "\n",
    "# --- Pipeline 2: Direct Quantile Model OOF Bounds ---\n",
    "best_a_quantile, best_b_quantile = 0.8118, 1.1960\n",
    "# Enforce non-crossing, as was done in the robust version\n",
    "oof_lower_quantile_raw = np.minimum(oof_lower_preds, oof_upper_preds)\n",
    "oof_upper_quantile_raw = np.maximum(oof_lower_preds, oof_upper_preds)\n",
    "oof_lower_quantile = oof_lower_quantile_raw * best_a_quantile\n",
    "oof_upper_quantile = oof_upper_quantile_raw * best_b_quantile\n",
    "\n",
    "\n",
    "# --- Step 3: Find the Optimal Blend Weight ---\n",
    "print(\"\\n--- Optimizing the blend weight for the two interval models ---\")\n",
    "\n",
    "def get_blended_winkler(weights, y_true_oof, lower_a, upper_a, lower_b, upper_b):\n",
    "    weight_a = weights[0]\n",
    "    weight_b = 1 - weight_a\n",
    "    final_lower = (lower_a * weight_a) + (lower_b * weight_b)\n",
    "    final_upper = (upper_a * weight_a) + (upper_b * weight_b)\n",
    "    return winkler_score(y_true_oof, final_lower, final_upper)\n",
    "\n",
    "initial_guess, bounds = [0.5], [(0, 1)]\n",
    "result_blend = minimize(\n",
    "    fun=get_blended_winkler,\n",
    "    x0=initial_guess,\n",
    "    args=(y_true, oof_lower_error, oof_upper_error, oof_lower_quantile, oof_upper_quantile),\n",
    "    method='L-BFGS-B',\n",
    "    bounds=bounds\n",
    ")\n",
    "\n",
    "best_weight_error_model = result_blend.x[0]\n",
    "best_weight_quantile_model = 1 - best_weight_error_model\n",
    "best_blended_score = result_blend.fun\n",
    "\n",
    "\n",
    "# --- Step 4: Display Final Results ---\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"FINAL ENSEMBLE RESULTS\")\n",
    "print(\"=\"*60)\n",
    "print(f\"Original Error Model OOF Score:     $292,680.00\") # From filename\n",
    "print(f\"Direct Quantile Model OOF Score:  $349,061.10\") # From filename\n",
    "print(f\"Final BLENDED OOF Winkler Score:    ${best_blended_score:,.2f}\")\n",
    "print(\"-\" * 60)\n",
    "print(f\"Optimal Blend Weights:\")\n",
    "print(f\"  -> Mean+Error Model:      {best_weight_error_model:.2%}\")\n",
    "print(f\"  -> Direct Quantile Model: {best_weight_quantile_model:.2%}\")\n",
    "\n",
    "\n",
    "# --- Step 5: Create and Save the Final Blended Submission ---\n",
    "print(\"\\n--- Creating final blended submission file... ---\")\n",
    "\n",
    "final_test_lower = (test_lower_error * best_weight_error_model) + (test_lower_quantile * best_weight_quantile_model)\n",
    "final_test_upper = (test_upper_error * best_weight_error_model) + (test_upper_quantile * best_weight_quantile_model)\n",
    "final_test_upper = np.maximum(final_test_lower + 1, final_test_upper)\n",
    "\n",
    "submission_df_final = pd.DataFrame({\n",
    "    'id': df_error_model['id'],\n",
    "    'pi_lower': final_test_lower,\n",
    "    'pi_upper': final_test_upper\n",
    "})\n",
    "\n",
    "submission_filename = f'submission_FINAL_BLEND_{int(best_blended_score)}.csv'\n",
    "submission_df_final.to_csv(submission_filename, index=False)\n",
    "\n",
    "print(f\"\\n'{submission_filename}' created successfully! This is your best shot!\")\n",
    "print(\"\\nFinal Submission Head:\")\n",
    "print(submission_df_final.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "335fddf3-c89a-46ff-8cb3-bb30507f8cf2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (Kaggle Comp)",
   "language": "python",
   "name": "kaggle-comp"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

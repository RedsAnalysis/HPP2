{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ba0c55c7-f25f-44ee-90cd-975b316adaed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Libraries imported successfully.\n",
      "Raw data loaded successfully.\n",
      "Setup complete.\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# BLOCK 1: SETUP, IMPORTS, AND DATA LOADING\n",
    "# =============================================================================\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "import time\n",
    "import os\n",
    "# --- Library Imports ---\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torch.optim.lr_scheduler import OneCycleLR\n",
    "from sklearn.model_selection import StratifiedKFold, train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import joblib\n",
    "import os\n",
    "import gc\n",
    "print(\"Libraries imported successfully.\")\n",
    "# --- Helper Function for Winkler Score ---\n",
    "def winkler_score(y_true, lower, upper, alpha=0.1, return_coverage=False):\n",
    "    width = upper - lower\n",
    "    penalty_lower = np.where(y_true < lower, (2 / alpha) * (lower - y_true), 0)\n",
    "    penalty_upper = np.where(y_true > upper, (2 / alpha) * (y_true - upper), 0)\n",
    "    score = width + penalty_lower + penalty_upper\n",
    "    if return_coverage:\n",
    "        coverage = np.mean((y_true >= lower) & (y_true <= upper))\n",
    "        return np.mean(score), coverage\n",
    "    return np.mean(score)\n",
    "# --- Global Constants ---\n",
    "N_SPLITS = 5\n",
    "RANDOM_STATE = 42\n",
    "DATA_PATH = './'\n",
    "N_OPTUNA_TRIALS = 30 # A strong number for a comprehensive search\n",
    "COMPETITION_ALPHA = 0.1\n",
    "\n",
    "# --- Load Raw Data ---\n",
    "try:\n",
    "    # We drop the low-variance columns they identified right away\n",
    "    drop_cols=['id', 'golf', 'view_rainier', 'view_skyline', 'view_lakesamm','view_otherwater', 'view_other']\n",
    "    df_train = pd.read_csv(DATA_PATH + 'dataset.csv').drop(columns=drop_cols)\n",
    "    df_test = pd.read_csv(DATA_PATH + 'test.csv').drop(columns=drop_cols)\n",
    "    print(\"Raw data loaded successfully.\")\n",
    "except FileNotFoundError:\n",
    "    print(\"ERROR: Could not find 'dataset.csv' or 'test.csv'.\")\n",
    "    exit()\n",
    "# --- Prepare Target Variable ---\n",
    "y_true = df_train['sale_price'].copy()\n",
    "grade_for_stratify = df_train['grade'].copy()\n",
    "# The mean-error model works best when predicting the raw price directly\n",
    "# So, we will NOT log-transform the target this time.\n",
    "# df_train.drop('sale_price', axis=1, inplace=True) # We keep sale_price for FE\n",
    "print(\"Setup complete.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "acdf2174-e398-432f-b37c-ec0dbfa1a584",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Starting Block 2.5: Executing Feature Engineering Pipeline ---\n",
      "--- Starting Comprehensive Feature Engineering ---\n",
      "Step 1: Creating brute-force numerical interaction features...\n",
      "Step 2: Creating date features...\n",
      "Step 3: Creating TF-IDF features for text columns...\n",
      "Step 4: Creating group-by aggregation features...\n",
      "Step 5: Creating ratio features...\n",
      "Step 6: Creating geospatial clustering features...\n",
      "Step 7: Finalizing feature set...\n",
      "\n",
      "Comprehensive FE complete. Total features: 233\n",
      "Feature engineering complete. X shape: (200000, 233), X_test shape: (200000, 233)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Make sure to have these libraries installed\n",
    "# pip install pandas numpy scikit-learn\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.cluster import KMeans\n",
    "import gc\n",
    "\n",
    "# Define a random state for reproducibility\n",
    "RANDOM_STATE = 42\n",
    "\n",
    "def create_comprehensive_features(df_train, df_test):\n",
    "    \"\"\"\n",
    "    Combines original and new advanced feature engineering steps into a single pipeline.\n",
    "    \"\"\"\n",
    "    print(\"--- Starting Comprehensive Feature Engineering ---\")\n",
    "\n",
    "    # Store original indices and target variable\n",
    "    train_ids = df_train.index\n",
    "    test_ids = df_test.index\n",
    "    y_train = df_train['sale_price'].copy() # Keep the target separate\n",
    "\n",
    "    # Combine for consistent processing\n",
    "    df_train_temp = df_train.drop(columns=['sale_price'])\n",
    "    all_data = pd.concat([df_train_temp, df_test], axis=0, ignore_index=True)\n",
    "\n",
    "    # --- Original Feature Engineering ---\n",
    "\n",
    "    # A) Brute-Force Numerical Interactions\n",
    "    print(\"Step 1: Creating brute-force numerical interaction features...\")\n",
    "    NUMS = ['area', 'land_val', 'imp_val', 'sqft_lot', 'sqft', 'sqft_1', 'grade', 'year_built']\n",
    "    # Ensure all columns exist and are numeric, fill missing with 0 for safety\n",
    "    for col in NUMS:\n",
    "        if col not in all_data.columns:\n",
    "            all_data[col] = 0\n",
    "        else:\n",
    "            all_data[col] = pd.to_numeric(all_data[col], errors='coerce').fillna(0)\n",
    "            \n",
    "    for i in range(len(NUMS)):\n",
    "        for j in range(i + 1, len(NUMS)):\n",
    "            all_data[f'{NUMS[i]}_x_{NUMS[j]}'] = all_data[NUMS[i]] * all_data[NUMS[j]]\n",
    "\n",
    "    # B) Date Features\n",
    "    print(\"Step 2: Creating date features...\")\n",
    "    all_data['sale_date'] = pd.to_datetime(all_data['sale_date'])\n",
    "    all_data['sale_year'] = all_data['sale_date'].dt.year\n",
    "    all_data['sale_month'] = all_data['sale_date'].dt.month\n",
    "    all_data['sale_dayofyear'] = all_data['sale_date'].dt.dayofyear\n",
    "    all_data['age_at_sale'] = all_data['sale_year'] - all_data['year_built']\n",
    "\n",
    "    # C) TF-IDF Text Features\n",
    "    print(\"Step 3: Creating TF-IDF features for text columns...\")\n",
    "    text_cols = ['subdivision', 'zoning', 'city', 'sale_warning', 'join_status', 'submarket']\n",
    "    all_data[text_cols] = all_data[text_cols].fillna('missing').astype(str)\n",
    "    \n",
    "    for col in text_cols:\n",
    "        tfidf = TfidfVectorizer(analyzer='char', ngram_range=(3, 5), max_features=128, binary=True)\n",
    "        svd = TruncatedSVD(n_components=8, random_state=RANDOM_STATE)\n",
    "        \n",
    "        tfidf_matrix = tfidf.fit_transform(all_data[col])\n",
    "        tfidf_svd = svd.fit_transform(tfidf_matrix)\n",
    "        \n",
    "        tfidf_df = pd.DataFrame(tfidf_svd, columns=[f'{col}_tfidf_svd_{i}' for i in range(8)])\n",
    "        all_data = pd.concat([all_data, tfidf_df], axis=1)\n",
    "\n",
    "    # D) Log transform some interaction features\n",
    "    for c in ['land_val_x_imp_val', 'land_val_x_sqft', 'imp_val_x_sqft']:\n",
    "        if c in all_data.columns:\n",
    "            all_data[c] = np.log1p(all_data[c].fillna(0))\n",
    "\n",
    "    # --- New Feature Engineering Ideas ---\n",
    "\n",
    "    # F) Group-By Aggregation Features\n",
    "    print(\"Step 4: Creating group-by aggregation features...\")\n",
    "    group_cols = ['submarket', 'city', 'zoning']\n",
    "    num_cols_for_agg = ['grade', 'sqft', 'imp_val', 'land_val', 'age_at_sale']\n",
    "\n",
    "    for group_col in group_cols:\n",
    "        for num_col in num_cols_for_agg:\n",
    "            agg_stats = all_data.groupby(group_col)[num_col].agg(['mean', 'std', 'max', 'min']).reset_index()\n",
    "            agg_stats.columns = [group_col] + [f'{group_col}_{num_col}_{stat}' for stat in ['mean', 'std', 'max', 'min']]\n",
    "            all_data = pd.merge(all_data, agg_stats, on=group_col, how='left')\n",
    "            all_data[f'{num_col}_minus_{group_col}_mean'] = all_data[num_col] - all_data[f'{group_col}_{num_col}_mean']\n",
    "\n",
    "    # G) Ratio Features\n",
    "    print(\"Step 5: Creating ratio features...\")\n",
    "    # Add a small epsilon to prevent division by zero\n",
    "    epsilon = 1e-6 \n",
    "    all_data['total_val'] = all_data['imp_val'] + all_data['land_val']\n",
    "    all_data['imp_val_to_land_val_ratio'] = all_data['imp_val'] / (all_data['land_val'] + epsilon)\n",
    "    all_data['land_val_ratio'] = all_data['land_val'] / (all_data['total_val'] + epsilon)\n",
    "    all_data['sqft_to_lot_ratio'] = all_data['sqft'] / (all_data['sqft_lot'] + epsilon)\n",
    "    all_data['was_renovated'] = (all_data['year_reno'] > 0).astype(int)\n",
    "    all_data['reno_age_at_sale'] = np.where(all_data['was_renovated'] == 1, all_data['sale_year'] - all_data['year_reno'], -1)\n",
    "\n",
    "    # H) Geospatial Clustering Features\n",
    "    print(\"Step 6: Creating geospatial clustering features...\")\n",
    "    coords = all_data[['latitude', 'longitude']].copy()\n",
    "    coords.fillna(coords.median(), inplace=True) # Simple imputation\n",
    "\n",
    "    # KMeans is sensitive to feature scaling, but for lat/lon it's often okay without it.\n",
    "    kmeans = KMeans(n_clusters=20, random_state=RANDOM_STATE, n_init=10) \n",
    "    all_data['location_cluster'] = kmeans.fit_predict(coords)\n",
    "    \n",
    "    # Calculate distance to each cluster center\n",
    "    cluster_centers = kmeans.cluster_centers_\n",
    "    for i in range(len(cluster_centers)):\n",
    "        center = cluster_centers[i]\n",
    "        all_data[f'dist_to_cluster_{i}'] = np.sqrt((coords['latitude'] - center[0])**2 + (coords['longitude'] - center[1])**2)\n",
    "\n",
    "    # --- Final Cleanup ---\n",
    "    print(\"Step 7: Finalizing feature set...\")\n",
    "    cols_to_drop = ['sale_date', 'subdivision', 'zoning', 'city', 'sale_warning', 'join_status', 'submarket']\n",
    "    all_data = all_data.drop(columns=cols_to_drop)\n",
    "\n",
    "    # One-hot encode the new cluster feature\n",
    "    all_data = pd.get_dummies(all_data, columns=['location_cluster'], prefix='loc_cluster')\n",
    "    \n",
    "    # Final check for any remaining object columns to be safe (besides index)\n",
    "    object_cols = all_data.select_dtypes(include='object').columns\n",
    "    if len(object_cols) > 0:\n",
    "        print(f\"Warning: Found unexpected object columns: {object_cols}. Dropping them.\")\n",
    "        all_data = all_data.drop(columns=object_cols)\n",
    "        \n",
    "    all_data.fillna(0, inplace=True)\n",
    "\n",
    "    # Separate back into train and test sets\n",
    "    train_len = len(train_ids)\n",
    "    X = all_data.iloc[:train_len].copy()\n",
    "    X_test = all_data.iloc[train_len:].copy()\n",
    "    \n",
    "    # Restore original indices\n",
    "    X.index = train_ids\n",
    "    X_test.index = test_ids\n",
    "    \n",
    "    # Align columns - crucial for model prediction\n",
    "    X_test = X_test[X.columns]\n",
    "    \n",
    "    print(f\"\\nComprehensive FE complete. Total features: {X.shape[1]}\")\n",
    "    gc.collect()\n",
    "    \n",
    "    return X, X_test, y_train\n",
    "# =============================================================================\n",
    "# BLOCK 2.5: EXECUTE FEATURE ENGINEERING\n",
    "# =============================================================================\n",
    "print(\"\\n--- Starting Block 2.5: Executing Feature Engineering Pipeline ---\")\n",
    "\n",
    "# This is the crucial step that was missing.\n",
    "# We call the function to create our training and testing dataframes.\n",
    "X, X_test, y_train = create_comprehensive_features(df_train, df_test)\n",
    "\n",
    "# Let's verify the output\n",
    "print(f\"Feature engineering complete. X shape: {X.shape}, X_test shape: {X_test.shape}\")\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c31c4b18-16a0-4afc-8d8a-10959546ea38",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Starting Block 3: PyTorch Setup & Data Preparation ---\n",
      "PyTorch version: 2.7.1+cu126\n",
      "Using device: cuda\n",
      "\n",
      "Scaling features and target variable for the Neural Network...\n",
      "PyTorch setup and full data scaling complete.\n",
      "\n",
      "--- Calculating the scaled value for a zero price ---\n",
      "The scaled value of a $0 house price is: -1.4006\n",
      "This will be used as the minimum clamp value in our neural network.\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# BLOCK 3: PYTORCH SETUP & FULL DATA PREPARATION\n",
    "# =============================================================================\n",
    "\n",
    "\n",
    "print(f\"--- Starting Block 3: PyTorch Setup & Data Preparation ---\")\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# Data Scaling\n",
    "print(\"\\nScaling features and target variable for the Neural Network...\")\n",
    "feature_scaler = StandardScaler()\n",
    "target_scaler = StandardScaler()\n",
    "X_scaled = feature_scaler.fit_transform(X)\n",
    "X_test_scaled = feature_scaler.transform(X_test)\n",
    "y_true_scaled = target_scaler.fit_transform(y_true.to_numpy().reshape(-1, 1))\n",
    "\n",
    "# Custom PyTorch Dataset\n",
    "class HousePriceDataset(Dataset):\n",
    "    def __init__(self, features, labels=None):\n",
    "        self.features = features\n",
    "        self.labels = labels\n",
    "    def __len__(self): return len(self.features)\n",
    "    def __getitem__(self, idx):\n",
    "        features = torch.tensor(self.features[idx], dtype=torch.float32)\n",
    "        if self.labels is not None:\n",
    "            labels = torch.tensor(self.labels[idx], dtype=torch.float32)\n",
    "            return features, labels\n",
    "        return features\n",
    "        \n",
    "print(\"PyTorch setup and full data scaling complete.\")\n",
    "\n",
    "# =============================================================================\n",
    "# BLOCK 3.5: CALCULATE THE SCALED ZERO THRESHOLD\n",
    "# =============================================================================\n",
    "print(\"\\n--- Calculating the scaled value for a zero price ---\")\n",
    "\n",
    "# We transform the value 0 using the FITTED target_scaler.\n",
    "# This gives us the exact value on the scaled distribution that corresponds to $0.\n",
    "scaled_zero_threshold = target_scaler.transform(np.array([[0]]))[0, 0]\n",
    "\n",
    "print(f\"The scaled value of a $0 house price is: {scaled_zero_threshold:.4f}\")\n",
    "print(\"This will be used as the minimum clamp value in our neural network.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdca0995-5c79-4870-bca2-f0e47212621e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5e8c4d59-8989-4146-97c2-458dc020c263",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "STABILIZED Residual Neural Network architecture defined successfully.\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# BLOCK 4: DEFINE THE STABILIZED RESIDUAL NEURAL NETWORK\n",
    "# =============================================================================\n",
    "class ResidualBlock(nn.Module):\n",
    "    def __init__(self, input_size, output_size, dropout_rate):\n",
    "        super(ResidualBlock, self).__init__()\n",
    "        self.main_path = nn.Sequential(\n",
    "            nn.Linear(input_size, output_size),\n",
    "            nn.BatchNorm1d(output_size),\n",
    "            nn.SiLU(),\n",
    "            nn.Dropout(dropout_rate)\n",
    "        )\n",
    "        self.shortcut = nn.Identity() if input_size == output_size else nn.Linear(input_size, output_size)\n",
    "    def forward(self, x): return self.main_path(x) + self.shortcut(x)\n",
    "\n",
    "class ResidualNet(nn.Module):\n",
    "    def __init__(self, input_shape, layer_sizes, dropout_rates):\n",
    "        super(ResidualNet, self).__init__()\n",
    "        layers = [nn.Linear(input_shape, layer_sizes[0]), nn.SiLU()]\n",
    "        in_size = layer_sizes[0]\n",
    "        for out_size, dropout in zip(layer_sizes, dropout_rates):\n",
    "            layers.append(ResidualBlock(in_size, out_size, dropout))\n",
    "            in_size = out_size\n",
    "        layers.append(nn.Linear(in_size, 1))\n",
    "        self.model = nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        output = self.model(x)\n",
    "        # CRITICAL FIX #1: OUTPUT CLAMPING\n",
    "        # Prevents the model from predicting impossibly large or small scaled values.\n",
    "        return torch.clamp(output, min=scaled_zero_threshold, max=5.0)\n",
    "\n",
    "print(\"STABILIZED Residual Neural Network architecture defined successfully.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2f1b5d79-c3bd-442b-93c7-bb806eef4418",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Step 1: Preparing data for faster Optuna tuning... ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-07-24 03:02:32,343] A new study created in memory with name: no-name-3e9886d0-26af-4c0a-a251-f29a84d52c92\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data prepared. Training set size: 160000, Validation set size: 40000\n",
      "\n",
      "--- Starting Neural Network Hyperparameter Tuning... ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-07-24 03:04:26,426] Trial 0 finished with value: 0.03391383343105075 and parameters: {'architecture': 'arch_1', 'dr_0': 0.45033774156523054, 'dr_1': 0.4308754629223541, 'dr_2': 0.3197897302274969, 'learning_rate': 0.00019326391470813071, 'weight_decay': 1.3551798292603313e-06}. Best is trial 0 with value: 0.03391383343105075.\n",
      "[I 2025-07-24 03:06:16,361] Trial 1 finished with value: 0.032120370577219164 and parameters: {'architecture': 'arch_2', 'dr_0': 0.3290716352055359, 'dr_1': 0.4791581967339372, 'dr_2': 0.21039946687518546, 'learning_rate': 0.0025080635391938055, 'weight_decay': 1.9879667167539206e-05}. Best is trial 1 with value: 0.032120370577219164.\n",
      "[I 2025-07-24 03:08:05,530] Trial 2 finished with value: 0.03108148566812654 and parameters: {'architecture': 'arch_1', 'dr_0': 0.4948646326319316, 'dr_1': 0.24635200513006292, 'dr_2': 0.3384175725914985, 'learning_rate': 0.002060608168544838, 'weight_decay': 0.00036745069199052436}. Best is trial 2 with value: 0.03108148566812654.\n",
      "[I 2025-07-24 03:09:54,238] Trial 3 finished with value: 0.03167844674538208 and parameters: {'architecture': 'arch_1', 'dr_0': 0.30700709653365427, 'dr_1': 0.11292666946832736, 'dr_2': 0.42640915372868593, 'learning_rate': 0.0003993095482865803, 'weight_decay': 0.00024619794315442725}. Best is trial 2 with value: 0.03108148566812654.\n",
      "[I 2025-07-24 03:10:36,395] Trial 4 finished with value: 0.041374052933688406 and parameters: {'architecture': 'arch_1', 'dr_0': 0.30327104550832856, 'dr_1': 0.24579712206742738, 'dr_2': 0.3612068358428897, 'learning_rate': 0.007747770879773587, 'weight_decay': 4.9723756562338946e-05}. Best is trial 2 with value: 0.03108148566812654.\n",
      "[I 2025-07-24 03:12:27,509] Trial 5 finished with value: 0.03154401995147331 and parameters: {'architecture': 'arch_1', 'dr_0': 0.2621637572126748, 'dr_1': 0.19770120085203785, 'dr_2': 0.10492671702672696, 'learning_rate': 0.0020366522669932313, 'weight_decay': 9.268866801139451e-05}. Best is trial 2 with value: 0.03108148566812654.\n",
      "[I 2025-07-24 03:14:14,864] Trial 6 finished with value: 0.031941113757747636 and parameters: {'architecture': 'arch_2', 'dr_0': 0.4401809617992555, 'dr_1': 0.23848646663319092, 'dr_2': 0.22460563246885484, 'learning_rate': 0.005116776039553822, 'weight_decay': 3.7203132053601056e-06}. Best is trial 2 with value: 0.03108148566812654.\n",
      "[I 2025-07-24 03:16:06,212] Trial 7 finished with value: 0.031514318683479405 and parameters: {'architecture': 'arch_1', 'dr_0': 0.3331430674238692, 'dr_1': 0.31841261714947394, 'dr_2': 0.15043658237634983, 'learning_rate': 0.002154143433858429, 'weight_decay': 5.7844687446343315e-06}. Best is trial 2 with value: 0.03108148566812654.\n",
      "[I 2025-07-24 03:16:41,548] Trial 8 finished with value: 0.043547731788852545 and parameters: {'architecture': 'arch_2', 'dr_0': 0.35503968422638155, 'dr_1': 0.19967233873119627, 'dr_2': 0.24158273899001978, 'learning_rate': 0.006738404142276755, 'weight_decay': 0.0002944439250901214}. Best is trial 2 with value: 0.03108148566812654.\n",
      "[I 2025-07-24 03:18:32,972] Trial 9 finished with value: 0.0325597801681938 and parameters: {'architecture': 'arch_2', 'dr_0': 0.3982474510147881, 'dr_1': 0.21505106442307165, 'dr_2': 0.2529672533997753, 'learning_rate': 0.0001986434706497893, 'weight_decay': 0.0007623597467049318}. Best is trial 2 with value: 0.03108148566812654.\n",
      "[I 2025-07-24 03:20:21,965] Trial 10 finished with value: 0.03185108726066124 and parameters: {'architecture': 'arch_1', 'dr_0': 0.13204211893238746, 'dr_1': 0.34753854044731447, 'dr_2': 0.467494504199118, 'learning_rate': 0.0006997871165913141, 'weight_decay': 0.0005091902821459096}. Best is trial 2 with value: 0.03108148566812654.\n",
      "[I 2025-07-24 03:22:12,066] Trial 11 finished with value: 0.03137917071580887 and parameters: {'architecture': 'arch_1', 'dr_0': 0.20529516225592742, 'dr_1': 0.3457883958182173, 'dr_2': 0.11896965449702325, 'learning_rate': 0.0017050006957658535, 'weight_decay': 1.1521972536376742e-05}. Best is trial 2 with value: 0.03108148566812654.\n",
      "[I 2025-07-24 03:24:06,285] Trial 12 finished with value: 0.031185783280790608 and parameters: {'architecture': 'arch_1', 'dr_0': 0.20591000973501628, 'dr_1': 0.3801496523516941, 'dr_2': 0.3476313065984914, 'learning_rate': 0.0009863342306923708, 'weight_decay': 1.354418202453107e-05}. Best is trial 2 with value: 0.03108148566812654.\n",
      "[I 2025-07-24 03:25:56,428] Trial 13 finished with value: 0.03152033059468752 and parameters: {'architecture': 'arch_1', 'dr_0': 0.49911966521750617, 'dr_1': 0.3912209278156859, 'dr_2': 0.37438328845680474, 'learning_rate': 0.0008220371048332689, 'weight_decay': 8.255345040670287e-05}. Best is trial 2 with value: 0.03108148566812654.\n",
      "[I 2025-07-24 03:27:49,044] Trial 14 finished with value: 0.03191776375604581 and parameters: {'architecture': 'arch_1', 'dr_0': 0.2096095494452381, 'dr_1': 0.2769791728334715, 'dr_2': 0.3067802357751873, 'learning_rate': 0.0004507459087239892, 'weight_decay': 2.754718570302627e-05}. Best is trial 2 with value: 0.03108148566812654.\n",
      "[I 2025-07-24 03:29:44,503] Trial 15 finished with value: 0.03085115426986278 and parameters: {'architecture': 'arch_1', 'dr_0': 0.1536850687212265, 'dr_1': 0.13395051899643365, 'dr_2': 0.3869295220018707, 'learning_rate': 0.0012139842735076395, 'weight_decay': 0.0001349274083891927}. Best is trial 15 with value: 0.03085115426986278.\n",
      "[I 2025-07-24 03:31:35,132] Trial 16 finished with value: 0.03179563492347923 and parameters: {'architecture': 'arch_1', 'dr_0': 0.13532033903486967, 'dr_1': 0.13047994379373634, 'dr_2': 0.42132496290368576, 'learning_rate': 0.003663902536160782, 'weight_decay': 0.0001598790892093993}. Best is trial 15 with value: 0.03085115426986278.\n",
      "[I 2025-07-24 03:33:26,209] Trial 17 finished with value: 0.0316422697675379 and parameters: {'architecture': 'arch_1', 'dr_0': 0.11183941865599986, 'dr_1': 0.15095021987595178, 'dr_2': 0.4016326353702677, 'learning_rate': 0.0015205694428175333, 'weight_decay': 0.0007173059289047061}. Best is trial 15 with value: 0.03085115426986278.\n",
      "[I 2025-07-24 03:35:18,172] Trial 18 finished with value: 0.033393018772896334 and parameters: {'architecture': 'arch_2', 'dr_0': 0.26242667537465253, 'dr_1': 0.16488467720007313, 'dr_2': 0.46118672315934073, 'learning_rate': 0.00011517041994355633, 'weight_decay': 0.0001579528520036006}. Best is trial 15 with value: 0.03085115426986278.\n",
      "[I 2025-07-24 03:36:54,739] Trial 19 finished with value: 0.03179081463361088 and parameters: {'architecture': 'arch_1', 'dr_0': 0.16683240879738065, 'dr_1': 0.282760930622961, 'dr_2': 0.4959402093021798, 'learning_rate': 0.0034600181169787813, 'weight_decay': 6.647315883927183e-05}. Best is trial 15 with value: 0.03085115426986278.\n",
      "[I 2025-07-24 03:38:44,885] Trial 20 finished with value: 0.030638271400445623 and parameters: {'architecture': 'arch_1', 'dr_0': 0.38443081869365026, 'dr_1': 0.10602597658019036, 'dr_2': 0.2720140566239113, 'learning_rate': 0.0012542922910471635, 'weight_decay': 0.00031212849597841833}. Best is trial 20 with value: 0.030638271400445623.\n",
      "[I 2025-07-24 03:40:40,318] Trial 21 finished with value: 0.030305643200497084 and parameters: {'architecture': 'arch_1', 'dr_0': 0.3873474539941557, 'dr_1': 0.11142102597607889, 'dr_2': 0.2828014964018199, 'learning_rate': 0.0012571166172228982, 'weight_decay': 0.00037736315423263645}. Best is trial 21 with value: 0.030305643200497084.\n",
      "[I 2025-07-24 03:42:32,539] Trial 22 finished with value: 0.03066135289831252 and parameters: {'architecture': 'arch_1', 'dr_0': 0.38499978166277704, 'dr_1': 0.11665471611751899, 'dr_2': 0.2699657736675855, 'learning_rate': 0.001286337066725311, 'weight_decay': 0.00018500988012408917}. Best is trial 21 with value: 0.030305643200497084.\n",
      "[I 2025-07-24 03:44:23,387] Trial 23 finished with value: 0.0312659940859185 and parameters: {'architecture': 'arch_1', 'dr_0': 0.38082519730500464, 'dr_1': 0.1048774837450199, 'dr_2': 0.27134639172773506, 'learning_rate': 0.0005446337842536985, 'weight_decay': 0.0009032052225483858}. Best is trial 21 with value: 0.030305643200497084.\n",
      "[I 2025-07-24 03:46:17,417] Trial 24 finished with value: 0.030588972370458555 and parameters: {'architecture': 'arch_1', 'dr_0': 0.4105832392374096, 'dr_1': 0.16724036753642335, 'dr_2': 0.1881499826101542, 'learning_rate': 0.0010317095770669077, 'weight_decay': 0.00038723222565392436}. Best is trial 21 with value: 0.030305643200497084.\n",
      "[I 2025-07-24 03:48:11,042] Trial 25 finished with value: 0.03269067467956603 and parameters: {'architecture': 'arch_1', 'dr_0': 0.43367269119444946, 'dr_1': 0.16751496833906585, 'dr_2': 0.1860260159751829, 'learning_rate': 0.0002802840384004697, 'weight_decay': 0.00043771228152017353}. Best is trial 21 with value: 0.030305643200497084.\n",
      "[I 2025-07-24 03:49:59,357] Trial 26 finished with value: 0.031018641411880904 and parameters: {'architecture': 'arch_2', 'dr_0': 0.4140413664116508, 'dr_1': 0.1751939860347828, 'dr_2': 0.1782556041610856, 'learning_rate': 0.0006851397810104117, 'weight_decay': 4.27479311194745e-05}. Best is trial 21 with value: 0.030305643200497084.\n",
      "[I 2025-07-24 03:51:53,168] Trial 27 finished with value: 0.030480908677925037 and parameters: {'architecture': 'arch_1', 'dr_0': 0.3621371961547997, 'dr_1': 0.14272139145136034, 'dr_2': 0.28702047786784396, 'learning_rate': 0.0010303141731055016, 'weight_decay': 0.0004984719727029996}. Best is trial 21 with value: 0.030305643200497084.\n",
      "[I 2025-07-24 03:53:46,476] Trial 28 finished with value: 0.03083034458605549 and parameters: {'architecture': 'arch_1', 'dr_0': 0.46235588449501624, 'dr_1': 0.18400223939306837, 'dr_2': 0.294929339032319, 'learning_rate': 0.0009144572955792694, 'weight_decay': 0.0004973288905244188}. Best is trial 21 with value: 0.030305643200497084.\n",
      "[I 2025-07-24 03:55:36,063] Trial 29 finished with value: 0.03210317015742199 and parameters: {'architecture': 'arch_1', 'dr_0': 0.35520466995112243, 'dr_1': 0.15157119626212429, 'dr_2': 0.3178922948540512, 'learning_rate': 0.00030533985659227016, 'weight_decay': 0.0009512879042508258}. Best is trial 21 with value: 0.030305643200497084.\n",
      "[I 2025-07-24 03:57:24,165] Trial 30 finished with value: 0.03160775462283364 and parameters: {'architecture': 'arch_1', 'dr_0': 0.26903280052230844, 'dr_1': 0.14275870719032338, 'dr_2': 0.2006302571183447, 'learning_rate': 0.00332421778698847, 'weight_decay': 0.0001099123788968109}. Best is trial 21 with value: 0.030305643200497084.\n",
      "[I 2025-07-24 03:59:19,615] Trial 31 finished with value: 0.030996408384246162 and parameters: {'architecture': 'arch_1', 'dr_0': 0.36649199602761245, 'dr_1': 0.10554583838696036, 'dr_2': 0.2838246954240937, 'learning_rate': 0.0011782154666966675, 'weight_decay': 0.00024089252647047115}. Best is trial 21 with value: 0.030305643200497084.\n",
      "[I 2025-07-24 04:01:09,494] Trial 32 finished with value: 0.03112664389648015 and parameters: {'architecture': 'arch_1', 'dr_0': 0.4129163811823141, 'dr_1': 0.13181685710472169, 'dr_2': 0.23589130840307582, 'learning_rate': 0.0005840648873627406, 'weight_decay': 0.0005431740801407431}. Best is trial 21 with value: 0.030305643200497084.\n",
      "[I 2025-07-24 04:01:53,033] Trial 33 finished with value: 0.038580747698492644 and parameters: {'architecture': 'arch_1', 'dr_0': 0.46708655334464744, 'dr_1': 0.4875200275541904, 'dr_2': 0.33067982595517825, 'learning_rate': 0.0014984924905398003, 'weight_decay': 0.0003290898701515614}. Best is trial 21 with value: 0.030305643200497084.\n",
      "[I 2025-07-24 04:03:46,312] Trial 34 finished with value: 0.03111141646587396 and parameters: {'architecture': 'arch_1', 'dr_0': 0.3244243256259959, 'dr_1': 0.10384975619164843, 'dr_2': 0.14423802834971447, 'learning_rate': 0.002659273161645659, 'weight_decay': 0.0002288952776407491}. Best is trial 21 with value: 0.030305643200497084.\n",
      "[I 2025-07-24 04:05:39,725] Trial 35 finished with value: 0.030907581242013583 and parameters: {'architecture': 'arch_1', 'dr_0': 0.418582216256297, 'dr_1': 0.20803862067494322, 'dr_2': 0.21062705433779108, 'learning_rate': 0.0008424349788548314, 'weight_decay': 1.1382031664599387e-06}. Best is trial 21 with value: 0.030305643200497084.\n",
      "[I 2025-07-24 04:07:25,791] Trial 36 finished with value: 0.03156596735779998 and parameters: {'architecture': 'arch_2', 'dr_0': 0.33755077105307874, 'dr_1': 0.2282510709605416, 'dr_2': 0.2559410196389236, 'learning_rate': 0.0017330608778882224, 'weight_decay': 0.0003572146476834442}. Best is trial 21 with value: 0.030305643200497084.\n",
      "[I 2025-07-24 04:09:16,217] Trial 37 finished with value: 0.03086738300191451 and parameters: {'architecture': 'arch_1', 'dr_0': 0.39291554129794826, 'dr_1': 0.26452105395360687, 'dr_2': 0.2972819900666677, 'learning_rate': 0.001142464552292089, 'weight_decay': 0.0005999290921767653}. Best is trial 21 with value: 0.030305643200497084.\n",
      "[I 2025-07-24 04:11:12,494] Trial 38 finished with value: 0.03169683015704909 and parameters: {'architecture': 'arch_1', 'dr_0': 0.30073684149522895, 'dr_1': 0.44360456576610163, 'dr_2': 0.1634073282686915, 'learning_rate': 0.0026371279431363207, 'weight_decay': 0.00031517290882983437}. Best is trial 21 with value: 0.030305643200497084.\n",
      "[I 2025-07-24 04:13:02,405] Trial 39 finished with value: 0.03220719150915931 and parameters: {'architecture': 'arch_1', 'dr_0': 0.47240710957841814, 'dr_1': 0.17875767454358937, 'dr_2': 0.2270720943659182, 'learning_rate': 0.0003662904968054237, 'weight_decay': 2.0808833311550723e-06}. Best is trial 21 with value: 0.030305643200497084.\n",
      "[I 2025-07-24 04:14:42,701] Trial 40 finished with value: 0.03141204029604604 and parameters: {'architecture': 'arch_2', 'dr_0': 0.36721113573711656, 'dr_1': 0.1224405578155273, 'dr_2': 0.3376128139581075, 'learning_rate': 0.0020898788388934012, 'weight_decay': 0.00022095868507247715}. Best is trial 21 with value: 0.030305643200497084.\n",
      "[I 2025-07-24 04:16:33,035] Trial 41 finished with value: 0.03026889075961294 and parameters: {'architecture': 'arch_1', 'dr_0': 0.3897635076916713, 'dr_1': 0.12176356784582548, 'dr_2': 0.2696972548349174, 'learning_rate': 0.001272196996939945, 'weight_decay': 0.00020557125870761814}. Best is trial 41 with value: 0.03026889075961294.\n",
      "[I 2025-07-24 04:18:24,723] Trial 42 finished with value: 0.030513463706909855 and parameters: {'architecture': 'arch_1', 'dr_0': 0.4398624832229432, 'dr_1': 0.1573115550777812, 'dr_2': 0.26908285806621546, 'learning_rate': 0.0014331836118340745, 'weight_decay': 0.0003524991679507557}. Best is trial 41 with value: 0.03026889075961294.\n",
      "[I 2025-07-24 04:20:17,326] Trial 43 finished with value: 0.030991690987838976 and parameters: {'architecture': 'arch_1', 'dr_0': 0.4440558295460291, 'dr_1': 0.15708056463364553, 'dr_2': 0.24969115037742134, 'learning_rate': 0.0007154492290437925, 'weight_decay': 0.00043177430001207384}. Best is trial 41 with value: 0.03026889075961294.\n",
      "[I 2025-07-24 04:22:09,230] Trial 44 finished with value: 0.030911685073677496 and parameters: {'architecture': 'arch_1', 'dr_0': 0.4309416877031663, 'dr_1': 0.22224763268758577, 'dr_2': 0.30836760295439886, 'learning_rate': 0.0009998293043520256, 'weight_decay': 0.0006782594297809559}. Best is trial 41 with value: 0.03026889075961294.\n",
      "[I 2025-07-24 04:23:59,462] Trial 45 finished with value: 0.03089905921605569 and parameters: {'architecture': 'arch_1', 'dr_0': 0.40577957665029696, 'dr_1': 0.1844963851941807, 'dr_2': 0.20895162111122667, 'learning_rate': 0.0019586645583352705, 'weight_decay': 7.747036359151474e-05}. Best is trial 41 with value: 0.03026889075961294.\n",
      "[I 2025-07-24 04:24:35,268] Trial 46 finished with value: 0.041910266692313965 and parameters: {'architecture': 'arch_1', 'dr_0': 0.3489585507354167, 'dr_1': 0.19352608411912783, 'dr_2': 0.349648670491425, 'learning_rate': 0.009687371748899826, 'weight_decay': 0.0009976031238381024}. Best is trial 41 with value: 0.03026889075961294.\n",
      "[I 2025-07-24 04:26:25,237] Trial 47 finished with value: 0.030481773676185667 and parameters: {'architecture': 'arch_1', 'dr_0': 0.4534264000273435, 'dr_1': 0.14048032355080897, 'dr_2': 0.26193331234063216, 'learning_rate': 0.0014411450995352026, 'weight_decay': 0.00012878151626829074}. Best is trial 41 with value: 0.03026889075961294.\n",
      "[I 2025-07-24 04:28:17,600] Trial 48 finished with value: 0.031371086647238915 and parameters: {'architecture': 'arch_2', 'dr_0': 0.48308232036123744, 'dr_1': 0.14033833109790206, 'dr_2': 0.2827259843038672, 'learning_rate': 0.001441525065501968, 'weight_decay': 0.00011715018995739887}. Best is trial 41 with value: 0.03026889075961294.\n",
      "[I 2025-07-24 04:30:05,588] Trial 49 finished with value: 0.031521682002687755 and parameters: {'architecture': 'arch_1', 'dr_0': 0.3189646383015744, 'dr_1': 0.11991278565263974, 'dr_2': 0.3223635051235402, 'learning_rate': 0.002521275337011875, 'weight_decay': 0.00019379691632916837}. Best is trial 41 with value: 0.03026889075961294.\n",
      "[I 2025-07-24 04:31:53,751] Trial 50 finished with value: 0.03101890001304542 and parameters: {'architecture': 'arch_1', 'dr_0': 0.4499070157887657, 'dr_1': 0.1454493144120725, 'dr_2': 0.25864047164257586, 'learning_rate': 0.004585838757002899, 'weight_decay': 4.8512973442223366e-05}. Best is trial 41 with value: 0.03026889075961294.\n",
      "[I 2025-07-24 04:33:44,179] Trial 51 finished with value: 0.03093423194523099 and parameters: {'architecture': 'arch_1', 'dr_0': 0.42370656951074753, 'dr_1': 0.16443820798915415, 'dr_2': 0.22901468271774617, 'learning_rate': 0.0017555372273714452, 'weight_decay': 0.00040720283586572457}. Best is trial 41 with value: 0.03026889075961294.\n",
      "[I 2025-07-24 04:35:35,677] Trial 52 finished with value: 0.030483532414028916 and parameters: {'architecture': 'arch_1', 'dr_0': 0.40227746421918065, 'dr_1': 0.13346924200296476, 'dr_2': 0.2913306155936727, 'learning_rate': 0.0010204243968750563, 'weight_decay': 0.00026031572769096684}. Best is trial 41 with value: 0.03026889075961294.\n",
      "[I 2025-07-24 04:37:26,409] Trial 53 finished with value: 0.03118289153598532 and parameters: {'architecture': 'arch_1', 'dr_0': 0.45401165570825197, 'dr_1': 0.1313356001011961, 'dr_2': 0.2878486480300621, 'learning_rate': 0.0007823993750822828, 'weight_decay': 0.000271934499100786}. Best is trial 41 with value: 0.03026889075961294.\n",
      "[I 2025-07-24 04:39:16,292] Trial 54 finished with value: 0.03067233257844478 and parameters: {'architecture': 'arch_1', 'dr_0': 0.48535434521817894, 'dr_1': 0.12102577040944931, 'dr_2': 0.312335438261612, 'learning_rate': 0.001394100479667168, 'weight_decay': 0.0001549179601849833}. Best is trial 41 with value: 0.03026889075961294.\n",
      "[I 2025-07-24 04:41:05,248] Trial 55 finished with value: 0.03168677383019954 and parameters: {'architecture': 'arch_1', 'dr_0': 0.3708483237215741, 'dr_1': 0.2039251117975062, 'dr_2': 0.2674046185845814, 'learning_rate': 0.0006118066095757771, 'weight_decay': 0.0001036999488697928}. Best is trial 41 with value: 0.03026889075961294.\n",
      "[I 2025-07-24 04:42:57,968] Trial 56 finished with value: 0.03049510073718391 and parameters: {'architecture': 'arch_1', 'dr_0': 0.2831843963266859, 'dr_1': 0.1003327290351658, 'dr_2': 0.2453706864079554, 'learning_rate': 0.0010492980148678304, 'weight_decay': 2.0572650050614217e-05}. Best is trial 41 with value: 0.03026889075961294.\n",
      "[I 2025-07-24 04:44:49,466] Trial 57 finished with value: 0.031325369059473654 and parameters: {'architecture': 'arch_1', 'dr_0': 0.2756784409292878, 'dr_1': 0.12171067514565846, 'dr_2': 0.24369869751843057, 'learning_rate': 0.00047162274144496016, 'weight_decay': 1.751398133919537e-05}. Best is trial 41 with value: 0.03026889075961294.\n",
      "[I 2025-07-24 04:46:38,850] Trial 58 finished with value: 0.03075891230962699 and parameters: {'architecture': 'arch_1', 'dr_0': 0.2852793199094726, 'dr_1': 0.10445386932910045, 'dr_2': 0.3750098236559087, 'learning_rate': 0.0010415622623346176, 'weight_decay': 8.032838234728824e-06}. Best is trial 41 with value: 0.03026889075961294.\n",
      "[I 2025-07-24 04:48:27,674] Trial 59 finished with value: 0.031278939375394506 and parameters: {'architecture': 'arch_2', 'dr_0': 0.22242824448605727, 'dr_1': 0.14095362717624846, 'dr_2': 0.22081710188645443, 'learning_rate': 0.0009075501111275237, 'weight_decay': 2.910695949558436e-05}. Best is trial 41 with value: 0.03026889075961294.\n",
      "[I 2025-07-24 04:50:17,598] Trial 60 finished with value: 0.0314301059734595 and parameters: {'architecture': 'arch_1', 'dr_0': 0.23797289725063459, 'dr_1': 0.30761606315056955, 'dr_2': 0.35466042833273537, 'learning_rate': 0.001677420037314694, 'weight_decay': 6.061804154796135e-05}. Best is trial 41 with value: 0.03026889075961294.\n",
      "[I 2025-07-24 04:52:08,641] Trial 61 finished with value: 0.030612786571624914 and parameters: {'architecture': 'arch_1', 'dr_0': 0.4026095497178525, 'dr_1': 0.15661581256654114, 'dr_2': 0.2807191341906695, 'learning_rate': 0.0011920694815680131, 'weight_decay': 2.1526665631987978e-05}. Best is trial 41 with value: 0.03026889075961294.\n",
      "[I 2025-07-24 04:53:58,300] Trial 62 finished with value: 0.031191775268768963 and parameters: {'architecture': 'arch_1', 'dr_0': 0.343980174568708, 'dr_1': 0.12181958575512329, 'dr_2': 0.261110352120655, 'learning_rate': 0.0022942366154944257, 'weight_decay': 1.1773068654537925e-05}. Best is trial 41 with value: 0.03026889075961294.\n",
      "[I 2025-07-24 04:55:48,166] Trial 63 finished with value: 0.03047415005821216 and parameters: {'architecture': 'arch_1', 'dr_0': 0.38848251586815624, 'dr_1': 0.10196846427109127, 'dr_2': 0.245446686453267, 'learning_rate': 0.0013608686228046547, 'weight_decay': 0.00013336761413687505}. Best is trial 41 with value: 0.03026889075961294.\n",
      "[I 2025-07-24 04:57:37,121] Trial 64 finished with value: 0.030823548784172986 and parameters: {'architecture': 'arch_1', 'dr_0': 0.38676423804391363, 'dr_1': 0.11419916925479943, 'dr_2': 0.24270581032005767, 'learning_rate': 0.0007620818814943701, 'weight_decay': 0.00012778735763132064}. Best is trial 41 with value: 0.03026889075961294.\n",
      "[I 2025-07-24 04:59:28,646] Trial 65 finished with value: 0.031038596613120428 and parameters: {'architecture': 'arch_1', 'dr_0': 0.24405189759003276, 'dr_1': 0.1349555705969053, 'dr_2': 0.3006951197540657, 'learning_rate': 0.0010955454477421407, 'weight_decay': 0.00017103601037047196}. Best is trial 41 with value: 0.03026889075961294.\n",
      "[I 2025-07-24 05:01:18,890] Trial 66 finished with value: 0.031130741244252726 and parameters: {'architecture': 'arch_1', 'dr_0': 0.3926602093884306, 'dr_1': 0.10049791165957649, 'dr_2': 0.29469604009161243, 'learning_rate': 0.0009182525411691432, 'weight_decay': 8.5864878106949e-05}. Best is trial 41 with value: 0.03026889075961294.\n",
      "[I 2025-07-24 05:03:09,489] Trial 67 finished with value: 0.03101095795348475 and parameters: {'architecture': 'arch_1', 'dr_0': 0.28960283330293835, 'dr_1': 0.11380986633112014, 'dr_2': 0.3252930947930656, 'learning_rate': 0.0006538650622997512, 'weight_decay': 3.439997472272071e-05}. Best is trial 41 with value: 0.03026889075961294.\n",
      "[I 2025-07-24 05:04:56,788] Trial 68 finished with value: 0.03158775992880139 and parameters: {'architecture': 'arch_1', 'dr_0': 0.315863759241332, 'dr_1': 0.12787822548875996, 'dr_2': 0.27694874546852216, 'learning_rate': 0.0005035217962857721, 'weight_decay': 0.0007564585282416913}. Best is trial 41 with value: 0.03026889075961294.\n",
      "[I 2025-07-24 05:06:48,612] Trial 69 finished with value: 0.03155137942750243 and parameters: {'architecture': 'arch_1', 'dr_0': 0.3747244498114355, 'dr_1': 0.3706750094877875, 'dr_2': 0.235581952645867, 'learning_rate': 0.003032405066251571, 'weight_decay': 0.00024725479543308206}. Best is trial 41 with value: 0.03026889075961294.\n",
      "[I 2025-07-24 05:08:41,226] Trial 70 finished with value: 0.030804756651573544 and parameters: {'architecture': 'arch_1', 'dr_0': 0.3574870766553974, 'dr_1': 0.14488889374917205, 'dr_2': 0.24692394135665066, 'learning_rate': 0.0012975579645076105, 'weight_decay': 3.797177740191346e-05}. Best is trial 41 with value: 0.03026889075961294.\n",
      "[I 2025-07-24 05:10:28,805] Trial 71 finished with value: 0.030697138956453228 and parameters: {'architecture': 'arch_1', 'dr_0': 0.3973475278098733, 'dr_1': 0.1601717089555239, 'dr_2': 0.2690386818659671, 'learning_rate': 0.0015920120272095218, 'weight_decay': 0.0005125126802528495}. Best is trial 41 with value: 0.03026889075961294.\n",
      "[I 2025-07-24 05:12:19,183] Trial 72 finished with value: 0.030801105607725397 and parameters: {'architecture': 'arch_1', 'dr_0': 0.43491906331225544, 'dr_1': 0.13368225098721126, 'dr_2': 0.2607361303905221, 'learning_rate': 0.0018403119530151777, 'weight_decay': 0.00020131348072542667}. Best is trial 41 with value: 0.03026889075961294.\n",
      "[I 2025-07-24 05:14:06,571] Trial 73 finished with value: 0.03057832749489742 and parameters: {'architecture': 'arch_1', 'dr_0': 0.4194410652259508, 'dr_1': 0.15053511229176655, 'dr_2': 0.21516846167867504, 'learning_rate': 0.0013835650373040065, 'weight_decay': 0.0003391878816153987}. Best is trial 41 with value: 0.03026889075961294.\n",
      "[I 2025-07-24 05:15:55,530] Trial 74 finished with value: 0.030605288222432137 and parameters: {'architecture': 'arch_1', 'dr_0': 0.4523103558644921, 'dr_1': 0.1764380195154695, 'dr_2': 0.29763606161470213, 'learning_rate': 0.001080958386628311, 'weight_decay': 0.00027217537216811744}. Best is trial 41 with value: 0.03026889075961294.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Neural Network Tuning Complete ---\n",
      "Best trial validation loss: 0.030269\n",
      "Best hyperparameters found for Neural Network: {'architecture': 'arch_1', 'dr_0': 0.3897635076916713, 'dr_1': 0.12176356784582548, 'dr_2': 0.2696972548349174, 'learning_rate': 0.001272196996939945, 'weight_decay': 0.00020557125870761814}\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# BLOCK 5: TUNE NEURAL NETWORK MEAN MODEL\n",
    "# =============================================================================\n",
    "import optuna\n",
    "from sklearn.model_selection import train_test_split\n",
    "import torch.optim as optim\n",
    "from torch.optim.lr_scheduler import OneCycleLR\n",
    "\n",
    "# --- 1. Prepare Data for Tuning (using a random split) ---\n",
    "print(\"--- Step 1: Preparing data for faster Optuna tuning... ---\")\n",
    "X_train_opt, X_val_opt, y_train_opt, y_val_opt = train_test_split(\n",
    "    X_scaled, y_true_scaled, test_size=0.2, random_state=RANDOM_STATE\n",
    ")\n",
    "print(f\"Data prepared. Training set size: {len(X_train_opt)}, Validation set size: {len(X_val_opt)}\")\n",
    "\n",
    "train_dataset_opt = HousePriceDataset(X_train_opt, y_train_opt)\n",
    "val_dataset_opt = HousePriceDataset(X_val_opt, y_val_opt)\n",
    "train_loader_opt = DataLoader(train_dataset_opt, batch_size=512, shuffle=True)\n",
    "val_loader_opt = DataLoader(val_dataset_opt, batch_size=512, shuffle=False)\n",
    "\n",
    "# --- 2. Define the Optuna Objective Function ---\n",
    "def objective_nn(trial):\n",
    "    # Define hyperparameter search space\n",
    "    arch_choice = trial.suggest_categorical('architecture', ['arch_1', 'arch_2'])\n",
    "    layer_sizes = {'arch_1': [512, 256, 128], 'arch_2': [1024, 512, 256]}[arch_choice]\n",
    "    dropout_rates = [trial.suggest_float(f'dr_{i}', 0.1, 0.5) for i in range(len(layer_sizes))]\n",
    "    learning_rate = trial.suggest_float('learning_rate', 1e-4, 1e-2, log=True)\n",
    "    weight_decay = trial.suggest_float('weight_decay', 1e-6, 1e-3, log=True)\n",
    "\n",
    "    model = ResidualNet(X_scaled.shape[1], layer_sizes, dropout_rates).to(device)\n",
    "    optimizer = optim.AdamW(model.parameters(), lr=learning_rate, weight_decay=weight_decay)\n",
    "    scheduler = OneCycleLR(optimizer, max_lr=learning_rate, epochs=50, steps_per_epoch=len(train_loader_opt))\n",
    "    loss_fn = nn.HuberLoss()\n",
    "    \n",
    "    best_val_loss = float('inf')\n",
    "    epochs_no_improve = 0\n",
    "\n",
    "    for epoch in range(50): # 50 epochs is enough for a tuning run\n",
    "        model.train()\n",
    "        for features, labels in train_loader_opt:\n",
    "            features, labels = features.to(device), labels.to(device)\n",
    "            optimizer.zero_grad(); loss = loss_fn(model(features), labels); loss.backward()\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0); optimizer.step(); scheduler.step()\n",
    "        \n",
    "        model.eval()\n",
    "        current_val_loss = 0\n",
    "        with torch.no_grad():\n",
    "            for features, labels in val_loader_opt:\n",
    "                current_val_loss += loss_fn(model(features.to(device)), labels.to(device)).item()\n",
    "        current_val_loss /= len(val_loader_opt)\n",
    "\n",
    "        if current_val_loss < best_val_loss:\n",
    "            best_val_loss = current_val_loss\n",
    "            epochs_no_improve = 0\n",
    "        else:\n",
    "            epochs_no_improve += 1\n",
    "            if epochs_no_improve >= 7: # Shorter patience for faster tuning\n",
    "                break\n",
    "                \n",
    "    return best_val_loss\n",
    "\n",
    "# --- 3. Run the Optuna Study ---\n",
    "study_nn = optuna.create_study(direction='minimize')\n",
    "print(\"\\n--- Starting Neural Network Hyperparameter Tuning... ---\")\n",
    "study_nn.optimize(objective_nn, n_trials=75)\n",
    "\n",
    "# --- 4. Store the Best Results ---\n",
    "print(\"\\n--- Neural Network Tuning Complete ---\")\n",
    "print(f\"Best trial validation loss: {study_nn.best_value:.6f}\")\n",
    "best_params_nn = study_nn.best_params\n",
    "print(\"Best hyperparameters found for Neural Network:\", best_params_nn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "bd284691-3a6c-4852-81a8-ca172b22d74a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "--- Step 6: K-Fold Cross-Validation with Optimal Hyperparameters ---\n",
      "================================================================================\n",
      "\n",
      "--- Training Fold 1/5 ---\n",
      " Epoch 020 | Val Loss: 0.037692\n",
      " Epoch 040 | Val Loss: 0.036052\n",
      " Epoch 060 | Val Loss: 0.034914\n",
      " Epoch 080 | Val Loss: 0.033411\n",
      " Epoch 100 | Val Loss: 0.033753\n",
      "\n",
      "Early stopping at epoch 104. Best validation loss: 0.032294\n",
      "\n",
      "--- Training Fold 2/5 ---\n",
      " Epoch 020 | Val Loss: 0.038847\n",
      " Epoch 040 | Val Loss: 0.036899\n",
      " Epoch 060 | Val Loss: 0.035268\n",
      "\n",
      "Early stopping at epoch 66. Best validation loss: 0.033471\n",
      "\n",
      "--- Training Fold 3/5 ---\n",
      " Epoch 020 | Val Loss: 0.038491\n",
      " Epoch 040 | Val Loss: 0.035928\n",
      " Epoch 060 | Val Loss: 0.036320\n",
      " Epoch 080 | Val Loss: 0.033668\n",
      "\n",
      "Early stopping at epoch 92. Best validation loss: 0.033142\n",
      "\n",
      "--- Training Fold 4/5 ---\n",
      " Epoch 020 | Val Loss: 0.040885\n",
      " Epoch 040 | Val Loss: 0.035814\n",
      " Epoch 060 | Val Loss: 0.032859\n",
      " Epoch 080 | Val Loss: 0.032837\n",
      "\n",
      "Early stopping at epoch 82. Best validation loss: 0.032535\n",
      "\n",
      "--- Training Fold 5/5 ---\n",
      " Epoch 020 | Val Loss: 0.039765\n",
      " Epoch 040 | Val Loss: 0.034432\n",
      "\n",
      "Early stopping at epoch 58. Best validation loss: 0.033014\n",
      "\n",
      "================================================================================\n",
      "--- Final Evaluation and Saving Predictions ---\n",
      "================================================================================\n",
      "Final NN Mean Model OOF RMSE: $113,594.46\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# BLOCK 6: K-FOLD TRAINING & SAVING WITH OPTIMAL NN PARAMETERS\n",
    "# =============================================================================\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"--- Step 6: K-Fold Cross-Validation with Optimal Hyperparameters ---\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# --- 1. Reconstruct Best Hyperparameters from Optuna ---\n",
    "best_arch_choice = best_params_nn['architecture']\n",
    "best_layer_sizes = {'arch_1': [512, 256, 128], 'arch_2': [1024, 512, 256]}[best_arch_choice]\n",
    "best_dropout_rates = [best_params_nn[f'dr_{i}'] for i in range(len(best_layer_sizes))]\n",
    "best_learning_rate = best_params_nn['learning_rate']\n",
    "best_weight_decay = best_params_nn['weight_decay']\n",
    "\n",
    "# --- 2. K-Fold Cross-Validation and Prediction Generation ---\n",
    "EPOCHS, BATCH_SIZE, PATIENCE = 200, 512, 15\n",
    "oof_nn_preds = np.zeros(len(X))\n",
    "test_nn_preds = np.zeros(len(X_test))\n",
    "\n",
    "skf = StratifiedKFold(n_splits=N_SPLITS, shuffle=True, random_state=RANDOM_STATE)\n",
    "for fold, (train_idx, val_idx) in enumerate(skf.split(X_scaled, grade_for_stratify)):\n",
    "    print(f\"\\n--- Training Fold {fold+1}/{N_SPLITS} ---\")\n",
    "    \n",
    "    X_train, y_train = X_scaled[train_idx], y_true_scaled[train_idx]\n",
    "    X_val, y_val = X_scaled[val_idx], y_true_scaled[val_idx]\n",
    "    \n",
    "    train_loader = DataLoader(HousePriceDataset(X_train, y_train), batch_size=BATCH_SIZE, shuffle=True)\n",
    "    val_loader = DataLoader(HousePriceDataset(X_val, y_val), batch_size=BATCH_SIZE, shuffle=False)\n",
    "    \n",
    "    model = ResidualNet(X_scaled.shape[1], best_layer_sizes, best_dropout_rates).to(device)\n",
    "    optimizer = optim.AdamW(model.parameters(), lr=best_learning_rate, weight_decay=best_weight_decay)\n",
    "    scheduler = OneCycleLR(optimizer, max_lr=best_learning_rate, epochs=EPOCHS, steps_per_epoch=len(train_loader))\n",
    "    loss_fn = nn.HuberLoss()\n",
    "    best_val_loss, best_model_state = float('inf'), None\n",
    "    epochs_no_improve = 0\n",
    "\n",
    "    for epoch in range(EPOCHS):\n",
    "        model.train()\n",
    "        for features, labels in train_loader:\n",
    "            features, labels = features.to(device), labels.to(device)\n",
    "            optimizer.zero_grad(); loss = loss_fn(model(features), labels); loss.backward()\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0); optimizer.step(); scheduler.step()\n",
    "        \n",
    "        model.eval()\n",
    "        val_loss = 0\n",
    "        with torch.no_grad():\n",
    "            for features, labels in val_loader:\n",
    "                val_loss += loss_fn(model(features.to(device)), labels.to(device)).item()\n",
    "        val_loss /= len(val_loader)\n",
    "\n",
    "        if (epoch + 1) % 20 == 0: print(f\" Epoch {epoch+1:03d} | Val Loss: {val_loss:.6f}\")\n",
    "        if val_loss < best_val_loss:\n",
    "            best_val_loss, epochs_no_improve, best_model_state = val_loss, 0, model.state_dict().copy()\n",
    "        else:\n",
    "            epochs_no_improve += 1\n",
    "            if epochs_no_improve >= PATIENCE:\n",
    "                print(f\"\\nEarly stopping at epoch {epoch+1}. Best validation loss: {best_val_loss:.6f}\")\n",
    "                break\n",
    "    \n",
    "    # Generate Predictions for the fold\n",
    "    model.load_state_dict(best_model_state)\n",
    "    model.eval()\n",
    "    \n",
    "    # OOF Predictions (inverse transform to get real prices)\n",
    "    with torch.no_grad():\n",
    "        raw_oof_preds = np.concatenate([model(f.to(device)).detach().cpu().numpy() for f, _ in val_loader])\n",
    "    oof_nn_preds[val_idx] = np.clip(target_scaler.inverse_transform(raw_oof_preds).flatten(), 0, None)\n",
    "    \n",
    "    # Test Predictions (accumulate for averaging later)\n",
    "    test_loader_fold = DataLoader(HousePriceDataset(X_test_scaled), batch_size=BATCH_SIZE*2, shuffle=False)\n",
    "    with torch.no_grad():\n",
    "        raw_test_preds = np.concatenate([model(f.to(device)).detach().cpu().numpy() for f in test_loader_fold])\n",
    "    test_nn_preds += np.clip(target_scaler.inverse_transform(raw_test_preds).flatten(), 0, None) / N_SPLITS\n",
    "\n",
    "# --- 3. Final Evaluation and Saving ---\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"--- Final Evaluation and Saving Predictions ---\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "final_oof_rmse = np.sqrt(mean_squared_error(y_true, oof_nn_preds))\n",
    "print(f\"Final NN Mean Model OOF RMSE: ${final_oof_rmse:,.2f}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e005b7dc-a3dc-41de-8bd9-19ae00991567",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Prediction arrays saved successfully to: './NN_model_predictions/'\n"
     ]
    }
   ],
   "source": [
    "# Save the Prediction Arrays\n",
    "SAVE_PATH = './NN_model_predictions/'\n",
    "os.makedirs(SAVE_PATH, exist_ok=True)\n",
    "np.save(os.path.join(SAVE_PATH, 'oof_nn_preds.npy'), oof_nn_preds)\n",
    "np.save(os.path.join(SAVE_PATH, 'test_nn_preds.npy'), test_nn_preds)\n",
    "print(f\"\\nPrediction arrays saved successfully to: '{SAVE_PATH}'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab264387-2100-4247-85dd-358665f57848",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (Kaggle Comp)",
   "language": "python",
   "name": "kaggle-comp"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4df3b0f0-4908-4037-97d1-fa2477abcf6f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Libraries imported successfully.\n",
      "Raw data loaded successfully.\n",
      "Setup complete.\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# BLOCK 1: SETUP, IMPORTS, AND DATA LOADING\n",
    "# =============================================================================\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "import time\n",
    "import os\n",
    "# --- Library Imports ---\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import gc\n",
    "from sklearn.model_selection import StratifiedKFold, train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import xgboost as xgb\n",
    "import catboost as cb\n",
    "import optuna\n",
    "print(\"Libraries imported successfully.\")\n",
    "# --- Helper Function for Winkler Score ---\n",
    "def winkler_score(y_true, lower, upper, alpha=0.1, return_coverage=False):\n",
    "    width = upper - lower\n",
    "    penalty_lower = np.where(y_true < lower, (2 / alpha) * (lower - y_true), 0)\n",
    "    penalty_upper = np.where(y_true > upper, (2 / alpha) * (y_true - upper), 0)\n",
    "    score = width + penalty_lower + penalty_upper\n",
    "    if return_coverage:\n",
    "        coverage = np.mean((y_true >= lower) & (y_true <= upper))\n",
    "        return np.mean(score), coverage\n",
    "    return np.mean(score)\n",
    "# --- Global Constants ---\n",
    "N_SPLITS = 5\n",
    "RANDOM_STATE = 42\n",
    "DATA_PATH = './'\n",
    "N_OPTUNA_TRIALS = 75 # A strong number for a comprehensive search\n",
    "COMPETITION_ALPHA = 0.1\n",
    "\n",
    "# --- Load Raw Data ---\n",
    "try:\n",
    "    # We drop the low-variance columns they identified right away\n",
    "    drop_cols=['id', 'golf', 'view_rainier', 'view_skyline', 'view_lakesamm','view_otherwater', 'view_other']\n",
    "    df_train = pd.read_csv(DATA_PATH + 'dataset.csv').drop(columns=drop_cols)\n",
    "    df_test = pd.read_csv(DATA_PATH + 'test.csv').drop(columns=drop_cols)\n",
    "    print(\"Raw data loaded successfully.\")\n",
    "except FileNotFoundError:\n",
    "    print(\"ERROR: Could not find 'dataset.csv' or 'test.csv'.\")\n",
    "    exit()\n",
    "# --- Prepare Target Variable ---\n",
    "y_true = df_train['sale_price'].copy()\n",
    "# The mean-error model works best when predicting the raw price directly\n",
    "# So, we will NOT log-transform the target this time.\n",
    "# df_train.drop('sale_price', axis=1, inplace=True) # We keep sale_price for FE\n",
    "print(\"Setup complete.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3f7c8978-4bc1-47bc-93f1-2288509df6aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Starting Block 2.5: Executing Feature Engineering Pipeline ---\n",
      "--- Starting Comprehensive Feature Engineering ---\n",
      "Step 1: Creating brute-force numerical interaction features...\n",
      "Step 2: Creating date features...\n",
      "Step 3: Creating TF-IDF features for text columns...\n",
      "Step 4: Creating group-by aggregation features...\n",
      "Step 5: Creating ratio features...\n",
      "Step 6: Creating geospatial clustering features...\n",
      "Step 7: Finalizing feature set...\n",
      "\n",
      "Comprehensive FE complete. Total features: 233\n",
      "Feature engineering complete. X shape: (200000, 233), X_test shape: (200000, 233)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Make sure to have these libraries installed\n",
    "# pip install pandas numpy scikit-learn\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.cluster import KMeans\n",
    "import gc\n",
    "\n",
    "# Define a random state for reproducibility\n",
    "RANDOM_STATE = 42\n",
    "\n",
    "def create_comprehensive_features(df_train, df_test):\n",
    "    \"\"\"\n",
    "    Combines original and new advanced feature engineering steps into a single pipeline.\n",
    "    \"\"\"\n",
    "    print(\"--- Starting Comprehensive Feature Engineering ---\")\n",
    "\n",
    "    # Store original indices and target variable\n",
    "    train_ids = df_train.index\n",
    "    test_ids = df_test.index\n",
    "    y_train = df_train['sale_price'].copy() # Keep the target separate\n",
    "\n",
    "    # Combine for consistent processing\n",
    "    df_train_temp = df_train.drop(columns=['sale_price'])\n",
    "    all_data = pd.concat([df_train_temp, df_test], axis=0, ignore_index=True)\n",
    "\n",
    "    # --- Original Feature Engineering ---\n",
    "\n",
    "    # A) Brute-Force Numerical Interactions\n",
    "    print(\"Step 1: Creating brute-force numerical interaction features...\")\n",
    "    NUMS = ['area', 'land_val', 'imp_val', 'sqft_lot', 'sqft', 'sqft_1', 'grade', 'year_built']\n",
    "    # Ensure all columns exist and are numeric, fill missing with 0 for safety\n",
    "    for col in NUMS:\n",
    "        if col not in all_data.columns:\n",
    "            all_data[col] = 0\n",
    "        else:\n",
    "            all_data[col] = pd.to_numeric(all_data[col], errors='coerce').fillna(0)\n",
    "            \n",
    "    for i in range(len(NUMS)):\n",
    "        for j in range(i + 1, len(NUMS)):\n",
    "            all_data[f'{NUMS[i]}_x_{NUMS[j]}'] = all_data[NUMS[i]] * all_data[NUMS[j]]\n",
    "\n",
    "    # B) Date Features\n",
    "    print(\"Step 2: Creating date features...\")\n",
    "    all_data['sale_date'] = pd.to_datetime(all_data['sale_date'])\n",
    "    all_data['sale_year'] = all_data['sale_date'].dt.year\n",
    "    all_data['sale_month'] = all_data['sale_date'].dt.month\n",
    "    all_data['sale_dayofyear'] = all_data['sale_date'].dt.dayofyear\n",
    "    all_data['age_at_sale'] = all_data['sale_year'] - all_data['year_built']\n",
    "\n",
    "    # C) TF-IDF Text Features\n",
    "    print(\"Step 3: Creating TF-IDF features for text columns...\")\n",
    "    text_cols = ['subdivision', 'zoning', 'city', 'sale_warning', 'join_status', 'submarket']\n",
    "    all_data[text_cols] = all_data[text_cols].fillna('missing').astype(str)\n",
    "    \n",
    "    for col in text_cols:\n",
    "        tfidf = TfidfVectorizer(analyzer='char', ngram_range=(3, 5), max_features=128, binary=True)\n",
    "        svd = TruncatedSVD(n_components=8, random_state=RANDOM_STATE)\n",
    "        \n",
    "        tfidf_matrix = tfidf.fit_transform(all_data[col])\n",
    "        tfidf_svd = svd.fit_transform(tfidf_matrix)\n",
    "        \n",
    "        tfidf_df = pd.DataFrame(tfidf_svd, columns=[f'{col}_tfidf_svd_{i}' for i in range(8)])\n",
    "        all_data = pd.concat([all_data, tfidf_df], axis=1)\n",
    "\n",
    "    # D) Log transform some interaction features\n",
    "    for c in ['land_val_x_imp_val', 'land_val_x_sqft', 'imp_val_x_sqft']:\n",
    "        if c in all_data.columns:\n",
    "            all_data[c] = np.log1p(all_data[c].fillna(0))\n",
    "\n",
    "    # --- New Feature Engineering Ideas ---\n",
    "\n",
    "    # F) Group-By Aggregation Features\n",
    "    print(\"Step 4: Creating group-by aggregation features...\")\n",
    "    group_cols = ['submarket', 'city', 'zoning']\n",
    "    num_cols_for_agg = ['grade', 'sqft', 'imp_val', 'land_val', 'age_at_sale']\n",
    "\n",
    "    for group_col in group_cols:\n",
    "        for num_col in num_cols_for_agg:\n",
    "            agg_stats = all_data.groupby(group_col)[num_col].agg(['mean', 'std', 'max', 'min']).reset_index()\n",
    "            agg_stats.columns = [group_col] + [f'{group_col}_{num_col}_{stat}' for stat in ['mean', 'std', 'max', 'min']]\n",
    "            all_data = pd.merge(all_data, agg_stats, on=group_col, how='left')\n",
    "            all_data[f'{num_col}_minus_{group_col}_mean'] = all_data[num_col] - all_data[f'{group_col}_{num_col}_mean']\n",
    "\n",
    "    # G) Ratio Features\n",
    "    print(\"Step 5: Creating ratio features...\")\n",
    "    # Add a small epsilon to prevent division by zero\n",
    "    epsilon = 1e-6 \n",
    "    all_data['total_val'] = all_data['imp_val'] + all_data['land_val']\n",
    "    all_data['imp_val_to_land_val_ratio'] = all_data['imp_val'] / (all_data['land_val'] + epsilon)\n",
    "    all_data['land_val_ratio'] = all_data['land_val'] / (all_data['total_val'] + epsilon)\n",
    "    all_data['sqft_to_lot_ratio'] = all_data['sqft'] / (all_data['sqft_lot'] + epsilon)\n",
    "    all_data['was_renovated'] = (all_data['year_reno'] > 0).astype(int)\n",
    "    all_data['reno_age_at_sale'] = np.where(all_data['was_renovated'] == 1, all_data['sale_year'] - all_data['year_reno'], -1)\n",
    "\n",
    "    # H) Geospatial Clustering Features\n",
    "    print(\"Step 6: Creating geospatial clustering features...\")\n",
    "    coords = all_data[['latitude', 'longitude']].copy()\n",
    "    coords.fillna(coords.median(), inplace=True) # Simple imputation\n",
    "\n",
    "    # KMeans is sensitive to feature scaling, but for lat/lon it's often okay without it.\n",
    "    kmeans = KMeans(n_clusters=20, random_state=RANDOM_STATE, n_init=10) \n",
    "    all_data['location_cluster'] = kmeans.fit_predict(coords)\n",
    "    \n",
    "    # Calculate distance to each cluster center\n",
    "    cluster_centers = kmeans.cluster_centers_\n",
    "    for i in range(len(cluster_centers)):\n",
    "        center = cluster_centers[i]\n",
    "        all_data[f'dist_to_cluster_{i}'] = np.sqrt((coords['latitude'] - center[0])**2 + (coords['longitude'] - center[1])**2)\n",
    "\n",
    "    # --- Final Cleanup ---\n",
    "    print(\"Step 7: Finalizing feature set...\")\n",
    "    cols_to_drop = ['sale_date', 'subdivision', 'zoning', 'city', 'sale_warning', 'join_status', 'submarket']\n",
    "    all_data = all_data.drop(columns=cols_to_drop)\n",
    "\n",
    "    # One-hot encode the new cluster feature\n",
    "    all_data = pd.get_dummies(all_data, columns=['location_cluster'], prefix='loc_cluster')\n",
    "    \n",
    "    # Final check for any remaining object columns to be safe (besides index)\n",
    "    object_cols = all_data.select_dtypes(include='object').columns\n",
    "    if len(object_cols) > 0:\n",
    "        print(f\"Warning: Found unexpected object columns: {object_cols}. Dropping them.\")\n",
    "        all_data = all_data.drop(columns=object_cols)\n",
    "        \n",
    "    all_data.fillna(0, inplace=True)\n",
    "\n",
    "    # Separate back into train and test sets\n",
    "    train_len = len(train_ids)\n",
    "    X = all_data.iloc[:train_len].copy()\n",
    "    X_test = all_data.iloc[train_len:].copy()\n",
    "    \n",
    "    # Restore original indices\n",
    "    X.index = train_ids\n",
    "    X_test.index = test_ids\n",
    "    \n",
    "    # Align columns - crucial for model prediction\n",
    "    X_test = X_test[X.columns]\n",
    "    \n",
    "    print(f\"\\nComprehensive FE complete. Total features: {X.shape[1]}\")\n",
    "    gc.collect()\n",
    "    \n",
    "    return X, X_test, y_train\n",
    "# =============================================================================\n",
    "# BLOCK 2.5: EXECUTE FEATURE ENGINEERING\n",
    "# =============================================================================\n",
    "print(\"\\n--- Starting Block 2.5: Executing Feature Engineering Pipeline ---\")\n",
    "\n",
    "# This is the crucial step that was missing.\n",
    "# We call the function to create our training and testing dataframes.\n",
    "X, X_test, y_train = create_comprehensive_features(df_train, df_test)\n",
    "\n",
    "# Let's verify the output\n",
    "print(f\"Feature engineering complete. X shape: {X.shape}, X_test shape: {X_test.shape}\")\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3910d553-0a53-4c85-a75e-b10e96af919c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-07-24 11:05:58,495] A new study created in memory with name: no-name-c3a86e07-b618-489e-adca-74dd0b8be857\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Starting CatBoost Hyperparameter Tuning... ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-07-24 11:06:43,007] Trial 0 finished with value: 98132.47034990958 and parameters: {'iterations': 1724, 'learning_rate': 0.08553734379319525, 'depth': 9, 'l2_leaf_reg': 0.0034611339608567834, 'subsample': 0.824226327997697, 'random_strength': 0.03361501975246283, 'bagging_temperature': 0.9162153901615222}. Best is trial 0 with value: 98132.47034990958.\n",
      "[I 2025-07-24 11:07:14,527] Trial 1 finished with value: 103050.68366818536 and parameters: {'iterations': 2121, 'learning_rate': 0.016116870504803364, 'depth': 8, 'l2_leaf_reg': 5.016282029850647, 'subsample': 0.775705794558661, 'random_strength': 0.13366380736653607, 'bagging_temperature': 0.14718337062265796}. Best is trial 0 with value: 98132.47034990958.\n",
      "[I 2025-07-24 11:07:46,305] Trial 2 finished with value: 99325.02461933375 and parameters: {'iterations': 1238, 'learning_rate': 0.04133952385334066, 'depth': 9, 'l2_leaf_reg': 0.17663790221349804, 'subsample': 0.8876050508601209, 'random_strength': 0.003949268356284431, 'bagging_temperature': 0.013030491459330551}. Best is trial 0 with value: 98132.47034990958.\n",
      "[I 2025-07-24 11:08:31,255] Trial 3 finished with value: 101205.86553815317 and parameters: {'iterations': 1809, 'learning_rate': 0.017173939742341285, 'depth': 9, 'l2_leaf_reg': 0.09322830132813713, 'subsample': 0.7510730996642296, 'random_strength': 0.07091418563351785, 'bagging_temperature': 0.5502794271728046}. Best is trial 0 with value: 98132.47034990958.\n",
      "[I 2025-07-24 11:08:50,346] Trial 4 finished with value: 98776.5501499671 and parameters: {'iterations': 1632, 'learning_rate': 0.09904275554740181, 'depth': 7, 'l2_leaf_reg': 0.011500911125440057, 'subsample': 0.9974719573383555, 'random_strength': 4.7399788493769375, 'bagging_temperature': 0.699843538171924}. Best is trial 0 with value: 98132.47034990958.\n",
      "[I 2025-07-24 11:09:15,723] Trial 5 finished with value: 104790.99927500087 and parameters: {'iterations': 1630, 'learning_rate': 0.012445794759372434, 'depth': 8, 'l2_leaf_reg': 0.0011547429698928566, 'subsample': 0.8503978041211012, 'random_strength': 0.002747940164573439, 'bagging_temperature': 0.9407364552366795}. Best is trial 0 with value: 98132.47034990958.\n",
      "[I 2025-07-24 11:09:53,428] Trial 6 finished with value: 99762.20049489236 and parameters: {'iterations': 2497, 'learning_rate': 0.02179102357382162, 'depth': 8, 'l2_leaf_reg': 0.001190546134915762, 'subsample': 0.7294392563297479, 'random_strength': 0.010715620588868395, 'bagging_temperature': 0.3359605035228882}. Best is trial 0 with value: 98132.47034990958.\n",
      "[I 2025-07-24 11:10:18,596] Trial 7 finished with value: 98491.00455711638 and parameters: {'iterations': 1535, 'learning_rate': 0.08551868252774053, 'depth': 8, 'l2_leaf_reg': 0.010192754481736288, 'subsample': 0.9298167626672877, 'random_strength': 3.8925334458023912, 'bagging_temperature': 0.30767641451984207}. Best is trial 0 with value: 98132.47034990958.\n",
      "[I 2025-07-24 11:10:37,097] Trial 8 finished with value: 104698.28462415449 and parameters: {'iterations': 2210, 'learning_rate': 0.016407372285310588, 'depth': 6, 'l2_leaf_reg': 0.006399495298619721, 'subsample': 0.8763439066839429, 'random_strength': 0.3912344796277231, 'bagging_temperature': 0.9917046902475344}. Best is trial 0 with value: 98132.47034990958.\n",
      "[I 2025-07-24 11:11:03,647] Trial 9 finished with value: 100755.23032877274 and parameters: {'iterations': 2482, 'learning_rate': 0.025279936224307412, 'depth': 7, 'l2_leaf_reg': 0.0030266147283242806, 'subsample': 0.7803909310665911, 'random_strength': 6.306287181608668, 'bagging_temperature': 0.9521220642918444}. Best is trial 0 with value: 98132.47034990958.\n",
      "[I 2025-07-24 11:12:02,080] Trial 10 finished with value: 98886.70758347558 and parameters: {'iterations': 1103, 'learning_rate': 0.04440376158577202, 'depth': 10, 'l2_leaf_reg': 0.05876710820039385, 'subsample': 0.8102811462223525, 'random_strength': 0.0243623854577863, 'bagging_temperature': 0.6681675171097026}. Best is trial 0 with value: 98132.47034990958.\n",
      "[I 2025-07-24 11:13:21,129] Trial 11 finished with value: 98330.85887357967 and parameters: {'iterations': 1391, 'learning_rate': 0.09064556093700966, 'depth': 10, 'l2_leaf_reg': 0.02098227749000795, 'subsample': 0.9403325360570602, 'random_strength': 0.7712337291810789, 'bagging_temperature': 0.34742618595830044}. Best is trial 0 with value: 98132.47034990958.\n",
      "[I 2025-07-24 11:14:37,290] Trial 12 finished with value: 98509.42316247676 and parameters: {'iterations': 1338, 'learning_rate': 0.06430207643181564, 'depth': 10, 'l2_leaf_reg': 0.024052805809008777, 'subsample': 0.9488285351826822, 'random_strength': 0.6002236085763049, 'bagging_temperature': 0.38511510280387945}. Best is trial 0 with value: 98132.47034990958.\n",
      "[I 2025-07-24 11:16:21,253] Trial 13 finished with value: 97185.03065187931 and parameters: {'iterations': 1948, 'learning_rate': 0.06279947942293788, 'depth': 10, 'l2_leaf_reg': 0.5367291862975647, 'subsample': 0.8207727576501618, 'random_strength': 0.8046050540497418, 'bagging_temperature': 0.762549759869238}. Best is trial 13 with value: 97185.03065187931.\n",
      "[I 2025-07-24 11:17:10,729] Trial 14 finished with value: 97414.14853302005 and parameters: {'iterations': 1952, 'learning_rate': 0.05778091186247124, 'depth': 9, 'l2_leaf_reg': 0.5125145762516621, 'subsample': 0.8183000239033299, 'random_strength': 0.09886093659963499, 'bagging_temperature': 0.8065539037369877}. Best is trial 13 with value: 97185.03065187931.\n",
      "[I 2025-07-24 11:18:20,892] Trial 15 finished with value: 97133.58769825859 and parameters: {'iterations': 2844, 'learning_rate': 0.05204317520362104, 'depth': 9, 'l2_leaf_reg': 0.7491650483319586, 'subsample': 0.7059597947753062, 'random_strength': 0.20861126026133742, 'bagging_temperature': 0.8210946822884628}. Best is trial 15 with value: 97133.58769825859.\n",
      "[I 2025-07-24 11:20:49,858] Trial 16 finished with value: 96859.64517375045 and parameters: {'iterations': 2957, 'learning_rate': 0.03930343152114869, 'depth': 10, 'l2_leaf_reg': 1.8131678207020587, 'subsample': 0.7055263867740631, 'random_strength': 1.2045635230887481, 'bagging_temperature': 0.771978734419476}. Best is trial 16 with value: 96859.64517375045.\n",
      "[I 2025-07-24 11:22:00,136] Trial 17 finished with value: 98220.39515281534 and parameters: {'iterations': 2913, 'learning_rate': 0.037014633757998495, 'depth': 9, 'l2_leaf_reg': 7.442565936796307, 'subsample': 0.7092844153317034, 'random_strength': 2.060477515081573, 'bagging_temperature': 0.5545866212917412}. Best is trial 16 with value: 96859.64517375045.\n",
      "[I 2025-07-24 11:24:28,247] Trial 18 finished with value: 97519.96666251704 and parameters: {'iterations': 2950, 'learning_rate': 0.030997813919708198, 'depth': 10, 'l2_leaf_reg': 1.8454829841703648, 'subsample': 0.7042781258524028, 'random_strength': 0.31377382946182075, 'bagging_temperature': 0.8489292548314219}. Best is trial 16 with value: 96859.64517375045.\n",
      "[I 2025-07-24 11:25:35,079] Trial 19 finished with value: 97069.74822448753 and parameters: {'iterations': 2701, 'learning_rate': 0.050681537296878805, 'depth': 9, 'l2_leaf_reg': 1.5327708528391728, 'subsample': 0.7486939862491968, 'random_strength': 1.693315947654751, 'bagging_temperature': 0.6442813336224931}. Best is trial 16 with value: 96859.64517375045.\n",
      "[I 2025-07-24 11:26:02,503] Trial 20 finished with value: 100251.23459824327 and parameters: {'iterations': 2633, 'learning_rate': 0.03029714939596608, 'depth': 7, 'l2_leaf_reg': 2.5589236877940102, 'subsample': 0.7607854556483429, 'random_strength': 1.739986367378182, 'bagging_temperature': 0.629700836659006}. Best is trial 16 with value: 96859.64517375045.\n",
      "[I 2025-07-24 11:27:11,170] Trial 21 finished with value: 97171.65406747654 and parameters: {'iterations': 2760, 'learning_rate': 0.05145888856129615, 'depth': 9, 'l2_leaf_reg': 0.7407607982390574, 'subsample': 0.7335346997505751, 'random_strength': 1.690366459282851, 'bagging_temperature': 0.7601947834381054}. Best is trial 16 with value: 96859.64517375045.\n",
      "[I 2025-07-24 11:28:09,618] Trial 22 finished with value: 97513.30593567374 and parameters: {'iterations': 2364, 'learning_rate': 0.04990479503819989, 'depth': 9, 'l2_leaf_reg': 1.2636973625549497, 'subsample': 0.7256454758549914, 'random_strength': 0.2516342669949674, 'bagging_temperature': 0.8458479539722531}. Best is trial 16 with value: 96859.64517375045.\n",
      "[I 2025-07-24 11:30:28,481] Trial 23 finished with value: 97261.02893453701 and parameters: {'iterations': 2745, 'learning_rate': 0.07158365642710138, 'depth': 10, 'l2_leaf_reg': 0.2075250287894235, 'subsample': 0.7080215947625617, 'random_strength': 0.1878722136694384, 'bagging_temperature': 0.45119087837694904}. Best is trial 16 with value: 96859.64517375045.\n",
      "[I 2025-07-24 11:31:41,557] Trial 24 finished with value: 97784.77191586878 and parameters: {'iterations': 2961, 'learning_rate': 0.041554952996124155, 'depth': 9, 'l2_leaf_reg': 3.944514081494596, 'subsample': 0.7524736063751088, 'random_strength': 9.972063247922335, 'bagging_temperature': 0.6473171615433242}. Best is trial 16 with value: 96859.64517375045.\n",
      "[I 2025-07-24 11:34:06,703] Trial 25 finished with value: 97212.05975912293 and parameters: {'iterations': 2760, 'learning_rate': 0.035515183236807454, 'depth': 10, 'l2_leaf_reg': 1.0814458925358401, 'subsample': 0.7923415094671373, 'random_strength': 1.4116053674431241, 'bagging_temperature': 0.5567842414987124}. Best is trial 16 with value: 96859.64517375045.\n",
      "[I 2025-07-24 11:35:12,178] Trial 26 finished with value: 96942.89047782785 and parameters: {'iterations': 2628, 'learning_rate': 0.051290575201538104, 'depth': 9, 'l2_leaf_reg': 0.32498251001567136, 'subsample': 0.7453206902196216, 'random_strength': 0.547137907187947, 'bagging_temperature': 0.7051916041922877}. Best is trial 16 with value: 96859.64517375045.\n",
      "[I 2025-07-24 11:37:25,454] Trial 27 finished with value: 97598.22698339012 and parameters: {'iterations': 2586, 'learning_rate': 0.07421984364565075, 'depth': 10, 'l2_leaf_reg': 0.2801778629886388, 'subsample': 0.7381446318587352, 'random_strength': 2.463335593071205, 'bagging_temperature': 0.7373646776071826}. Best is trial 16 with value: 96859.64517375045.\n",
      "[I 2025-07-24 11:38:00,206] Trial 28 finished with value: 100589.50958288419 and parameters: {'iterations': 2284, 'learning_rate': 0.023218697884920963, 'depth': 8, 'l2_leaf_reg': 2.6136273545928126, 'subsample': 0.7677904919469898, 'random_strength': 0.59073225126062, 'bagging_temperature': 0.5000634603706612}. Best is trial 16 with value: 96859.64517375045.\n",
      "[I 2025-07-24 11:39:05,634] Trial 29 finished with value: 99630.0764101652 and parameters: {'iterations': 2636, 'learning_rate': 0.027231848491130084, 'depth': 9, 'l2_leaf_reg': 8.810453895456837, 'subsample': 0.799666041825913, 'random_strength': 0.05643117563700199, 'bagging_temperature': 0.594103265982597}. Best is trial 16 with value: 96859.64517375045.\n",
      "[I 2025-07-24 11:40:07,266] Trial 30 finished with value: 97563.87184269022 and parameters: {'iterations': 2410, 'learning_rate': 0.03601752510123132, 'depth': 9, 'l2_leaf_reg': 0.06650018429043827, 'subsample': 0.8452377169798034, 'random_strength': 1.0627440300833426, 'bagging_temperature': 0.694837060055462}. Best is trial 16 with value: 96859.64517375045.\n",
      "[I 2025-07-24 11:41:16,545] Trial 31 finished with value: 96943.04729605875 and parameters: {'iterations': 2801, 'learning_rate': 0.04973183529651176, 'depth': 9, 'l2_leaf_reg': 0.4701022763240375, 'subsample': 0.7007798344535486, 'random_strength': 0.4121557167589586, 'bagging_temperature': 0.8808499759839441}. Best is trial 16 with value: 96859.64517375045.\n",
      "[I 2025-07-24 11:41:59,117] Trial 32 finished with value: 97354.87062181531 and parameters: {'iterations': 2810, 'learning_rate': 0.04748118950715453, 'depth': 8, 'l2_leaf_reg': 0.3582829905364013, 'subsample': 0.7432489967455735, 'random_strength': 0.4507911122169547, 'bagging_temperature': 0.8812757578093721}. Best is trial 16 with value: 96859.64517375045.\n",
      "[I 2025-07-24 11:43:13,258] Trial 33 finished with value: 97087.78498819517 and parameters: {'iterations': 2995, 'learning_rate': 0.05786896955675425, 'depth': 9, 'l2_leaf_reg': 0.15392250536562146, 'subsample': 0.7211544850355383, 'random_strength': 3.4431230722750312, 'bagging_temperature': 0.7828791413768028}. Best is trial 16 with value: 96859.64517375045.\n",
      "[I 2025-07-24 11:44:20,341] Trial 34 finished with value: 97223.4119271821 and parameters: {'iterations': 2684, 'learning_rate': 0.04069529101966429, 'depth': 9, 'l2_leaf_reg': 1.497018702054356, 'subsample': 0.7832287357576496, 'random_strength': 0.11681612952572001, 'bagging_temperature': 0.9274259295910303}. Best is trial 16 with value: 96859.64517375045.\n",
      "[I 2025-07-24 11:46:08,720] Trial 35 finished with value: 97303.24020302412 and parameters: {'iterations': 2083, 'learning_rate': 0.07360768398865042, 'depth': 10, 'l2_leaf_reg': 4.195530483750831, 'subsample': 0.7675937174250349, 'random_strength': 1.2266138527968011, 'bagging_temperature': 0.7143501558668401}. Best is trial 16 with value: 96859.64517375045.\n",
      "[I 2025-07-24 11:46:46,978] Trial 36 finished with value: 97792.33862149401 and parameters: {'iterations': 2559, 'learning_rate': 0.0545467160980662, 'depth': 8, 'l2_leaf_reg': 0.11414571491009332, 'subsample': 0.7201666121496486, 'random_strength': 2.6926589328025465, 'bagging_temperature': 0.8881088031382476}. Best is trial 16 with value: 96859.64517375045.\n",
      "[I 2025-07-24 11:47:30,370] Trial 37 finished with value: 97452.93486941412 and parameters: {'iterations': 2866, 'learning_rate': 0.044779889946644634, 'depth': 8, 'l2_leaf_reg': 0.36390109706357326, 'subsample': 0.7476323988654744, 'random_strength': 0.36010799380891934, 'bagging_temperature': 0.06124427962502588}. Best is trial 16 with value: 96859.64517375045.\n",
      "[I 2025-07-24 11:48:31,084] Trial 38 finished with value: 102624.48884287507 and parameters: {'iterations': 2457, 'learning_rate': 0.010396729071140587, 'depth': 9, 'l2_leaf_reg': 0.9143585262395815, 'subsample': 0.7010402033312051, 'random_strength': 0.03357857093948908, 'bagging_temperature': 0.2585405729065515}. Best is trial 16 with value: 96859.64517375045.\n",
      "[I 2025-07-24 11:49:29,385] Trial 39 finished with value: 98372.42592754333 and parameters: {'iterations': 2299, 'learning_rate': 0.03359150228517633, 'depth': 9, 'l2_leaf_reg': 2.481211970634064, 'subsample': 0.8426354686066101, 'random_strength': 0.8621436633339141, 'bagging_temperature': 0.6467239253200692}. Best is trial 16 with value: 96859.64517375045.\n",
      "[I 2025-07-24 11:49:51,476] Trial 40 finished with value: 102958.40975959784 and parameters: {'iterations': 2849, 'learning_rate': 0.020378694067910066, 'depth': 6, 'l2_leaf_reg': 0.5223272900975056, 'subsample': 0.7225691326764412, 'random_strength': 6.4886804100271975, 'bagging_temperature': 0.4830714550985932}. Best is trial 16 with value: 96859.64517375045.\n",
      "[I 2025-07-24 11:51:04,738] Trial 41 finished with value: 96387.86640381806 and parameters: {'iterations': 2959, 'learning_rate': 0.0604931337615656, 'depth': 9, 'l2_leaf_reg': 0.1195942543993236, 'subsample': 0.7215144912978994, 'random_strength': 0.0011789533818766123, 'bagging_temperature': 0.8080844703507192}. Best is trial 41 with value: 96387.86640381806.\n",
      "[I 2025-07-24 11:51:36,444] Trial 42 finished with value: 97460.58609741076 and parameters: {'iterations': 3000, 'learning_rate': 0.06238044025657079, 'depth': 7, 'l2_leaf_reg': 0.1738117181509826, 'subsample': 0.7505312282484441, 'random_strength': 0.001604052825535282, 'bagging_temperature': 0.9767626913798078}. Best is trial 41 with value: 96387.86640381806.\n",
      "[I 2025-07-24 11:52:18,316] Trial 43 finished with value: 97716.59812243129 and parameters: {'iterations': 2757, 'learning_rate': 0.04441013279549659, 'depth': 8, 'l2_leaf_reg': 0.03979621251459929, 'subsample': 0.7177731209451619, 'random_strength': 0.007719384121489213, 'bagging_temperature': 0.8882666709997027}. Best is trial 41 with value: 96387.86640381806.\n",
      "[I 2025-07-24 11:53:21,211] Trial 44 finished with value: 97155.84564882299 and parameters: {'iterations': 2526, 'learning_rate': 0.03942673943104394, 'depth': 9, 'l2_leaf_reg': 0.21688004164022526, 'subsample': 0.733678086149044, 'random_strength': 0.016148474777162386, 'bagging_temperature': 0.7279393539243507}. Best is trial 41 with value: 96387.86640381806.\n",
      "[I 2025-07-24 11:55:51,168] Trial 45 finished with value: 97025.89747957616 and parameters: {'iterations': 2691, 'learning_rate': 0.06716599941105843, 'depth': 10, 'l2_leaf_reg': 0.3702754196180532, 'subsample': 0.8960336136083947, 'random_strength': 0.0011469415474754587, 'bagging_temperature': 0.7907025631882683}. Best is trial 41 with value: 96387.86640381806.\n",
      "[I 2025-07-24 11:58:30,661] Trial 46 finished with value: 97208.40836136088 and parameters: {'iterations': 2845, 'learning_rate': 0.0818053115020072, 'depth': 10, 'l2_leaf_reg': 0.10008946728440268, 'subsample': 0.9055013819634792, 'random_strength': 0.001285383603537434, 'bagging_temperature': 0.8121186520568223}. Best is trial 41 with value: 96387.86640381806.\n",
      "[I 2025-07-24 12:00:58,131] Trial 47 finished with value: 97052.19820959594 and parameters: {'iterations': 2645, 'learning_rate': 0.06443724077680245, 'depth': 10, 'l2_leaf_reg': 0.35954362305415327, 'subsample': 0.8934176004510014, 'random_strength': 0.003244255589801778, 'bagging_temperature': 0.9343147081971626}. Best is trial 41 with value: 96387.86640381806.\n",
      "[I 2025-07-24 12:03:16,365] Trial 48 finished with value: 97394.74041126446 and parameters: {'iterations': 2890, 'learning_rate': 0.09345139836080509, 'depth': 10, 'l2_leaf_reg': 0.061452760153764566, 'subsample': 0.983844766293683, 'random_strength': 0.005231761109864768, 'bagging_temperature': 0.8431388848030446}. Best is trial 41 with value: 96387.86640381806.\n",
      "[I 2025-07-24 12:05:38,212] Trial 49 finished with value: 97187.43459278898 and parameters: {'iterations': 2573, 'learning_rate': 0.06926900966749233, 'depth': 10, 'l2_leaf_reg': 0.03943885390058999, 'subsample': 0.8646014324611395, 'random_strength': 0.0018275830454074463, 'bagging_temperature': 0.7869323853781318}. Best is trial 41 with value: 96387.86640381806.\n",
      "[I 2025-07-24 12:07:40,112] Trial 50 finished with value: 96972.1116254719 and parameters: {'iterations': 2164, 'learning_rate': 0.05728181262832203, 'depth': 10, 'l2_leaf_reg': 0.1256634462701612, 'subsample': 0.9186475663441289, 'random_strength': 0.07670932559916643, 'bagging_temperature': 0.999551196552215}. Best is trial 41 with value: 96387.86640381806.\n",
      "[I 2025-07-24 12:09:23,023] Trial 51 finished with value: 97440.81688461849 and parameters: {'iterations': 1831, 'learning_rate': 0.05623645781813692, 'depth': 10, 'l2_leaf_reg': 0.12867314504772415, 'subsample': 0.9165671185135169, 'random_strength': 0.0010388627948178261, 'bagging_temperature': 0.989793026997118}. Best is trial 41 with value: 96387.86640381806.\n",
      "[I 2025-07-24 12:11:24,980] Trial 52 finished with value: 97629.69064422471 and parameters: {'iterations': 2134, 'learning_rate': 0.060389319213238396, 'depth': 10, 'l2_leaf_reg': 0.2467696720872884, 'subsample': 0.9509366953755568, 'random_strength': 0.055252629872371374, 'bagging_temperature': 0.899156896054285}. Best is trial 41 with value: 96387.86640381806.\n",
      "[I 2025-07-24 12:14:04,745] Trial 53 finished with value: 97571.92819661804 and parameters: {'iterations': 2899, 'learning_rate': 0.07802980092025948, 'depth': 10, 'l2_leaf_reg': 0.07900989219505267, 'subsample': 0.8768260908329781, 'random_strength': 0.005646393211022574, 'bagging_temperature': 0.9457568090018476}. Best is trial 41 with value: 96387.86640381806.\n",
      "[I 2025-07-24 12:15:42,838] Trial 54 finished with value: 97922.59715078899 and parameters: {'iterations': 1737, 'learning_rate': 0.048858700271484, 'depth': 10, 'l2_leaf_reg': 0.6487554030518202, 'subsample': 0.9288944733576318, 'random_strength': 0.002219654024817784, 'bagging_temperature': 0.8472205516445286}. Best is trial 41 with value: 96387.86640381806.\n",
      "[I 2025-07-24 12:16:23,161] Trial 55 finished with value: 97985.69754659582 and parameters: {'iterations': 1532, 'learning_rate': 0.06858249331603139, 'depth': 9, 'l2_leaf_reg': 0.4233994961745973, 'subsample': 0.9607676504587843, 'random_strength': 0.44231553285760555, 'bagging_temperature': 0.7585284317815681}. Best is trial 41 with value: 96387.86640381806.\n",
      "[I 2025-07-24 12:17:19,259] Trial 56 finished with value: 99139.3963868228 and parameters: {'iterations': 1002, 'learning_rate': 0.05390915572391072, 'depth': 10, 'l2_leaf_reg': 0.03912589184514691, 'subsample': 0.9017765936036869, 'random_strength': 0.1482297971262938, 'bagging_temperature': 0.6771469696185073}. Best is trial 41 with value: 96387.86640381806.\n",
      "[I 2025-07-24 12:18:34,967] Trial 57 finished with value: 97353.34689612078 and parameters: {'iterations': 2799, 'learning_rate': 0.0996833333349875, 'depth': 9, 'l2_leaf_reg': 0.02094128230871104, 'subsample': 0.9223829782101888, 'random_strength': 0.01230322026596758, 'bagging_temperature': 0.9600789385105557}. Best is trial 41 with value: 96387.86640381806.\n",
      "[I 2025-07-24 12:21:12,792] Trial 58 finished with value: 97271.34640073581 and parameters: {'iterations': 2926, 'learning_rate': 0.04715018802580957, 'depth': 10, 'l2_leaf_reg': 0.27597295968953484, 'subsample': 0.8307288010673215, 'random_strength': 0.07494854522427996, 'bagging_temperature': 0.8162153191633112}. Best is trial 41 with value: 96387.86640381806.\n",
      "[I 2025-07-24 12:22:21,045] Trial 59 finished with value: 97084.66565785791 and parameters: {'iterations': 2721, 'learning_rate': 0.06615053039920428, 'depth': 9, 'l2_leaf_reg': 0.1561915483577456, 'subsample': 0.7109817403677122, 'random_strength': 0.002551646586906846, 'bagging_temperature': 0.8616161115617931}. Best is trial 41 with value: 96387.86640381806.\n",
      "[I 2025-07-24 12:24:32,127] Trial 60 finished with value: 97379.16121722298 and parameters: {'iterations': 2332, 'learning_rate': 0.087740887595981, 'depth': 10, 'l2_leaf_reg': 0.8666422845522299, 'subsample': 0.8590656953267355, 'random_strength': 0.6317828799776269, 'bagging_temperature': 0.912961651143374}. Best is trial 41 with value: 96387.86640381806.\n",
      "[I 2025-07-24 12:27:00,506] Trial 61 finished with value: 97136.93368148016 and parameters: {'iterations': 2641, 'learning_rate': 0.06044392802953519, 'depth': 10, 'l2_leaf_reg': 0.3929581778033546, 'subsample': 0.9092135551152858, 'random_strength': 0.0030023587413005055, 'bagging_temperature': 0.9306044075477705}. Best is trial 41 with value: 96387.86640381806.\n",
      "[I 2025-07-24 12:29:18,324] Trial 62 finished with value: 97422.46091587482 and parameters: {'iterations': 2486, 'learning_rate': 0.0635897704489072, 'depth': 10, 'l2_leaf_reg': 0.5903627258684203, 'subsample': 0.8809711177522583, 'random_strength': 0.0035552297928739883, 'bagging_temperature': 0.9909594118427462}. Best is trial 41 with value: 96387.86640381806.\n",
      "[I 2025-07-24 12:31:47,746] Trial 63 finished with value: 97269.076589221 and parameters: {'iterations': 2677, 'learning_rate': 0.07694480776352403, 'depth': 10, 'l2_leaf_reg': 0.2834422147349443, 'subsample': 0.8968945172616036, 'random_strength': 0.001140019790185941, 'bagging_temperature': 0.7808355565772493}. Best is trial 41 with value: 96387.86640381806.\n",
      "[I 2025-07-24 12:33:01,005] Trial 64 finished with value: 97027.30983929746 and parameters: {'iterations': 2801, 'learning_rate': 0.04351005719825513, 'depth': 9, 'l2_leaf_reg': 0.11801459067340825, 'subsample': 0.9342765924599825, 'random_strength': 0.0015618705768765265, 'bagging_temperature': 0.6072463317775018}. Best is trial 41 with value: 96387.86640381806.\n",
      "[I 2025-07-24 12:34:15,484] Trial 65 finished with value: 97063.3253639336 and parameters: {'iterations': 2798, 'learning_rate': 0.03859254273699587, 'depth': 9, 'l2_leaf_reg': 0.08737959446649837, 'subsample': 0.969411314553904, 'random_strength': 0.26190981415417763, 'bagging_temperature': 0.5972351691055426}. Best is trial 41 with value: 96387.86640381806.\n",
      "[I 2025-07-24 12:35:13,509] Trial 66 finished with value: 97669.42586215743 and parameters: {'iterations': 2232, 'learning_rate': 0.04280006382799025, 'depth': 9, 'l2_leaf_reg': 0.14204485320438437, 'subsample': 0.935346531895943, 'random_strength': 0.0019404474104084875, 'bagging_temperature': 0.7433217608456264}. Best is trial 41 with value: 96387.86640381806.\n",
      "[I 2025-07-24 12:36:25,516] Trial 67 finished with value: 97109.5798582563 and parameters: {'iterations': 2924, 'learning_rate': 0.03402052520785327, 'depth': 9, 'l2_leaf_reg': 0.19624579817209234, 'subsample': 0.7116820874836262, 'random_strength': 0.943119218449895, 'bagging_temperature': 0.6067998012048315}. Best is trial 41 with value: 96387.86640381806.\n",
      "[I 2025-07-24 12:37:12,065] Trial 68 finished with value: 98440.93107327192 and parameters: {'iterations': 2723, 'learning_rate': 0.029838780624263057, 'depth': 8, 'l2_leaf_reg': 0.047918474621243025, 'subsample': 0.9421002432865608, 'random_strength': 0.0048929031312947005, 'bagging_temperature': 0.7011928573018605}. Best is trial 41 with value: 96387.86640381806.\n",
      "[I 2025-07-24 12:38:26,896] Trial 69 finished with value: 97233.98906567329 and parameters: {'iterations': 2963, 'learning_rate': 0.047039725551783776, 'depth': 9, 'l2_leaf_reg': 0.002468996632232976, 'subsample': 0.7394259488880747, 'random_strength': 0.02510132569666926, 'bagging_temperature': 0.6841300975784126}. Best is trial 41 with value: 96387.86640381806.\n",
      "[I 2025-07-24 12:39:36,894] Trial 70 finished with value: 97061.48353485599 and parameters: {'iterations': 2818, 'learning_rate': 0.0531278438414749, 'depth': 9, 'l2_leaf_reg': 0.028886306240397352, 'subsample': 0.7307947155607508, 'random_strength': 0.0015393597098084697, 'bagging_temperature': 0.5159038609460022}. Best is trial 41 with value: 96387.86640381806.\n",
      "[I 2025-07-24 12:42:00,702] Trial 71 finished with value: 96964.73173871462 and parameters: {'iterations': 2590, 'learning_rate': 0.057895127706704, 'depth': 10, 'l2_leaf_reg': 0.3218658276938528, 'subsample': 0.8911281031871249, 'random_strength': 0.0010081189981592403, 'bagging_temperature': 0.8296052778715074}. Best is trial 41 with value: 96387.86640381806.\n",
      "[I 2025-07-24 12:44:27,250] Trial 72 finished with value: 97066.768432468 and parameters: {'iterations': 2601, 'learning_rate': 0.05777402405756851, 'depth': 10, 'l2_leaf_reg': 0.10994229478254934, 'subsample': 0.9179852470954821, 'random_strength': 0.0010553328454238503, 'bagging_temperature': 0.7981917649020719}. Best is trial 41 with value: 96387.86640381806.\n",
      "[I 2025-07-24 12:46:17,979] Trial 73 finished with value: 97480.70832854744 and parameters: {'iterations': 2007, 'learning_rate': 0.05231314816669163, 'depth': 10, 'l2_leaf_reg': 1.207334547104817, 'subsample': 0.8838390880166453, 'random_strength': 0.002392945496266115, 'bagging_temperature': 0.8315427194347695}. Best is trial 41 with value: 96387.86640381806.\n",
      "[I 2025-07-24 12:48:38,406] Trial 74 finished with value: 96983.14124804056 and parameters: {'iterations': 2784, 'learning_rate': 0.04460488272254511, 'depth': 10, 'l2_leaf_reg': 1.842788310268277, 'subsample': 0.7009012727726912, 'random_strength': 0.0014506409248864756, 'bagging_temperature': 0.8673563381275617}. Best is trial 41 with value: 96387.86640381806.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- CatBoost Tuning Complete ---\n",
      "Best trial validation RMSE: $96,387.87\n",
      "Best hyperparameters found for CatBoost:\n",
      "  'iterations': 2959,\n",
      "  'learning_rate': 0.0604931337615656,\n",
      "  'depth': 9,\n",
      "  'l2_leaf_reg': 0.1195942543993236,\n",
      "  'subsample': 0.7215144912978994,\n",
      "  'random_strength': 0.0011789533818766123,\n",
      "  'bagging_temperature': 0.8080844703507192,\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# BLOCK 3: TUNE CATBOOST MEAN MODEL\n",
    "# =============================================================================\n",
    "import catboost as cb\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import optuna\n",
    "\n",
    "# --- 1. Prepare Data for Tuning ---\n",
    "# We create a single, smaller train/validation split from the full dataset\n",
    "# This makes the tuning process much faster than using K-Folds for every trial.\n",
    "X_train_opt, X_val_opt, y_train_opt, y_val_opt = train_test_split(\n",
    "    X, y_true, test_size=0.2, random_state=RANDOM_STATE\n",
    ")\n",
    "\n",
    "# --- 2. Define the Optuna Objective Function for CatBoost ---\n",
    "def objective_catboost(trial):\n",
    "    \"\"\"\n",
    "    This function takes an Optuna 'trial' object and does the following:\n",
    "    1. Defines a search space for CatBoost's hyperparameters.\n",
    "    2. Trains a CatBoost model with a set of hyperparameters suggested by the trial.\n",
    "    3. Evaluates the model on the validation set.\n",
    "    4. Returns the validation score (RMSE), which Optuna tries to minimize.\n",
    "    \"\"\"\n",
    "    # Define the hyperparameter search space\n",
    "    params = {\n",
    "        'iterations': trial.suggest_int('iterations', 1000, 3000),\n",
    "        'learning_rate': trial.suggest_float('learning_rate', 0.01, 0.1, log=True),\n",
    "        'depth': trial.suggest_int('depth', 6, 10),\n",
    "        'l2_leaf_reg': trial.suggest_float('l2_leaf_reg', 1e-3, 10.0, log=True),\n",
    "        'subsample': trial.suggest_float('subsample', 0.7, 1.0),\n",
    "        'random_strength': trial.suggest_float('random_strength', 1e-3, 10.0, log=True),\n",
    "        'bagging_temperature': trial.suggest_float('bagging_temperature', 0.0, 1.0),\n",
    "        \n",
    "        # Fixed parameters\n",
    "        'loss_function': 'RMSE',\n",
    "        'eval_metric': 'RMSE',\n",
    "        'random_seed': RANDOM_STATE,\n",
    "        'verbose': 0,  # Suppress verbose output during training\n",
    "        'early_stopping_rounds': 100\n",
    "    }\n",
    "\n",
    "    # Initialize and train the CatBoost model with the suggested parameters\n",
    "    model = cb.CatBoostRegressor(**params)\n",
    "    model.fit(\n",
    "        X_train_opt, y_train_opt,\n",
    "        eval_set=[(X_val_opt, y_val_opt)],\n",
    "        use_best_model=True\n",
    "    )\n",
    "\n",
    "    # Make predictions on the validation set\n",
    "    preds = model.predict(X_val_opt)\n",
    "\n",
    "    # Calculate and return the Root Mean Squared Error\n",
    "    rmse = np.sqrt(mean_squared_error(y_val_opt, preds))\n",
    "    return rmse\n",
    "\n",
    "# --- 3. Create and Run the Optuna Study ---\n",
    "# Create a study object and specify the direction is 'minimize' (for RMSE)\n",
    "study_catboost = optuna.create_study(direction='minimize')\n",
    "\n",
    "# Start the optimization process. \n",
    "# n_trials can be increased for a more thorough search, but this is a good start.\n",
    "print(\"--- Starting CatBoost Hyperparameter Tuning... ---\")\n",
    "study_catboost.optimize(objective_catboost, n_trials=N_OPTUNA_TRIALS)\n",
    "\n",
    "# --- 4. Print the Best Results ---\n",
    "print(\"\\n--- CatBoost Tuning Complete ---\")\n",
    "print(f\"Best trial validation RMSE: ${study_catboost.best_value:,.2f}\")\n",
    "print(\"Best hyperparameters found for CatBoost:\")\n",
    "for key, value in study_catboost.best_params.items():\n",
    "    print(f\"  '{key}': {value},\")\n",
    "\n",
    "# Store the best parameters in a dictionary for later use in the K-Fold training loop\n",
    "best_params_catboost = study_catboost.best_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4e74c7d0-5c30-4f81-8ca3-c806d7a36b17",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- STAGE 1, PART 2: K-Fold Training of CatBoost Mean Model ---\n",
      "# Using the optimal hyperparameters found by Optuna.\n",
      "  Training CatBoost Mean Model - Fold 1/5...\n",
      "  Training CatBoost Mean Model - Fold 2/5...\n",
      "  Training CatBoost Mean Model - Fold 3/5...\n",
      "  Training CatBoost Mean Model - Fold 4/5...\n",
      "  Training CatBoost Mean Model - Fold 5/5...\n",
      "\n",
      "--- CatBoost K-Fold Training Complete & Performance Metrics ---\n",
      "CatBoost Final OOF RMSE: $96,371.94\n",
      "Reference XGBoost RMSE (from winner_v1_301): $98,990.27\n",
      "\n",
      "CONCLUSION: SUCCESS! The tuned CatBoost model is MORE accurate than the previous XGBoost model.\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# BLOCK 4: K-FOLD TRAINING OF CATBOOST MEAN MODEL\n",
    "# =============================================================================\n",
    "import catboost as cb\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "print(\"\\n--- STAGE 1, PART 2: K-Fold Training of CatBoost Mean Model ---\")\n",
    "print(\"# Using the optimal hyperparameters found by Optuna.\")\n",
    "\n",
    "# --- 1. Setup K-Fold and Prediction Arrays ---\n",
    "# Use StratifiedKFold on 'grade' to ensure consistent splits across models\n",
    "skf = StratifiedKFold(n_splits=N_SPLITS, shuffle=True, random_state=RANDOM_STATE)\n",
    "grade_for_stratify = pd.read_csv(DATA_PATH + 'dataset.csv')['grade']\n",
    "\n",
    "# Initialize arrays to store the out-of-fold (OOF) and test set predictions\n",
    "oof_catboost_preds = np.zeros(len(X))\n",
    "test_catboost_preds = np.zeros(len(X_test))\n",
    "\n",
    "# --- 2. Combine Tuned Params with Fixed Params ---\n",
    "# The best_params_catboost dictionary should be in memory from the previous cell\n",
    "# We add the fixed parameters needed for CatBoost training here\n",
    "final_params_catboost = {\n",
    "    'loss_function': 'RMSE',\n",
    "    'eval_metric': 'RMSE',\n",
    "    'random_seed': RANDOM_STATE,\n",
    "    'verbose': 0,  # Suppress in-loop verbosity for cleaner output\n",
    "    'early_stopping_rounds': 100,\n",
    "    **best_params_catboost # Unpack the tuned hyperparameters\n",
    "}\n",
    "\n",
    "# --- 3. K-Fold Training Loop ---\n",
    "for fold, (train_idx, val_idx) in enumerate(skf.split(X, grade_for_stratify)):\n",
    "    print(f\"  Training CatBoost Mean Model - Fold {fold+1}/{N_SPLITS}...\")\n",
    "    \n",
    "    # Split the data for the current fold\n",
    "    X_train, X_val = X.iloc[train_idx], X.iloc[val_idx]\n",
    "    y_train, y_val = y_true.iloc[train_idx], y_true.iloc[val_idx]\n",
    "\n",
    "    # Initialize and train the CatBoost model for this fold\n",
    "    model = cb.CatBoostRegressor(**final_params_catboost)\n",
    "    model.fit(\n",
    "        X_train, y_train,\n",
    "        eval_set=[(X_val, y_val)],\n",
    "        use_best_model=True\n",
    "    )\n",
    "\n",
    "    # Generate and store predictions\n",
    "    # OOF predictions are made on the validation set for this fold\n",
    "    oof_catboost_preds[val_idx] = model.predict(X_val)\n",
    "    # Test predictions are averaged across all 5 fold models\n",
    "    test_catboost_preds += model.predict(X_test) / N_SPLITS\n",
    "    \n",
    "    gc.collect()\n",
    "\n",
    "# --- 4. Final Metrics and Comparison ---\n",
    "print(\"\\n--- CatBoost K-Fold Training Complete & Performance Metrics ---\")\n",
    "\n",
    "# Calculate the final RMSE score across all OOF predictions\n",
    "final_catboost_oof_rmse = np.sqrt(mean_squared_error(y_true, oof_catboost_preds))\n",
    "print(f\"CatBoost Final OOF RMSE: ${final_catboost_oof_rmse:,.2f}\")\n",
    "\n",
    "# For direct comparison, here is the score from your best XGBoost model\n",
    "# This value is taken from the logs of the winner_v1_301 notebook\n",
    "reference_xgb_rmse = 98990.27\n",
    "print(f\"Reference XGBoost RMSE (from winner_v1_301): ${reference_xgb_rmse:,.2f}\")\n",
    "\n",
    "# Provide a clear conclusion\n",
    "if final_catboost_oof_rmse < reference_xgb_rmse:\n",
    "    print(\"\\nCONCLUSION: SUCCESS! The tuned CatBoost model is MORE accurate than the previous XGBoost model.\")\n",
    "else:\n",
    "    print(\"\\nCONCLUSION: The tuned CatBoost model is LESS accurate than the previous XGBoost model.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8d6f98d0-60b3-45cf-b367-2929d2873147",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "'oof_cb_preds.npy' and 'test_cb_preds.npy' saved successfully.\n"
     ]
    }
   ],
   "source": [
    "# Define the save path and save the final prediction arrays\n",
    "SAVE_PATH = './mean_models_v1/'\n",
    "os.makedirs(SAVE_PATH, exist_ok=True)\n",
    "np.save(os.path.join(SAVE_PATH, 'oof_cb_preds.npy'), oof_catboost_preds)\n",
    "np.save(os.path.join(SAVE_PATH, 'test_cb_preds.npy'), test_catboost_preds)\n",
    "print(\"\\n'oof_cb_preds.npy' and 'test_cb_preds.npy' saved successfully.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b93a7b53-969c-4293-acc6-c3e7356f5368",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (Kaggle Comp)",
   "language": "python",
   "name": "kaggle-comp"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

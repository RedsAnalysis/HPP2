{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "54958bce-ecab-43cf-b221-7ea3b4552a3a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Libraries imported successfully.\n",
      "Raw data loaded successfully.\n",
      "Setup complete.\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# BLOCK 1: SETUP, IMPORTS, AND DATA LOADING\n",
    "# =============================================================================\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "import time\n",
    "import os\n",
    "# --- Library Imports ---\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import gc\n",
    "from sklearn.model_selection import StratifiedKFold, train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import xgboost as xgb\n",
    "import catboost as cb\n",
    "import optuna\n",
    "print(\"Libraries imported successfully.\")\n",
    "# --- Helper Function for Winkler Score ---\n",
    "def winkler_score(y_true, lower, upper, alpha=0.1, return_coverage=False):\n",
    "    width = upper - lower\n",
    "    penalty_lower = np.where(y_true < lower, (2 / alpha) * (lower - y_true), 0)\n",
    "    penalty_upper = np.where(y_true > upper, (2 / alpha) * (y_true - upper), 0)\n",
    "    score = width + penalty_lower + penalty_upper\n",
    "    if return_coverage:\n",
    "        coverage = np.mean((y_true >= lower) & (y_true <= upper))\n",
    "        return np.mean(score), coverage\n",
    "    return np.mean(score)\n",
    "# --- Global Constants ---\n",
    "N_SPLITS = 5\n",
    "RANDOM_STATE = 42\n",
    "DATA_PATH = './'\n",
    "N_OPTUNA_TRIALS = 75 # A strong number for a comprehensive search\n",
    "COMPETITION_ALPHA = 0.1\n",
    "\n",
    "# --- Load Raw Data ---\n",
    "try:\n",
    "    # We drop the low-variance columns they identified right away\n",
    "    drop_cols=['id', 'golf', 'view_rainier', 'view_skyline', 'view_lakesamm','view_otherwater', 'view_other']\n",
    "    df_train = pd.read_csv(DATA_PATH + 'dataset.csv').drop(columns=drop_cols)\n",
    "    df_test = pd.read_csv(DATA_PATH + 'test.csv').drop(columns=drop_cols)\n",
    "    print(\"Raw data loaded successfully.\")\n",
    "except FileNotFoundError:\n",
    "    print(\"ERROR: Could not find 'dataset.csv' or 'test.csv'.\")\n",
    "    exit()\n",
    "# --- Prepare Target Variable ---\n",
    "y_true = df_train['sale_price'].copy()\n",
    "grade_for_stratify = df_train['grade'].copy()\n",
    "# The mean-error model works best when predicting the raw price directly\n",
    "# So, we will NOT log-transform the target this time.\n",
    "# df_train.drop('sale_price', axis=1, inplace=True) # We keep sale_price for FE\n",
    "print(\"Setup complete.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fe5b1e10-dca7-4b0e-9f8b-90b0196ab8cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Starting Block 2.5: Executing Feature Engineering Pipeline ---\n",
      "--- Starting Comprehensive Feature Engineering ---\n",
      "Step 1: Creating brute-force numerical interaction features...\n",
      "Step 2: Creating date features...\n",
      "Step 3: Creating TF-IDF features for text columns...\n",
      "Step 4: Creating group-by aggregation features...\n",
      "Step 5: Creating ratio features...\n",
      "Step 6: Creating geospatial clustering features...\n",
      "Step 7: Finalizing feature set...\n",
      "\n",
      "Comprehensive FE complete. Total features: 233\n",
      "Feature engineering complete. X shape: (200000, 233), X_test shape: (200000, 233)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Make sure to have these libraries installed\n",
    "# pip install pandas numpy scikit-learn\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.cluster import KMeans\n",
    "import gc\n",
    "\n",
    "# Define a random state for reproducibility\n",
    "RANDOM_STATE = 42\n",
    "\n",
    "def create_comprehensive_features(df_train, df_test):\n",
    "    \"\"\"\n",
    "    Combines original and new advanced feature engineering steps into a single pipeline.\n",
    "    \"\"\"\n",
    "    print(\"--- Starting Comprehensive Feature Engineering ---\")\n",
    "\n",
    "    # Store original indices and target variable\n",
    "    train_ids = df_train.index\n",
    "    test_ids = df_test.index\n",
    "    y_train = df_train['sale_price'].copy() # Keep the target separate\n",
    "\n",
    "    # Combine for consistent processing\n",
    "    df_train_temp = df_train.drop(columns=['sale_price'])\n",
    "    all_data = pd.concat([df_train_temp, df_test], axis=0, ignore_index=True)\n",
    "\n",
    "    # --- Original Feature Engineering ---\n",
    "\n",
    "    # A) Brute-Force Numerical Interactions\n",
    "    print(\"Step 1: Creating brute-force numerical interaction features...\")\n",
    "    NUMS = ['area', 'land_val', 'imp_val', 'sqft_lot', 'sqft', 'sqft_1', 'grade', 'year_built']\n",
    "    # Ensure all columns exist and are numeric, fill missing with 0 for safety\n",
    "    for col in NUMS:\n",
    "        if col not in all_data.columns:\n",
    "            all_data[col] = 0\n",
    "        else:\n",
    "            all_data[col] = pd.to_numeric(all_data[col], errors='coerce').fillna(0)\n",
    "            \n",
    "    for i in range(len(NUMS)):\n",
    "        for j in range(i + 1, len(NUMS)):\n",
    "            all_data[f'{NUMS[i]}_x_{NUMS[j]}'] = all_data[NUMS[i]] * all_data[NUMS[j]]\n",
    "\n",
    "    # B) Date Features\n",
    "    print(\"Step 2: Creating date features...\")\n",
    "    all_data['sale_date'] = pd.to_datetime(all_data['sale_date'])\n",
    "    all_data['sale_year'] = all_data['sale_date'].dt.year\n",
    "    all_data['sale_month'] = all_data['sale_date'].dt.month\n",
    "    all_data['sale_dayofyear'] = all_data['sale_date'].dt.dayofyear\n",
    "    all_data['age_at_sale'] = all_data['sale_year'] - all_data['year_built']\n",
    "\n",
    "    # C) TF-IDF Text Features\n",
    "    print(\"Step 3: Creating TF-IDF features for text columns...\")\n",
    "    text_cols = ['subdivision', 'zoning', 'city', 'sale_warning', 'join_status', 'submarket']\n",
    "    all_data[text_cols] = all_data[text_cols].fillna('missing').astype(str)\n",
    "    \n",
    "    for col in text_cols:\n",
    "        tfidf = TfidfVectorizer(analyzer='char', ngram_range=(3, 5), max_features=128, binary=True)\n",
    "        svd = TruncatedSVD(n_components=8, random_state=RANDOM_STATE)\n",
    "        \n",
    "        tfidf_matrix = tfidf.fit_transform(all_data[col])\n",
    "        tfidf_svd = svd.fit_transform(tfidf_matrix)\n",
    "        \n",
    "        tfidf_df = pd.DataFrame(tfidf_svd, columns=[f'{col}_tfidf_svd_{i}' for i in range(8)])\n",
    "        all_data = pd.concat([all_data, tfidf_df], axis=1)\n",
    "\n",
    "    # D) Log transform some interaction features\n",
    "    for c in ['land_val_x_imp_val', 'land_val_x_sqft', 'imp_val_x_sqft']:\n",
    "        if c in all_data.columns:\n",
    "            all_data[c] = np.log1p(all_data[c].fillna(0))\n",
    "\n",
    "    # --- New Feature Engineering Ideas ---\n",
    "\n",
    "    # F) Group-By Aggregation Features\n",
    "    print(\"Step 4: Creating group-by aggregation features...\")\n",
    "    group_cols = ['submarket', 'city', 'zoning']\n",
    "    num_cols_for_agg = ['grade', 'sqft', 'imp_val', 'land_val', 'age_at_sale']\n",
    "\n",
    "    for group_col in group_cols:\n",
    "        for num_col in num_cols_for_agg:\n",
    "            agg_stats = all_data.groupby(group_col)[num_col].agg(['mean', 'std', 'max', 'min']).reset_index()\n",
    "            agg_stats.columns = [group_col] + [f'{group_col}_{num_col}_{stat}' for stat in ['mean', 'std', 'max', 'min']]\n",
    "            all_data = pd.merge(all_data, agg_stats, on=group_col, how='left')\n",
    "            all_data[f'{num_col}_minus_{group_col}_mean'] = all_data[num_col] - all_data[f'{group_col}_{num_col}_mean']\n",
    "\n",
    "    # G) Ratio Features\n",
    "    print(\"Step 5: Creating ratio features...\")\n",
    "    # Add a small epsilon to prevent division by zero\n",
    "    epsilon = 1e-6 \n",
    "    all_data['total_val'] = all_data['imp_val'] + all_data['land_val']\n",
    "    all_data['imp_val_to_land_val_ratio'] = all_data['imp_val'] / (all_data['land_val'] + epsilon)\n",
    "    all_data['land_val_ratio'] = all_data['land_val'] / (all_data['total_val'] + epsilon)\n",
    "    all_data['sqft_to_lot_ratio'] = all_data['sqft'] / (all_data['sqft_lot'] + epsilon)\n",
    "    all_data['was_renovated'] = (all_data['year_reno'] > 0).astype(int)\n",
    "    all_data['reno_age_at_sale'] = np.where(all_data['was_renovated'] == 1, all_data['sale_year'] - all_data['year_reno'], -1)\n",
    "\n",
    "    # H) Geospatial Clustering Features\n",
    "    print(\"Step 6: Creating geospatial clustering features...\")\n",
    "    coords = all_data[['latitude', 'longitude']].copy()\n",
    "    coords.fillna(coords.median(), inplace=True) # Simple imputation\n",
    "\n",
    "    # KMeans is sensitive to feature scaling, but for lat/lon it's often okay without it.\n",
    "    kmeans = KMeans(n_clusters=20, random_state=RANDOM_STATE, n_init=10) \n",
    "    all_data['location_cluster'] = kmeans.fit_predict(coords)\n",
    "    \n",
    "    # Calculate distance to each cluster center\n",
    "    cluster_centers = kmeans.cluster_centers_\n",
    "    for i in range(len(cluster_centers)):\n",
    "        center = cluster_centers[i]\n",
    "        all_data[f'dist_to_cluster_{i}'] = np.sqrt((coords['latitude'] - center[0])**2 + (coords['longitude'] - center[1])**2)\n",
    "\n",
    "    # --- Final Cleanup ---\n",
    "    print(\"Step 7: Finalizing feature set...\")\n",
    "    cols_to_drop = ['sale_date', 'subdivision', 'zoning', 'city', 'sale_warning', 'join_status', 'submarket']\n",
    "    all_data = all_data.drop(columns=cols_to_drop)\n",
    "\n",
    "    # One-hot encode the new cluster feature\n",
    "    all_data = pd.get_dummies(all_data, columns=['location_cluster'], prefix='loc_cluster')\n",
    "    \n",
    "    # Final check for any remaining object columns to be safe (besides index)\n",
    "    object_cols = all_data.select_dtypes(include='object').columns\n",
    "    if len(object_cols) > 0:\n",
    "        print(f\"Warning: Found unexpected object columns: {object_cols}. Dropping them.\")\n",
    "        all_data = all_data.drop(columns=object_cols)\n",
    "        \n",
    "    all_data.fillna(0, inplace=True)\n",
    "\n",
    "    # Separate back into train and test sets\n",
    "    train_len = len(train_ids)\n",
    "    X = all_data.iloc[:train_len].copy()\n",
    "    X_test = all_data.iloc[train_len:].copy()\n",
    "    \n",
    "    # Restore original indices\n",
    "    X.index = train_ids\n",
    "    X_test.index = test_ids\n",
    "    \n",
    "    # Align columns - crucial for model prediction\n",
    "    X_test = X_test[X.columns]\n",
    "    \n",
    "    print(f\"\\nComprehensive FE complete. Total features: {X.shape[1]}\")\n",
    "    gc.collect()\n",
    "    \n",
    "    return X, X_test, y_train\n",
    "# =============================================================================\n",
    "# BLOCK 2.5: EXECUTE FEATURE ENGINEERING\n",
    "# =============================================================================\n",
    "print(\"\\n--- Starting Block 2.5: Executing Feature Engineering Pipeline ---\")\n",
    "\n",
    "# This is the crucial step that was missing.\n",
    "# We call the function to create our training and testing dataframes.\n",
    "X, X_test, y_train = create_comprehensive_features(df_train, df_test)\n",
    "\n",
    "# Let's verify the output\n",
    "print(f\"Feature engineering complete. X shape: {X.shape}, X_test shape: {X_test.shape}\")\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7cca7ba3-d33f-4a36-824a-8ebab2f8980a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-07-24 15:32:39,480] A new study created in memory with name: no-name-1ed9314c-8822-45b4-88dc-da215d2c9764\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Starting LightGBM Hyperparameter Tuning... ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-07-24 15:33:13,243] Trial 0 finished with value: 102657.84906543046 and parameters: {'learning_rate': 0.0694158863341818, 'n_estimators': 2350, 'num_leaves': 72, 'max_depth': 9, 'lambda_l1': 1.9858992218372273, 'lambda_l2': 0.09996862401592421, 'feature_fraction': 0.9101103081177001, 'bagging_fraction': 0.9166550428530107, 'bagging_freq': 5}. Best is trial 0 with value: 102657.84906543046.\n",
      "[I 2025-07-24 15:33:39,645] Trial 1 finished with value: 103187.11873057194 and parameters: {'learning_rate': 0.0628655381280119, 'n_estimators': 2532, 'num_leaves': 72, 'max_depth': 7, 'lambda_l1': 0.10842533445489826, 'lambda_l2': 0.013299799605664488, 'feature_fraction': 0.8860513122252763, 'bagging_fraction': 0.908309782389596, 'bagging_freq': 1}. Best is trial 0 with value: 102657.84906543046.\n",
      "[I 2025-07-24 15:34:05,072] Trial 2 finished with value: 103098.20971078215 and parameters: {'learning_rate': 0.050330960512912686, 'n_estimators': 2949, 'num_leaves': 61, 'max_depth': 7, 'lambda_l1': 0.006471189124788026, 'lambda_l2': 1.1210261810235835, 'feature_fraction': 0.8207551730344295, 'bagging_fraction': 0.7871344282583159, 'bagging_freq': 7}. Best is trial 0 with value: 102657.84906543046.\n",
      "[I 2025-07-24 15:34:29,719] Trial 3 finished with value: 107812.72085067832 and parameters: {'learning_rate': 0.01204949354071343, 'n_estimators': 2249, 'num_leaves': 77, 'max_depth': 7, 'lambda_l1': 0.03589520308890275, 'lambda_l2': 0.06500770384309162, 'feature_fraction': 0.8042852444919801, 'bagging_fraction': 0.8643848450955025, 'bagging_freq': 2}. Best is trial 0 with value: 102657.84906543046.\n",
      "[I 2025-07-24 15:34:56,541] Trial 4 finished with value: 103018.69721683554 and parameters: {'learning_rate': 0.08729565472093627, 'n_estimators': 2512, 'num_leaves': 71, 'max_depth': 10, 'lambda_l1': 0.04993362155078393, 'lambda_l2': 0.002366545950731751, 'feature_fraction': 0.9996143848196353, 'bagging_fraction': 0.9262358977348442, 'bagging_freq': 5}. Best is trial 0 with value: 102657.84906543046.\n",
      "[I 2025-07-24 15:35:06,413] Trial 5 finished with value: 107730.23081461866 and parameters: {'learning_rate': 0.04723712717113508, 'n_estimators': 1234, 'num_leaves': 25, 'max_depth': 7, 'lambda_l1': 0.0038595989470098543, 'lambda_l2': 0.0019462038694518361, 'feature_fraction': 0.7544904861441577, 'bagging_fraction': 0.7547952224416459, 'bagging_freq': 1}. Best is trial 0 with value: 102657.84906543046.\n",
      "[I 2025-07-24 15:35:29,712] Trial 6 finished with value: 107260.64142509084 and parameters: {'learning_rate': 0.016420575058426156, 'n_estimators': 2518, 'num_leaves': 60, 'max_depth': 6, 'lambda_l1': 0.07166257322241856, 'lambda_l2': 0.71426272829575, 'feature_fraction': 0.7954145734401061, 'bagging_fraction': 0.7986046577309259, 'bagging_freq': 7}. Best is trial 0 with value: 102657.84906543046.\n",
      "[I 2025-07-24 15:36:01,902] Trial 7 finished with value: 102916.76236465457 and parameters: {'learning_rate': 0.03819396603518615, 'n_estimators': 2971, 'num_leaves': 97, 'max_depth': 9, 'lambda_l1': 0.041329885247443486, 'lambda_l2': 0.04704113585146644, 'feature_fraction': 0.8708389632220404, 'bagging_fraction': 0.8778182339053322, 'bagging_freq': 5}. Best is trial 0 with value: 102657.84906543046.\n",
      "[I 2025-07-24 15:36:25,480] Trial 8 finished with value: 104801.05805440337 and parameters: {'learning_rate': 0.03377864691551631, 'n_estimators': 2586, 'num_leaves': 48, 'max_depth': 6, 'lambda_l1': 2.4861999855245984, 'lambda_l2': 0.09853785170755493, 'feature_fraction': 0.9724530446699382, 'bagging_fraction': 0.9902882882960982, 'bagging_freq': 6}. Best is trial 0 with value: 102657.84906543046.\n",
      "[I 2025-07-24 15:36:39,861] Trial 9 finished with value: 108237.27397017559 and parameters: {'learning_rate': 0.0167722119587817, 'n_estimators': 1932, 'num_leaves': 65, 'max_depth': 6, 'lambda_l1': 0.6561466687513446, 'lambda_l2': 7.29947347196611, 'feature_fraction': 0.7008229252508308, 'bagging_fraction': 0.7550369744457354, 'bagging_freq': 5}. Best is trial 0 with value: 102657.84906543046.\n",
      "[I 2025-07-24 15:37:02,956] Trial 10 finished with value: 103446.7783101283 and parameters: {'learning_rate': 0.08960452837602258, 'n_estimators': 1851, 'num_leaves': 94, 'max_depth': 9, 'lambda_l1': 7.1916916200091325, 'lambda_l2': 0.6057796536978672, 'feature_fraction': 0.9489701890114998, 'bagging_fraction': 0.9803799035288959, 'bagging_freq': 3}. Best is trial 0 with value: 102657.84906543046.\n",
      "[I 2025-07-24 15:37:37,528] Trial 11 finished with value: 102924.70409910378 and parameters: {'learning_rate': 0.02961991741774935, 'n_estimators': 2957, 'num_leaves': 100, 'max_depth': 9, 'lambda_l1': 0.465875871641412, 'lambda_l2': 0.01960160515947146, 'feature_fraction': 0.8941779142093584, 'bagging_fraction': 0.8659695152409831, 'bagging_freq': 4}. Best is trial 0 with value: 102657.84906543046.\n",
      "[I 2025-07-24 15:37:55,993] Trial 12 finished with value: 104908.5251633115 and parameters: {'learning_rate': 0.031931526598362354, 'n_estimators': 1522, 'num_leaves': 86, 'max_depth': 9, 'lambda_l1': 0.0010622435401455596, 'lambda_l2': 0.018040030803642353, 'feature_fraction': 0.9023954239162728, 'bagging_fraction': 0.9210937251433984, 'bagging_freq': 5}. Best is trial 0 with value: 102657.84906543046.\n",
      "[I 2025-07-24 15:38:13,262] Trial 13 finished with value: 103576.42972032199 and parameters: {'learning_rate': 0.06219472705889437, 'n_estimators': 2149, 'num_leaves': 45, 'max_depth': 10, 'lambda_l1': 0.5576019196643591, 'lambda_l2': 0.24845590419703845, 'feature_fraction': 0.8620497507047998, 'bagging_fraction': 0.8314842276293116, 'bagging_freq': 4}. Best is trial 0 with value: 102657.84906543046.\n",
      "[I 2025-07-24 15:38:44,737] Trial 14 finished with value: 104171.21274826543 and parameters: {'learning_rate': 0.02299419777109561, 'n_estimators': 2755, 'num_leaves': 86, 'max_depth': 8, 'lambda_l1': 0.015452065357012771, 'lambda_l2': 0.042809187049419925, 'feature_fraction': 0.9317004685848856, 'bagging_fraction': 0.9453839651440857, 'bagging_freq': 6}. Best is trial 0 with value: 102657.84906543046.\n",
      "[I 2025-07-24 15:39:02,594] Trial 15 finished with value: 103801.42563156903 and parameters: {'learning_rate': 0.04613437145554157, 'n_estimators': 1668, 'num_leaves': 86, 'max_depth': 8, 'lambda_l1': 0.17408459802323473, 'lambda_l2': 0.006164264265389196, 'feature_fraction': 0.8523230059189969, 'bagging_fraction': 0.8862798173889727, 'bagging_freq': 3}. Best is trial 0 with value: 102657.84906543046.\n",
      "[I 2025-07-24 15:39:16,098] Trial 16 finished with value: 104901.89984336194 and parameters: {'learning_rate': 0.07027579924957243, 'n_estimators': 2236, 'num_leaves': 20, 'max_depth': 9, 'lambda_l1': 2.110518528790652, 'lambda_l2': 0.24127238614496427, 'feature_fraction': 0.9230465064461181, 'bagging_fraction': 0.7109327850708345, 'bagging_freq': 6}. Best is trial 0 with value: 102657.84906543046.\n",
      "[I 2025-07-24 15:39:27,365] Trial 17 finished with value: 106893.95929274673 and parameters: {'learning_rate': 0.03978043216519239, 'n_estimators': 1109, 'num_leaves': 50, 'max_depth': 10, 'lambda_l1': 7.079825886136122, 'lambda_l2': 2.2845333759008506, 'feature_fraction': 0.8448045852282604, 'bagging_fraction': 0.8316642816254765, 'bagging_freq': 3}. Best is trial 0 with value: 102657.84906543046.\n",
      "[I 2025-07-24 15:39:45,733] Trial 18 finished with value: 107482.40921940737 and parameters: {'learning_rate': 0.02146280150419207, 'n_estimators': 2794, 'num_leaves': 79, 'max_depth': 5, 'lambda_l1': 0.21797050980616242, 'lambda_l2': 0.30669865650831096, 'feature_fraction': 0.7621751116390147, 'bagging_fraction': 0.9647453548348955, 'bagging_freq': 5}. Best is trial 0 with value: 102657.84906543046.\n",
      "[I 2025-07-24 15:40:09,649] Trial 19 finished with value: 102907.9345076487 and parameters: {'learning_rate': 0.07228309192743015, 'n_estimators': 2386, 'num_leaves': 100, 'max_depth': 8, 'lambda_l1': 0.01797764641871383, 'lambda_l2': 0.034364272894794975, 'feature_fraction': 0.9592822041272192, 'bagging_fraction': 0.892061957404401, 'bagging_freq': 4}. Best is trial 0 with value: 102657.84906543046.\n",
      "[I 2025-07-24 15:40:25,145] Trial 20 finished with value: 103444.3384932586 and parameters: {'learning_rate': 0.09971850307134426, 'n_estimators': 2107, 'num_leaves': 34, 'max_depth': 8, 'lambda_l1': 0.01414487961939739, 'lambda_l2': 0.005356570486540713, 'feature_fraction': 0.9736003937088565, 'bagging_fraction': 0.8987655711545662, 'bagging_freq': 4}. Best is trial 0 with value: 102657.84906543046.\n",
      "[I 2025-07-24 15:40:50,739] Trial 21 finished with value: 102616.42157441938 and parameters: {'learning_rate': 0.07490840953367699, 'n_estimators': 2354, 'num_leaves': 99, 'max_depth': 9, 'lambda_l1': 0.025199243596514313, 'lambda_l2': 0.04332012450608868, 'feature_fraction': 0.9458395996561455, 'bagging_fraction': 0.8756007096644876, 'bagging_freq': 4}. Best is trial 21 with value: 102616.42157441938.\n",
      "[I 2025-07-24 15:41:14,877] Trial 22 finished with value: 103233.08055505445 and parameters: {'learning_rate': 0.07507702848901643, 'n_estimators': 2378, 'num_leaves': 93, 'max_depth': 8, 'lambda_l1': 0.016765975011468356, 'lambda_l2': 0.1299652865936886, 'feature_fraction': 0.9500969453034727, 'bagging_fraction': 0.9580593908006861, 'bagging_freq': 3}. Best is trial 21 with value: 102616.42157441938.\n",
      "[I 2025-07-24 15:41:40,470] Trial 23 finished with value: 102909.41603377156 and parameters: {'learning_rate': 0.05857946678205584, 'n_estimators': 2372, 'num_leaves': 90, 'max_depth': 9, 'lambda_l1': 0.005262664574316391, 'lambda_l2': 0.02764324089423399, 'feature_fraction': 0.9124880372167439, 'bagging_fraction': 0.8418168606920197, 'bagging_freq': 4}. Best is trial 21 with value: 102616.42157441938.\n",
      "[I 2025-07-24 15:41:59,616] Trial 24 finished with value: 103422.37200545867 and parameters: {'learning_rate': 0.0791204862497711, 'n_estimators': 2011, 'num_leaves': 79, 'max_depth': 8, 'lambda_l1': 0.002912522858584151, 'lambda_l2': 0.10505375813782139, 'feature_fraction': 0.9948454307694317, 'bagging_fraction': 0.9345725770972091, 'bagging_freq': 2}. Best is trial 21 with value: 102616.42157441938.\n",
      "[I 2025-07-24 15:42:31,766] Trial 25 finished with value: 102464.9075291867 and parameters: {'learning_rate': 0.05379618713379226, 'n_estimators': 2705, 'num_leaves': 100, 'max_depth': 10, 'lambda_l1': 1.5386347727319154, 'lambda_l2': 0.006858713627127976, 'feature_fraction': 0.9463664613274183, 'bagging_fraction': 0.8949971955668848, 'bagging_freq': 4}. Best is trial 25 with value: 102464.9075291867.\n",
      "[I 2025-07-24 15:43:02,867] Trial 26 finished with value: 102318.64613937955 and parameters: {'learning_rate': 0.05298133133188679, 'n_estimators': 2695, 'num_leaves': 88, 'max_depth': 10, 'lambda_l1': 1.8096859697314422, 'lambda_l2': 0.008249452460810758, 'feature_fraction': 0.934011851734392, 'bagging_fraction': 0.8118139754667775, 'bagging_freq': 6}. Best is trial 26 with value: 102318.64613937955.\n",
      "[I 2025-07-24 15:43:32,897] Trial 27 finished with value: 102243.7951826879 and parameters: {'learning_rate': 0.05780781818677885, 'n_estimators': 2712, 'num_leaves': 89, 'max_depth': 10, 'lambda_l1': 1.0150528099348823, 'lambda_l2': 0.007229351787128197, 'feature_fraction': 0.9361412185107361, 'bagging_fraction': 0.8127028948655933, 'bagging_freq': 6}. Best is trial 27 with value: 102243.7951826879.\n",
      "[I 2025-07-24 15:44:03,037] Trial 28 finished with value: 102463.38935080143 and parameters: {'learning_rate': 0.05342361815111402, 'n_estimators': 2735, 'num_leaves': 89, 'max_depth': 10, 'lambda_l1': 1.0248790902326725, 'lambda_l2': 0.006704700327800949, 'feature_fraction': 0.9323667625668997, 'bagging_fraction': 0.8056970533938654, 'bagging_freq': 7}. Best is trial 27 with value: 102243.7951826879.\n",
      "[I 2025-07-24 15:44:32,061] Trial 29 finished with value: 102925.72525378865 and parameters: {'learning_rate': 0.03917373710098493, 'n_estimators': 2705, 'num_leaves': 81, 'max_depth': 10, 'lambda_l1': 1.0853435287497484, 'lambda_l2': 0.0012104312493456497, 'feature_fraction': 0.9235451046136911, 'bagging_fraction': 0.8074833184833703, 'bagging_freq': 7}. Best is trial 27 with value: 102243.7951826879.\n",
      "[I 2025-07-24 15:45:03,591] Trial 30 finished with value: 103951.04715049444 and parameters: {'learning_rate': 0.027277030405681878, 'n_estimators': 2833, 'num_leaves': 71, 'max_depth': 10, 'lambda_l1': 3.861201756826466, 'lambda_l2': 0.010414024890227302, 'feature_fraction': 0.8873419397762428, 'bagging_fraction': 0.7652255988781217, 'bagging_freq': 6}. Best is trial 27 with value: 102243.7951826879.\n",
      "[I 2025-07-24 15:45:35,264] Trial 31 finished with value: 102832.88977263514 and parameters: {'learning_rate': 0.05374996093390115, 'n_estimators': 2667, 'num_leaves': 89, 'max_depth': 10, 'lambda_l1': 0.9990381383717417, 'lambda_l2': 0.005873094193356197, 'feature_fraction': 0.9728202116675759, 'bagging_fraction': 0.8208643958758349, 'bagging_freq': 7}. Best is trial 27 with value: 102243.7951826879.\n",
      "[I 2025-07-24 15:46:06,333] Trial 32 finished with value: 102335.346706449 and parameters: {'learning_rate': 0.05701691325173463, 'n_estimators': 2636, 'num_leaves': 92, 'max_depth': 10, 'lambda_l1': 1.3283762390971838, 'lambda_l2': 0.0032298373262732985, 'feature_fraction': 0.9389711080143957, 'bagging_fraction': 0.78833086496326, 'bagging_freq': 6}. Best is trial 27 with value: 102243.7951826879.\n",
      "[I 2025-07-24 15:46:39,519] Trial 33 finished with value: 102122.08582492758 and parameters: {'learning_rate': 0.04410261748565086, 'n_estimators': 2851, 'num_leaves': 83, 'max_depth': 10, 'lambda_l1': 0.3107722805768278, 'lambda_l2': 0.0035889347226658774, 'feature_fraction': 0.9099337108142731, 'bagging_fraction': 0.7768012020322244, 'bagging_freq': 6}. Best is trial 33 with value: 102122.08582492758.\n",
      "[I 2025-07-24 15:47:10,565] Trial 34 finished with value: 102448.41751178412 and parameters: {'learning_rate': 0.04506541241152677, 'n_estimators': 2909, 'num_leaves': 83, 'max_depth': 10, 'lambda_l1': 0.27546348125926884, 'lambda_l2': 0.002965110436701048, 'feature_fraction': 0.875912605167698, 'bagging_fraction': 0.7728927101103527, 'bagging_freq': 6}. Best is trial 33 with value: 102122.08582492758.\n",
      "[I 2025-07-24 15:47:35,920] Trial 35 finished with value: 102469.99116509657 and parameters: {'learning_rate': 0.06392013660663833, 'n_estimators': 2596, 'num_leaves': 68, 'max_depth': 10, 'lambda_l1': 0.1280739444862985, 'lambda_l2': 0.0012101942303665712, 'feature_fraction': 0.9139996053562556, 'bagging_fraction': 0.7375688776101826, 'bagging_freq': 6}. Best is trial 33 with value: 102122.08582492758.\n",
      "[I 2025-07-24 15:48:09,520] Trial 36 finished with value: 102349.87874762437 and parameters: {'learning_rate': 0.042848134293368846, 'n_estimators': 2846, 'num_leaves': 93, 'max_depth': 10, 'lambda_l1': 4.1196003427076375, 'lambda_l2': 0.003594162237195279, 'feature_fraction': 0.8328637177992713, 'bagging_fraction': 0.7840987901737216, 'bagging_freq': 6}. Best is trial 33 with value: 102122.08582492758.\n",
      "[I 2025-07-24 15:48:36,765] Trial 37 finished with value: 102406.50816983207 and parameters: {'learning_rate': 0.05978047065333293, 'n_estimators': 2494, 'num_leaves': 76, 'max_depth': 9, 'lambda_l1': 3.831729186358286, 'lambda_l2': 0.013726055358114584, 'feature_fraction': 0.9015356681526184, 'bagging_fraction': 0.7905764802543352, 'bagging_freq': 7}. Best is trial 33 with value: 102122.08582492758.\n",
      "[I 2025-07-24 15:49:02,964] Trial 38 finished with value: 103413.2412686227 and parameters: {'learning_rate': 0.05030612734804404, 'n_estimators': 2478, 'num_leaves': 74, 'max_depth': 10, 'lambda_l1': 0.561517942932256, 'lambda_l2': 0.001647927836634954, 'feature_fraction': 0.985942804820682, 'bagging_fraction': 0.7282735234188399, 'bagging_freq': 5}. Best is trial 33 with value: 102122.08582492758.\n",
      "[I 2025-07-24 15:49:31,817] Trial 39 finished with value: 103496.29171191462 and parameters: {'learning_rate': 0.03562224885875062, 'n_estimators': 2615, 'num_leaves': 61, 'max_depth': 10, 'lambda_l1': 0.40265161896185614, 'lambda_l2': 0.0037312061054221232, 'feature_fraction': 0.9631862773269798, 'bagging_fraction': 0.8109432957918796, 'bagging_freq': 6}. Best is trial 33 with value: 102122.08582492758.\n",
      "[I 2025-07-24 15:50:08,251] Trial 40 finished with value: 106378.19039542241 and parameters: {'learning_rate': 0.011099506616186673, 'n_estimators': 2869, 'num_leaves': 83, 'max_depth': 9, 'lambda_l1': 0.08333114087040062, 'lambda_l2': 0.011275923248290218, 'feature_fraction': 0.9184420423568258, 'bagging_fraction': 0.8462304020162837, 'bagging_freq': 5}. Best is trial 33 with value: 102122.08582492758.\n",
      "[I 2025-07-24 15:50:43,338] Trial 41 finished with value: 102274.2442228962 and parameters: {'learning_rate': 0.04127331867276796, 'n_estimators': 2989, 'num_leaves': 95, 'max_depth': 10, 'lambda_l1': 3.204147073877045, 'lambda_l2': 0.003444834619673976, 'feature_fraction': 0.8149516621542529, 'bagging_fraction': 0.7828504231494661, 'bagging_freq': 6}. Best is trial 33 with value: 102122.08582492758.\n",
      "[I 2025-07-24 15:51:15,876] Trial 42 finished with value: 101844.16187906738 and parameters: {'learning_rate': 0.04957911555457465, 'n_estimators': 2983, 'num_leaves': 95, 'max_depth': 10, 'lambda_l1': 1.6748732479784827, 'lambda_l2': 0.0021981724098079276, 'feature_fraction': 0.8011306635524761, 'bagging_fraction': 0.7821871615135502, 'bagging_freq': 7}. Best is trial 42 with value: 101844.16187906738.\n",
      "[I 2025-07-24 15:51:50,217] Trial 43 finished with value: 102402.39801574335 and parameters: {'learning_rate': 0.0412086557284482, 'n_estimators': 2996, 'num_leaves': 97, 'max_depth': 10, 'lambda_l1': 2.646771883734519, 'lambda_l2': 0.002264861046613118, 'feature_fraction': 0.7939787289371809, 'bagging_fraction': 0.7762205351409663, 'bagging_freq': 7}. Best is trial 42 with value: 101844.16187906738.\n",
      "[I 2025-07-24 15:52:22,033] Trial 44 finished with value: 102003.90309418499 and parameters: {'learning_rate': 0.04949524641035684, 'n_estimators': 2881, 'num_leaves': 96, 'max_depth': 9, 'lambda_l1': 2.7112015715010687, 'lambda_l2': 0.001493172606920541, 'feature_fraction': 0.8182795986015976, 'bagging_fraction': 0.7465384474310562, 'bagging_freq': 7}. Best is trial 42 with value: 101844.16187906738.\n",
      "[I 2025-07-24 15:52:52,672] Trial 45 finished with value: 102598.97812210098 and parameters: {'learning_rate': 0.034971332622370455, 'n_estimators': 2913, 'num_leaves': 95, 'max_depth': 9, 'lambda_l1': 0.787390113273068, 'lambda_l2': 0.0010008184781126492, 'feature_fraction': 0.8115424245094158, 'bagging_fraction': 0.7467962926500098, 'bagging_freq': 7}. Best is trial 42 with value: 101844.16187906738.\n",
      "[I 2025-07-24 15:53:21,160] Trial 46 finished with value: 103101.61248809447 and parameters: {'learning_rate': 0.047576382815789006, 'n_estimators': 2986, 'num_leaves': 95, 'max_depth': 7, 'lambda_l1': 8.851635172246967, 'lambda_l2': 0.0015153283028317538, 'feature_fraction': 0.7680605330661143, 'bagging_fraction': 0.7079715240868847, 'bagging_freq': 7}. Best is trial 42 with value: 101844.16187906738.\n",
      "[I 2025-07-24 15:53:46,973] Trial 47 finished with value: 104143.4941381933 and parameters: {'learning_rate': 0.02858507622985886, 'n_estimators': 2874, 'num_leaves': 55, 'max_depth': 9, 'lambda_l1': 0.3372257110306766, 'lambda_l2': 0.0021657642864497187, 'feature_fraction': 0.7893125830206942, 'bagging_fraction': 0.765674078427615, 'bagging_freq': 7}. Best is trial 42 with value: 101844.16187906738.\n",
      "[I 2025-07-24 15:54:17,410] Trial 48 finished with value: 102897.3923900476 and parameters: {'learning_rate': 0.06416709371289502, 'n_estimators': 2816, 'num_leaves': 85, 'max_depth': 9, 'lambda_l1': 5.393377626625837, 'lambda_l2': 0.003728693388070603, 'feature_fraction': 0.8220915430014948, 'bagging_fraction': 0.7241454580954331, 'bagging_freq': 7}. Best is trial 42 with value: 101844.16187906738.\n",
      "[I 2025-07-24 15:54:33,483] Trial 49 finished with value: 104589.5636646687 and parameters: {'learning_rate': 0.03627810233369134, 'n_estimators': 1389, 'num_leaves': 91, 'max_depth': 10, 'lambda_l1': 1.8047277200146363, 'lambda_l2': 0.0019088460757066938, 'feature_fraction': 0.777794239649839, 'bagging_fraction': 0.7483888174798047, 'bagging_freq': 6}. Best is trial 42 with value: 101844.16187906738.\n",
      "[I 2025-07-24 15:55:07,677] Trial 50 finished with value: 102769.2212843546 and parameters: {'learning_rate': 0.026158078659255065, 'n_estimators': 3000, 'num_leaves': 96, 'max_depth': 9, 'lambda_l1': 2.38021782827121, 'lambda_l2': 0.004153545804001653, 'feature_fraction': 0.746683418030158, 'bagging_fraction': 0.7976487209918748, 'bagging_freq': 5}. Best is trial 42 with value: 101844.16187906738.\n",
      "[I 2025-07-24 15:55:38,936] Trial 51 finished with value: 102360.6466351253 and parameters: {'learning_rate': 0.049526732368121294, 'n_estimators': 2762, 'num_leaves': 88, 'max_depth': 10, 'lambda_l1': 3.0158922064803475, 'lambda_l2': 0.018894941729492635, 'feature_fraction': 0.8089884541417908, 'bagging_fraction': 0.8207309130510378, 'bagging_freq': 6}. Best is trial 42 with value: 101844.16187906738.\n",
      "[I 2025-07-24 15:56:06,780] Trial 52 finished with value: 102721.87461052126 and parameters: {'learning_rate': 0.04586257340589785, 'n_estimators': 2540, 'num_leaves': 83, 'max_depth': 10, 'lambda_l1': 5.516454786203825, 'lambda_l2': 0.008096996920457225, 'feature_fraction': 0.7401413087985287, 'bagging_fraction': 0.7629204903883222, 'bagging_freq': 6}. Best is trial 42 with value: 101844.16187906738.\n",
      "[I 2025-07-24 15:56:40,872] Trial 53 finished with value: 101793.26781449067 and parameters: {'learning_rate': 0.04319018800313558, 'n_estimators': 2922, 'num_leaves': 96, 'max_depth': 10, 'lambda_l1': 0.7450803654698834, 'lambda_l2': 0.0025965835924400744, 'feature_fraction': 0.8397666851103172, 'bagging_fraction': 0.8573095640467909, 'bagging_freq': 5}. Best is trial 53 with value: 101793.26781449067.\n",
      "[I 2025-07-24 15:56:59,877] Trial 54 finished with value: 105439.04652846883 and parameters: {'learning_rate': 0.03126018989372455, 'n_estimators': 2904, 'num_leaves': 97, 'max_depth': 5, 'lambda_l1': 0.7494687336665772, 'lambda_l2': 0.0025817482428821606, 'feature_fraction': 0.8330088084549245, 'bagging_fraction': 0.8589695037892164, 'bagging_freq': 5}. Best is trial 53 with value: 101793.26781449067.\n",
      "[I 2025-07-24 15:57:30,307] Trial 55 finished with value: 102439.74050365895 and parameters: {'learning_rate': 0.042502100171548055, 'n_estimators': 2790, 'num_leaves': 92, 'max_depth': 10, 'lambda_l1': 0.14855507662175546, 'lambda_l2': 0.004722654978469721, 'feature_fraction': 0.849485764391353, 'bagging_fraction': 0.8288109203265864, 'bagging_freq': 5}. Best is trial 53 with value: 101793.26781449067.\n",
      "[I 2025-07-24 15:58:01,495] Trial 56 finished with value: 102421.1327123738 and parameters: {'learning_rate': 0.03334815903035633, 'n_estimators': 2894, 'num_leaves': 95, 'max_depth': 9, 'lambda_l1': 0.2429743090483254, 'lambda_l2': 0.0015352479516147711, 'feature_fraction': 0.8265984955201248, 'bagging_fraction': 0.7796180830711553, 'bagging_freq': 7}. Best is trial 53 with value: 101793.26781449067.\n",
      "[I 2025-07-24 15:58:24,193] Trial 57 finished with value: 104260.81824356825 and parameters: {'learning_rate': 0.038362159580339596, 'n_estimators': 2950, 'num_leaves': 37, 'max_depth': 10, 'lambda_l1': 0.48657640764161625, 'lambda_l2': 0.002559106197024196, 'feature_fraction': 0.867055758964441, 'bagging_fraction': 0.7945435829888632, 'bagging_freq': 6}. Best is trial 53 with value: 101793.26781449067.\n",
      "[I 2025-07-24 15:58:55,061] Trial 58 finished with value: 101609.72953692608 and parameters: {'learning_rate': 0.06641957832283381, 'n_estimators': 2777, 'num_leaves': 98, 'max_depth': 10, 'lambda_l1': 1.3088673513828804, 'lambda_l2': 0.0014231169064005797, 'feature_fraction': 0.781741153364829, 'bagging_fraction': 0.8580908617679706, 'bagging_freq': 5}. Best is trial 58 with value: 101609.72953692608.\n",
      "[I 2025-07-24 15:59:08,635] Trial 59 finished with value: 103851.01895499464 and parameters: {'learning_rate': 0.08257802500093218, 'n_estimators': 1816, 'num_leaves': 100, 'max_depth': 6, 'lambda_l1': 1.2377872971041657, 'lambda_l2': 0.0012596779774358936, 'feature_fraction': 0.7839823972029769, 'bagging_fraction': 0.8680882912808146, 'bagging_freq': 5}. Best is trial 58 with value: 101609.72953692608.\n",
      "[I 2025-07-24 15:59:35,216] Trial 60 finished with value: 102316.69824657691 and parameters: {'learning_rate': 0.06516671670927156, 'n_estimators': 2769, 'num_leaves': 86, 'max_depth': 9, 'lambda_l1': 0.707995801027056, 'lambda_l2': 0.0010372332742278297, 'feature_fraction': 0.718660391861143, 'bagging_fraction': 0.8545398827440038, 'bagging_freq': 1}. Best is trial 58 with value: 101609.72953692608.\n",
      "[I 2025-07-24 16:00:06,115] Trial 61 finished with value: 102550.84988574502 and parameters: {'learning_rate': 0.06888098210855616, 'n_estimators': 2840, 'num_leaves': 98, 'max_depth': 10, 'lambda_l1': 1.550654115486921, 'lambda_l2': 0.001926285055261954, 'feature_fraction': 0.8030488320982985, 'bagging_fraction': 0.7516218213202089, 'bagging_freq': 5}. Best is trial 58 with value: 101609.72953692608.\n",
      "[I 2025-07-24 16:00:39,294] Trial 62 finished with value: 102042.23574669317 and parameters: {'learning_rate': 0.05661953617476438, 'n_estimators': 2925, 'num_leaves': 92, 'max_depth': 10, 'lambda_l1': 0.8361248045707358, 'lambda_l2': 9.158866135805333, 'feature_fraction': 0.844190961818129, 'bagging_fraction': 0.8334462556290976, 'bagging_freq': 6}. Best is trial 58 with value: 101609.72953692608.\n",
      "[I 2025-07-24 16:01:07,333] Trial 63 finished with value: 102264.25376657513 and parameters: {'learning_rate': 0.05033401645520336, 'n_estimators': 2735, 'num_leaves': 91, 'max_depth': 10, 'lambda_l1': 0.3480728538660883, 'lambda_l2': 1.3066456857268764, 'feature_fraction': 0.8411515404776397, 'bagging_fraction': 0.8426541328397721, 'bagging_freq': 7}. Best is trial 58 with value: 101609.72953692608.\n",
      "[I 2025-07-24 16:01:35,047] Trial 64 finished with value: 102150.81341695663 and parameters: {'learning_rate': 0.05638352949326279, 'n_estimators': 2663, 'num_leaves': 88, 'max_depth': 10, 'lambda_l1': 0.8370036723519195, 'lambda_l2': 2.10129270210937, 'feature_fraction': 0.8793609438128496, 'bagging_fraction': 0.8331262540394029, 'bagging_freq': 6}. Best is trial 58 with value: 101609.72953692608.\n",
      "[I 2025-07-24 16:02:04,970] Trial 65 finished with value: 102370.18077970174 and parameters: {'learning_rate': 0.05750788354590402, 'n_estimators': 2932, 'num_leaves': 80, 'max_depth': 10, 'lambda_l1': 0.8540795654697264, 'lambda_l2': 7.901301999590429, 'feature_fraction': 0.8787457077255628, 'bagging_fraction': 0.9091266751522705, 'bagging_freq': 6}. Best is trial 58 with value: 101609.72953692608.\n",
      "[I 2025-07-24 16:02:30,432] Trial 66 finished with value: 103317.17730414867 and parameters: {'learning_rate': 0.08857886685298291, 'n_estimators': 2541, 'num_leaves': 98, 'max_depth': 9, 'lambda_l1': 0.19809710537293987, 'lambda_l2': 4.557382474489339, 'feature_fraction': 0.853803775275239, 'bagging_fraction': 0.8361186160269476, 'bagging_freq': 5}. Best is trial 58 with value: 101609.72953692608.\n",
      "[I 2025-07-24 16:03:03,859] Trial 67 finished with value: 104792.67131042948 and parameters: {'learning_rate': 0.016050114474319376, 'n_estimators': 2807, 'num_leaves': 87, 'max_depth': 10, 'lambda_l1': 0.6239480452613788, 'lambda_l2': 4.920160148392252, 'feature_fraction': 0.8591011554529434, 'bagging_fraction': 0.8549604157066741, 'bagging_freq': 7}. Best is trial 58 with value: 101609.72953692608.\n",
      "[I 2025-07-24 16:03:33,478] Trial 68 finished with value: 102284.56684361391 and parameters: {'learning_rate': 0.04360249572404962, 'n_estimators': 2681, 'num_leaves': 93, 'max_depth': 10, 'lambda_l1': 1.897135684323033, 'lambda_l2': 2.165289924968604, 'feature_fraction': 0.8021345986795895, 'bagging_fraction': 0.8754895951232448, 'bagging_freq': 4}. Best is trial 58 with value: 101609.72953692608.\n",
      "[I 2025-07-24 16:04:00,475] Trial 69 finished with value: 102367.80886003909 and parameters: {'learning_rate': 0.06720330417423503, 'n_estimators': 2930, 'num_leaves': 76, 'max_depth': 9, 'lambda_l1': 0.062114934538627035, 'lambda_l2': 0.34353686471695183, 'feature_fraction': 0.8913817228734843, 'bagging_fraction': 0.8832425765813104, 'bagging_freq': 6}. Best is trial 58 with value: 101609.72953692608.\n",
      "[I 2025-07-24 16:04:28,322] Trial 70 finished with value: 102299.36825798308 and parameters: {'learning_rate': 0.06133383644542957, 'n_estimators': 2636, 'num_leaves': 100, 'max_depth': 10, 'lambda_l1': 0.48399916482954647, 'lambda_l2': 0.06547775951641113, 'feature_fraction': 0.8388799710732864, 'bagging_fraction': 0.8284738174364431, 'bagging_freq': 7}. Best is trial 58 with value: 101609.72953692608.\n",
      "[I 2025-07-24 16:04:53,729] Trial 71 finished with value: 102447.32237634828 and parameters: {'learning_rate': 0.055510647697342444, 'n_estimators': 2284, 'num_leaves': 89, 'max_depth': 10, 'lambda_l1': 1.258000638722852, 'lambda_l2': 0.631286339121454, 'feature_fraction': 0.9035801364515993, 'bagging_fraction': 0.8188135175021979, 'bagging_freq': 6}. Best is trial 58 with value: 101609.72953692608.\n",
      "[I 2025-07-24 16:05:18,850] Trial 72 finished with value: 102532.42439662178 and parameters: {'learning_rate': 0.04919443176627491, 'n_estimators': 2444, 'num_leaves': 91, 'max_depth': 10, 'lambda_l1': 0.9310088367122419, 'lambda_l2': 4.636603710262713, 'feature_fraction': 0.7769192113415251, 'bagging_fraction': 0.803555566949076, 'bagging_freq': 5}. Best is trial 58 with value: 101609.72953692608.\n",
      "[I 2025-07-24 16:05:47,598] Trial 73 finished with value: 102754.68548688715 and parameters: {'learning_rate': 0.07405101742099617, 'n_estimators': 2847, 'num_leaves': 85, 'max_depth': 10, 'lambda_l1': 0.30238965450755934, 'lambda_l2': 2.9045589128149776, 'feature_fraction': 0.8827418112332213, 'bagging_fraction': 0.8485786969794514, 'bagging_freq': 6}. Best is trial 58 with value: 101609.72953692608.\n",
      "[I 2025-07-24 16:06:16,587] Trial 74 finished with value: 101943.76314754503 and parameters: {'learning_rate': 0.05925102381428323, 'n_estimators': 2756, 'num_leaves': 93, 'max_depth': 10, 'lambda_l1': 1.4706744427129879, 'lambda_l2': 0.004830418162114271, 'feature_fraction': 0.8710664976997397, 'bagging_fraction': 0.8654075219067454, 'bagging_freq': 6}. Best is trial 58 with value: 101609.72953692608.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- LightGBM Tuning Complete ---\n",
      "Best trial validation RMSE: $101,609.73\n",
      "Best hyperparameters found for LightGBM:\n",
      "  'learning_rate': 0.06641957832283381,\n",
      "  'n_estimators': 2777,\n",
      "  'num_leaves': 98,\n",
      "  'max_depth': 10,\n",
      "  'lambda_l1': 1.3088673513828804,\n",
      "  'lambda_l2': 0.0014231169064005797,\n",
      "  'feature_fraction': 0.781741153364829,\n",
      "  'bagging_fraction': 0.8580908617679706,\n",
      "  'bagging_freq': 5,\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# BLOCK 3.5: TUNE LIGHTGBM MEAN MODEL\n",
    "# =============================================================================\n",
    "import lightgbm as lgb\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import optuna\n",
    "\n",
    "# --- 1. Prepare Data for Tuning ---\n",
    "# We use the same train/validation split from the full dataset\n",
    "X_train_opt, X_val_opt, y_train_opt, y_val_opt = train_test_split(\n",
    "    X, y_true, test_size=0.2, random_state=RANDOM_STATE\n",
    ")\n",
    "\n",
    "# Convert data to LightGBM's Dataset format for efficiency\n",
    "lgb_train_opt = lgb.Dataset(X_train_opt, y_train_opt)\n",
    "lgb_val_opt = lgb.Dataset(X_val_opt, y_val_opt, reference=lgb_train_opt)\n",
    "\n",
    "# --- 2. Define the Optuna Objective Function for LightGBM ---\n",
    "def objective_lightgbm(trial):\n",
    "    \"\"\"\n",
    "    This function takes an Optuna 'trial' object and does the following:\n",
    "    1. Defines a search space for LightGBM's hyperparameters.\n",
    "    2. Trains a LightGBM model with a set of hyperparameters suggested by the trial.\n",
    "    3. Evaluates the model on the validation set.\n",
    "    4. Returns the validation score (RMSE), which Optuna tries to minimize.\n",
    "    \"\"\"\n",
    "    # Define the hyperparameter search space\n",
    "    params = {\n",
    "        'objective': 'regression_l1',\n",
    "        'metric': 'rmse',\n",
    "        'random_state': RANDOM_STATE,\n",
    "        'n_jobs': -1,\n",
    "        'boosting_type': 'gbdt',\n",
    "        \n",
    "        # --- THE FIX IS HERE ---\n",
    "        'verbosity': -1,  # Add this line to disable all warnings\n",
    "        # ---------------------\n",
    "        \n",
    "        'learning_rate': trial.suggest_float('learning_rate', 0.01, 0.1, log=True),\n",
    "        'n_estimators': trial.suggest_int('n_estimators', 1000, 3000),\n",
    "        'num_leaves': trial.suggest_int('num_leaves', 20, 100),\n",
    "        'max_depth': trial.suggest_int('max_depth', 5, 10),\n",
    "        'lambda_l1': trial.suggest_float('lambda_l1', 1e-3, 10.0, log=True),\n",
    "        'lambda_l2': trial.suggest_float('lambda_l2', 1e-3, 10.0, log=True),\n",
    "        'feature_fraction': trial.suggest_float('feature_fraction', 0.7, 1.0),\n",
    "        'bagging_fraction': trial.suggest_float('bagging_fraction', 0.7, 1.0),\n",
    "        'bagging_freq': trial.suggest_int('bagging_freq', 1, 7)\n",
    "    }\n",
    "\n",
    "    # The rest of the function remains the same...\n",
    "    model = lgb.train(\n",
    "        params=params,\n",
    "        train_set=lgb_train_opt,\n",
    "        valid_sets=[lgb_val_opt],\n",
    "        callbacks=[lgb.early_stopping(100, verbose=False)]\n",
    "    )\n",
    "    preds = model.predict(X_val_opt, num_iteration=model.best_iteration)\n",
    "    rmse = np.sqrt(mean_squared_error(y_val_opt, preds))\n",
    "    return rmse\n",
    "\n",
    "# --- 3. Create and Run the Optuna Study ---\n",
    "# Create a study object and specify the direction is 'minimize' (for RMSE)\n",
    "study_lgbm = optuna.create_study(direction='minimize')\n",
    "\n",
    "# Start the optimization process. \n",
    "# n_trials can be increased for a more thorough search, but this is a good start.\n",
    "print(\"--- Starting LightGBM Hyperparameter Tuning... ---\")\n",
    "study_lgbm.optimize(objective_lightgbm, n_trials=N_OPTUNA_TRIALS)\n",
    "\n",
    "# --- 4. Print the Best Results ---\n",
    "print(\"\\n--- LightGBM Tuning Complete ---\")\n",
    "print(f\"Best trial validation RMSE: ${study_lgbm.best_value:,.2f}\")\n",
    "print(\"Best hyperparameters found for LightGBM:\")\n",
    "\n",
    "best_params_lgbm = study_lgbm.best_params\n",
    "for key, value in best_params_lgbm.items():\n",
    "    print(f\"  '{key}': {value},\")\n",
    "\n",
    "# Store the best parameters in a dictionary for later use in the K-Fold training loop\n",
    "# (The n_estimators is already included from the tuning search)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9e624cc7-5331-4b33-9805-cdcdedcb15de",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "--- Step 4.5: K-Fold Cross-Validation with Optimal LightGBM Hyperparameters ---\n",
      "================================================================================\n",
      "\n",
      "--- Training Fold 1/5 ---\n",
      "  Fold 1 Validation RMSE: $98,328.04\n",
      "\n",
      "--- Training Fold 2/5 ---\n",
      "  Fold 2 Validation RMSE: $97,550.84\n",
      "\n",
      "--- Training Fold 3/5 ---\n",
      "  Fold 3 Validation RMSE: $98,208.78\n",
      "\n",
      "--- Training Fold 4/5 ---\n",
      "  Fold 4 Validation RMSE: $97,811.70\n",
      "\n",
      "--- Training Fold 5/5 ---\n",
      "  Fold 5 Validation RMSE: $97,511.02\n",
      "\n",
      "================================================================================\n",
      "--- Step 5.5: Final Evaluation and Saving Predictions ---\n",
      "================================================================================\n",
      "Final LightGBM OOF RMSE across all 5 folds: $97,882.64\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# BLOCK 4.5: K-FOLD TRAINING & SAVING WITH OPTIMAL LIGHTGBM PARAMETERS\n",
    "# =============================================================================\n",
    "import lightgbm as lgb\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "import gc\n",
    "import os\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"--- Step 4.5: K-Fold Cross-Validation with Optimal LightGBM Hyperparameters ---\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "\n",
    "# Initialize arrays to store the predictions\n",
    "oof_lgbm_preds = np.zeros(len(X))\n",
    "test_lgbm_preds = np.zeros(len(X_test))\n",
    "\n",
    "# Use StratifiedKFold for the final training, as per the winning strategy\n",
    "skf = StratifiedKFold(n_splits=N_SPLITS, shuffle=True, random_state=RANDOM_STATE)\n",
    "\n",
    "# This assumes 'best_params_lgbm' exists from the Optuna tuning block\n",
    "# Let's ensure 'n_estimators' is an integer, as Optuna might return a float\n",
    "best_params_lgbm['n_estimators'] = int(best_params_lgbm.get('n_estimators', 2000))\n",
    "# Add other fixed parameters for scikit-learn wrapper\n",
    "best_params_lgbm['random_state'] = RANDOM_STATE\n",
    "best_params_lgbm['n_jobs'] = -1\n",
    "\n",
    "for fold, (train_idx, val_idx) in enumerate(skf.split(X, grade_for_stratify)):\n",
    "    print(f\"\\n--- Training Fold {fold+1}/{N_SPLITS} ---\")\n",
    "    X_train, X_val = X.iloc[train_idx], X.iloc[val_idx]\n",
    "    y_train_fold, y_val_fold = y_true.iloc[train_idx], y_true.iloc[val_idx]\n",
    "\n",
    "    # Initialize and train the model for this fold using the best parameters\n",
    "    model = lgb.LGBMRegressor(**best_params_lgbm)\n",
    "    \n",
    "    # LGBM's scikit-learn wrapper handles early stopping within .fit()\n",
    "    model.fit(X_train, y_train_fold,\n",
    "              eval_set=[(X_val, y_val_fold)],\n",
    "              callbacks=[lgb.early_stopping(100, verbose=False)])\n",
    "    \n",
    "\n",
    "    \n",
    "    # Generate predictions for the validation set (this fold's OOF part)\n",
    "    # model.best_iteration_ is available after early stopping\n",
    "    oof_preds_fold = model.predict(X_val, num_iteration=model.best_iteration_)\n",
    "    oof_lgbm_preds[val_idx] = np.clip(oof_preds_fold, 0, None)\n",
    "    \n",
    "    # Generate predictions for the test set and accumulate them\n",
    "    test_lgbm_preds += np.clip(model.predict(X_test, num_iteration=model.best_iteration_), 0, None) / N_SPLITS\n",
    "    \n",
    "    fold_rmse = np.sqrt(mean_squared_error(y_val_fold, oof_preds_fold))\n",
    "    print(f\"  Fold {fold+1} Validation RMSE: ${fold_rmse:,.2f}\")\n",
    "    del model, X_train, X_val, y_train_fold, y_val_fold\n",
    "    gc.collect()\n",
    "\n",
    "# --- Final Evaluation and Saving ---\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"--- Step 5.5: Final Evaluation and Saving Predictions ---\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "final_oof_rmse = np.sqrt(mean_squared_error(y_true, oof_lgbm_preds))\n",
    "print(f\"Final LightGBM OOF RMSE across all {N_SPLITS} folds: ${final_oof_rmse:,.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "aa8674cf-dd73-45ca-9352-caa78a47f6c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "'oof_lgbm_preds.npy' and 'test_lgbm_preds.npy' saved successfully.\n"
     ]
    }
   ],
   "source": [
    "# Define the save path and save the final prediction arrays\n",
    "SAVE_PATH = './mean_models_v1/' # Saving to the same place as other mean models\n",
    "os.makedirs(SAVE_PATH, exist_ok=True)\n",
    "np.save(os.path.join(SAVE_PATH, 'oof_lgbm_preds.npy'), oof_lgbm_preds)\n",
    "np.save(os.path.join(SAVE_PATH, 'test_lgbm_preds.npy'), test_lgbm_preds)\n",
    "print(\"\\n'oof_lgbm_preds.npy' and 'test_lgbm_preds.npy' saved successfully.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b4bb535-6936-4c35-9082-50bd3da1c5dd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (Kaggle Comp)",
   "language": "python",
   "name": "kaggle-comp"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

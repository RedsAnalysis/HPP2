{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "638ecf42-9062-4d63-b745-4404dde5f540",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Libraries imported successfully.\n",
      "Raw data loaded successfully.\n",
      "Setup complete.\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# BLOCK 1: SETUP, IMPORTS, AND DATA LOADING\n",
    "# =============================================================================\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "import time\n",
    "import os\n",
    "# --- Library Imports ---\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import gc\n",
    "from sklearn.model_selection import StratifiedKFold, train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import xgboost as xgb\n",
    "import catboost as cb\n",
    "import optuna\n",
    "from scipy.optimize import minimize\n",
    "print(\"Libraries imported successfully.\")\n",
    "# --- Helper Function for Winkler Score ---\n",
    "def winkler_score(y_true, lower, upper, alpha=0.1, return_coverage=False):\n",
    "    width = upper - lower\n",
    "    penalty_lower = np.where(y_true < lower, (2 / alpha) * (lower - y_true), 0)\n",
    "    penalty_upper = np.where(y_true > upper, (2 / alpha) * (y_true - upper), 0)\n",
    "    score = width + penalty_lower + penalty_upper\n",
    "    if return_coverage:\n",
    "        coverage = np.mean((y_true >= lower) & (y_true <= upper))\n",
    "        return np.mean(score), coverage\n",
    "    return np.mean(score)\n",
    "# --- Global Constants ---\n",
    "N_SPLITS = 5\n",
    "RANDOM_STATE = 42\n",
    "DATA_PATH = './'\n",
    "N_OPTUNA_TRIALS = 30 # A strong number for a comprehensive search\n",
    "COMPETITION_ALPHA = 0.1\n",
    "\n",
    "# --- Load Raw Data ---\n",
    "try:\n",
    "    # We drop the low-variance columns they identified right away\n",
    "    drop_cols=['id', 'golf', 'view_rainier', 'view_skyline', 'view_lakesamm','view_otherwater', 'view_other']\n",
    "    df_train = pd.read_csv(DATA_PATH + 'dataset.csv').drop(columns=drop_cols)\n",
    "    df_test = pd.read_csv(DATA_PATH + 'test.csv').drop(columns=drop_cols)\n",
    "    print(\"Raw data loaded successfully.\")\n",
    "except FileNotFoundError:\n",
    "    print(\"ERROR: Could not find 'dataset.csv' or 'test.csv'.\")\n",
    "    exit()\n",
    "# --- Prepare Target Variable ---\n",
    "y_true = df_train['sale_price'].copy()\n",
    "grade_for_stratify = df_train['grade'].copy()\n",
    "# The mean-error model works best when predicting the raw price directly\n",
    "# So, we will NOT log-transform the target this time.\n",
    "# df_train.drop('sale_price', axis=1, inplace=True) # We keep sale_price for FE\n",
    "print(\"Setup complete.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6e238bea-a0b6-47c2-b9ee-4173028ff907",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Starting Block 2.5: Executing Feature Engineering Pipeline ---\n",
      "--- Starting Comprehensive Feature Engineering ---\n",
      "Step 1: Creating brute-force numerical interaction features...\n",
      "Step 2: Creating date features...\n",
      "Step 3: Creating TF-IDF features for text columns...\n",
      "Step 4: Creating group-by aggregation features...\n",
      "Step 5: Creating ratio features...\n",
      "Step 6: Creating geospatial clustering features...\n",
      "Step 7: Finalizing feature set...\n",
      "\n",
      "Comprehensive FE complete. Total features: 233\n",
      "Feature engineering complete. X shape: (200000, 233), X_test shape: (200000, 233)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Make sure to have these libraries installed\n",
    "# pip install pandas numpy scikit-learn\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.cluster import KMeans\n",
    "import gc\n",
    "\n",
    "# Define a random state for reproducibility\n",
    "RANDOM_STATE = 42\n",
    "\n",
    "def create_comprehensive_features(df_train, df_test):\n",
    "    \"\"\"\n",
    "    Combines original and new advanced feature engineering steps into a single pipeline.\n",
    "    \"\"\"\n",
    "    print(\"--- Starting Comprehensive Feature Engineering ---\")\n",
    "\n",
    "    # Store original indices and target variable\n",
    "    train_ids = df_train.index\n",
    "    test_ids = df_test.index\n",
    "    y_train = df_train['sale_price'].copy() # Keep the target separate\n",
    "\n",
    "    # Combine for consistent processing\n",
    "    df_train_temp = df_train.drop(columns=['sale_price'])\n",
    "    all_data = pd.concat([df_train_temp, df_test], axis=0, ignore_index=True)\n",
    "\n",
    "    # --- Original Feature Engineering ---\n",
    "\n",
    "    # A) Brute-Force Numerical Interactions\n",
    "    print(\"Step 1: Creating brute-force numerical interaction features...\")\n",
    "    NUMS = ['area', 'land_val', 'imp_val', 'sqft_lot', 'sqft', 'sqft_1', 'grade', 'year_built']\n",
    "    # Ensure all columns exist and are numeric, fill missing with 0 for safety\n",
    "    for col in NUMS:\n",
    "        if col not in all_data.columns:\n",
    "            all_data[col] = 0\n",
    "        else:\n",
    "            all_data[col] = pd.to_numeric(all_data[col], errors='coerce').fillna(0)\n",
    "            \n",
    "    for i in range(len(NUMS)):\n",
    "        for j in range(i + 1, len(NUMS)):\n",
    "            all_data[f'{NUMS[i]}_x_{NUMS[j]}'] = all_data[NUMS[i]] * all_data[NUMS[j]]\n",
    "\n",
    "    # B) Date Features\n",
    "    print(\"Step 2: Creating date features...\")\n",
    "    all_data['sale_date'] = pd.to_datetime(all_data['sale_date'])\n",
    "    all_data['sale_year'] = all_data['sale_date'].dt.year\n",
    "    all_data['sale_month'] = all_data['sale_date'].dt.month\n",
    "    all_data['sale_dayofyear'] = all_data['sale_date'].dt.dayofyear\n",
    "    all_data['age_at_sale'] = all_data['sale_year'] - all_data['year_built']\n",
    "\n",
    "    # C) TF-IDF Text Features\n",
    "    print(\"Step 3: Creating TF-IDF features for text columns...\")\n",
    "    text_cols = ['subdivision', 'zoning', 'city', 'sale_warning', 'join_status', 'submarket']\n",
    "    all_data[text_cols] = all_data[text_cols].fillna('missing').astype(str)\n",
    "    \n",
    "    for col in text_cols:\n",
    "        tfidf = TfidfVectorizer(analyzer='char', ngram_range=(3, 5), max_features=128, binary=True)\n",
    "        svd = TruncatedSVD(n_components=8, random_state=RANDOM_STATE)\n",
    "        \n",
    "        tfidf_matrix = tfidf.fit_transform(all_data[col])\n",
    "        tfidf_svd = svd.fit_transform(tfidf_matrix)\n",
    "        \n",
    "        tfidf_df = pd.DataFrame(tfidf_svd, columns=[f'{col}_tfidf_svd_{i}' for i in range(8)])\n",
    "        all_data = pd.concat([all_data, tfidf_df], axis=1)\n",
    "\n",
    "    # D) Log transform some interaction features\n",
    "    for c in ['land_val_x_imp_val', 'land_val_x_sqft', 'imp_val_x_sqft']:\n",
    "        if c in all_data.columns:\n",
    "            all_data[c] = np.log1p(all_data[c].fillna(0))\n",
    "\n",
    "    # --- New Feature Engineering Ideas ---\n",
    "\n",
    "    # F) Group-By Aggregation Features\n",
    "    print(\"Step 4: Creating group-by aggregation features...\")\n",
    "    group_cols = ['submarket', 'city', 'zoning']\n",
    "    num_cols_for_agg = ['grade', 'sqft', 'imp_val', 'land_val', 'age_at_sale']\n",
    "\n",
    "    for group_col in group_cols:\n",
    "        for num_col in num_cols_for_agg:\n",
    "            agg_stats = all_data.groupby(group_col)[num_col].agg(['mean', 'std', 'max', 'min']).reset_index()\n",
    "            agg_stats.columns = [group_col] + [f'{group_col}_{num_col}_{stat}' for stat in ['mean', 'std', 'max', 'min']]\n",
    "            all_data = pd.merge(all_data, agg_stats, on=group_col, how='left')\n",
    "            all_data[f'{num_col}_minus_{group_col}_mean'] = all_data[num_col] - all_data[f'{group_col}_{num_col}_mean']\n",
    "\n",
    "    # G) Ratio Features\n",
    "    print(\"Step 5: Creating ratio features...\")\n",
    "    # Add a small epsilon to prevent division by zero\n",
    "    epsilon = 1e-6 \n",
    "    all_data['total_val'] = all_data['imp_val'] + all_data['land_val']\n",
    "    all_data['imp_val_to_land_val_ratio'] = all_data['imp_val'] / (all_data['land_val'] + epsilon)\n",
    "    all_data['land_val_ratio'] = all_data['land_val'] / (all_data['total_val'] + epsilon)\n",
    "    all_data['sqft_to_lot_ratio'] = all_data['sqft'] / (all_data['sqft_lot'] + epsilon)\n",
    "    all_data['was_renovated'] = (all_data['year_reno'] > 0).astype(int)\n",
    "    all_data['reno_age_at_sale'] = np.where(all_data['was_renovated'] == 1, all_data['sale_year'] - all_data['year_reno'], -1)\n",
    "\n",
    "    # H) Geospatial Clustering Features\n",
    "    print(\"Step 6: Creating geospatial clustering features...\")\n",
    "    coords = all_data[['latitude', 'longitude']].copy()\n",
    "    coords.fillna(coords.median(), inplace=True) # Simple imputation\n",
    "\n",
    "    # KMeans is sensitive to feature scaling, but for lat/lon it's often okay without it.\n",
    "    kmeans = KMeans(n_clusters=20, random_state=RANDOM_STATE, n_init=10) \n",
    "    all_data['location_cluster'] = kmeans.fit_predict(coords)\n",
    "    \n",
    "    # Calculate distance to each cluster center\n",
    "    cluster_centers = kmeans.cluster_centers_\n",
    "    for i in range(len(cluster_centers)):\n",
    "        center = cluster_centers[i]\n",
    "        all_data[f'dist_to_cluster_{i}'] = np.sqrt((coords['latitude'] - center[0])**2 + (coords['longitude'] - center[1])**2)\n",
    "\n",
    "    # --- Final Cleanup ---\n",
    "    print(\"Step 7: Finalizing feature set...\")\n",
    "    cols_to_drop = ['sale_date', 'subdivision', 'zoning', 'city', 'sale_warning', 'join_status', 'submarket']\n",
    "    all_data = all_data.drop(columns=cols_to_drop)\n",
    "\n",
    "    # One-hot encode the new cluster feature\n",
    "    all_data = pd.get_dummies(all_data, columns=['location_cluster'], prefix='loc_cluster')\n",
    "    \n",
    "    # Final check for any remaining object columns to be safe (besides index)\n",
    "    object_cols = all_data.select_dtypes(include='object').columns\n",
    "    if len(object_cols) > 0:\n",
    "        print(f\"Warning: Found unexpected object columns: {object_cols}. Dropping them.\")\n",
    "        all_data = all_data.drop(columns=object_cols)\n",
    "        \n",
    "    all_data.fillna(0, inplace=True)\n",
    "\n",
    "    # Separate back into train and test sets\n",
    "    train_len = len(train_ids)\n",
    "    X = all_data.iloc[:train_len].copy()\n",
    "    X_test = all_data.iloc[train_len:].copy()\n",
    "    \n",
    "    # Restore original indices\n",
    "    X.index = train_ids\n",
    "    X_test.index = test_ids\n",
    "    \n",
    "    # Align columns - crucial for model prediction\n",
    "    X_test = X_test[X.columns]\n",
    "    \n",
    "    print(f\"\\nComprehensive FE complete. Total features: {X.shape[1]}\")\n",
    "    gc.collect()\n",
    "    \n",
    "    return X, X_test, y_train\n",
    "# =============================================================================\n",
    "# BLOCK 2.5: EXECUTE FEATURE ENGINEERING\n",
    "# =============================================================================\n",
    "print(\"\\n--- Starting Block 2.5: Executing Feature Engineering Pipeline ---\")\n",
    "\n",
    "# This is the crucial step that was missing.\n",
    "# We call the function to create our training and testing dataframes.\n",
    "X, X_test, y_train = create_comprehensive_features(df_train, df_test)\n",
    "\n",
    "# Let's verify the output\n",
    "print(f\"Feature engineering complete. X shape: {X.shape}, X_test shape: {X_test.shape}\")\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bca9c850-0f15-4a05-b4ad-63c44f7874a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Loading all base model predictions from saved .npy files... ---\n",
      "All MEAN model predictions loaded successfully.\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# BLOCK 3: LOAD ALL PRE-TRAINED MODEL PREDICTIONS\n",
    "# =============================================================================\n",
    "\n",
    "# Define paths to your saved prediction files\n",
    "PREDS_SAVE_PATH = './mean_models_v1/' # For XGB and CatBoost preds\n",
    "NN_PREDS_PATH = './NN_model_predictions/' # For NN preds\n",
    "\n",
    "print(\"--- Loading all base model predictions from saved .npy files... ---\")\n",
    "try:\n",
    "    # Load Mean Model OOF (Out-of-Fold) Predictions\n",
    "    oof_xgb_preds = np.load(os.path.join(PREDS_SAVE_PATH, 'oof_xgb_preds.npy'))\n",
    "    oof_cb_preds = np.load(os.path.join(PREDS_SAVE_PATH, 'oof_cb_preds.npy'))\n",
    "    oof_lgbm_preds = np.load(os.path.join(PREDS_SAVE_PATH, 'oof_lgbm_preds.npy'))\n",
    "    oof_nn_preds = np.load(os.path.join(NN_PREDS_PATH, 'oof_nn_preds.npy'))\n",
    "    \n",
    "    # Load Mean Model Test Predictions\n",
    "    test_xgb_preds = np.load(os.path.join(PREDS_SAVE_PATH, 'test_xgb_preds.npy'))\n",
    "    test_cb_preds = np.load(os.path.join(PREDS_SAVE_PATH, 'test_cb_preds.npy'))\n",
    "    test_lgbm_preds = np.load(os.path.join(PREDS_SAVE_PATH, 'test_lgbm_preds.npy'))\n",
    "    test_nn_preds = np.load(os.path.join(NN_PREDS_PATH, 'test_nn_preds.npy'))\n",
    "    \n",
    "    print(\"All MEAN model predictions loaded successfully.\")\n",
    "    \n",
    "except FileNotFoundError as e:\n",
    "    print(f\"\\nERROR: Could not find a required prediction file. {e}\")\n",
    "    print(\"Please ensure you have run all training notebooks and saved their predictions first.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "95994879-e2a8-4910-a387-ac7db62213f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Finding optimal weights for the 4-model mean ensemble ---\n",
      "\n",
      "--- STAGE 1 (MEAN) ENSEMBLE RESULTS ---\n",
      "Optimal Weights (XGB/CB/LGBM/NN): 0.2107 / 0.4508 / 0.2485 / 0.0900\n",
      "Final Ensemble Mean OOF RMSE: $94,960.11\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# BLOCK 4: BUILD AND EVALUATE THE STAGE 1 (MEAN) ENSEMBLE\n",
    "# =============================================================================\n",
    "\n",
    "print(\"\\n--- Finding optimal weights for the 4-model mean ensemble ---\")\n",
    "\n",
    "# Stack the OOF predictions for easy matrix multiplication\n",
    "oof_preds_stack_mean = np.vstack([oof_xgb_preds, oof_cb_preds, oof_lgbm_preds, oof_nn_preds]).T\n",
    "\n",
    "# Define the function for the optimizer to minimize (RMSE)\n",
    "def get_ensemble_rmse(weights):\n",
    "    final_prediction = np.dot(oof_preds_stack_mean, weights)\n",
    "    return np.sqrt(mean_squared_error(y_true, final_prediction))\n",
    "\n",
    "# Run the optimizer to find the best weights\n",
    "# Constraints ensure weights sum to 1 and are between 0 and 1\n",
    "result = minimize(get_ensemble_rmse, [1/4]*4, method='SLSQP', bounds=[(0,1)]*4, \n",
    "                  constraints=({'type': 'eq', 'fun': lambda w: 1 - np.sum(w)}))\n",
    "best_mean_weights = result.x\n",
    "\n",
    "# Create the final blended mean predictions for both OOF and test sets\n",
    "oof_ensemble_mean = np.dot(oof_preds_stack_mean, best_mean_weights)\n",
    "test_ensemble_mean = np.dot(np.vstack([test_xgb_preds, test_cb_preds, test_lgbm_preds, test_nn_preds]).T, best_mean_weights)\n",
    "\n",
    "print(\"\\n--- STAGE 1 (MEAN) ENSEMBLE RESULTS ---\")\n",
    "print(f\"Optimal Weights (XGB/CB/LGBM/NN): {best_mean_weights[0]:.4f} / {best_mean_weights[1]:.4f} / {best_mean_weights[2]:.4f} / {best_mean_weights[3]:.4f}\")\n",
    "print(f\"Final Ensemble Mean OOF RMSE: ${np.sqrt(mean_squared_error(y_true, oof_ensemble_mean)):,.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d0bd975f-034f-4583-a83b-24c1edb6b391",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Preparing features for the XGBoost error model ---\n",
      "\n",
      "--- Tuning the XGBoost Error Model with Optuna... ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-07-24 22:48:17,859] A new study created in memory with name: no-name-85cb1abc-f49b-4bb8-bd85-9e9c693d1cd6\n",
      "[I 2025-07-24 22:48:27,967] Trial 0 finished with value: 61569.60421908874 and parameters: {'eta': 0.02596274385884213, 'max_depth': 5, 'subsample': 0.7165035200223228, 'colsample_bytree': 0.7553808604507141, 'lambda': 1.3621040017388202, 'alpha': 4.855224057249528}. Best is trial 0 with value: 61569.60421908874.\n",
      "[I 2025-07-24 22:48:42,827] Trial 1 finished with value: 61176.599508171304 and parameters: {'eta': 0.04596427351611171, 'max_depth': 6, 'subsample': 0.7755357269088934, 'colsample_bytree': 0.6040542831570711, 'lambda': 25.4559921097289, 'alpha': 10.431141936315996}. Best is trial 1 with value: 61176.599508171304.\n",
      "[I 2025-07-24 22:49:19,446] Trial 2 finished with value: 61160.35264782224 and parameters: {'eta': 0.013854342741406706, 'max_depth': 6, 'subsample': 0.8806135554312897, 'colsample_bytree': 0.8585148328772866, 'lambda': 35.58153391734345, 'alpha': 9.580986510997118}. Best is trial 2 with value: 61160.35264782224.\n",
      "[I 2025-07-24 22:49:50,693] Trial 3 finished with value: 61352.656263942255 and parameters: {'eta': 0.012336249683534342, 'max_depth': 5, 'subsample': 0.7243412277583202, 'colsample_bytree': 0.7998084445021925, 'lambda': 8.292532835351484, 'alpha': 1.1614222617967747}. Best is trial 2 with value: 61160.35264782224.\n",
      "[I 2025-07-24 22:50:05,287] Trial 4 finished with value: 61514.80463848542 and parameters: {'eta': 0.023774979906791234, 'max_depth': 4, 'subsample': 0.7844917287030722, 'colsample_bytree': 0.6652252956709651, 'lambda': 5.474753907144861, 'alpha': 40.298748755264754}. Best is trial 2 with value: 61160.35264782224.\n",
      "[I 2025-07-24 22:50:21,164] Trial 5 finished with value: 61530.738418285066 and parameters: {'eta': 0.031510705682864654, 'max_depth': 4, 'subsample': 0.6383385143929818, 'colsample_bytree': 0.7180372329110577, 'lambda': 44.950540772175906, 'alpha': 6.102997040136121}. Best is trial 2 with value: 61160.35264782224.\n",
      "[I 2025-07-24 22:50:36,019] Trial 6 finished with value: 61801.99840322363 and parameters: {'eta': 0.029045859493916974, 'max_depth': 3, 'subsample': 0.6177668148590714, 'colsample_bytree': 0.6421070888831266, 'lambda': 1.9326834004287388, 'alpha': 2.3983915618246323}. Best is trial 2 with value: 61160.35264782224.\n",
      "[I 2025-07-24 22:50:44,635] Trial 7 finished with value: 61812.962117376774 and parameters: {'eta': 0.04168943188980468, 'max_depth': 3, 'subsample': 0.6539659976546749, 'colsample_bytree': 0.8862095685680572, 'lambda': 11.438514620705606, 'alpha': 22.57890471444153}. Best is trial 2 with value: 61160.35264782224.\n",
      "[I 2025-07-24 22:50:59,984] Trial 8 finished with value: 61493.92734649263 and parameters: {'eta': 0.03543155334410021, 'max_depth': 4, 'subsample': 0.8014249864993293, 'colsample_bytree': 0.8407264885258949, 'lambda': 7.559764279855662, 'alpha': 2.5501650080407328}. Best is trial 2 with value: 61160.35264782224.\n",
      "[I 2025-07-24 22:51:22,902] Trial 9 finished with value: 61786.59966058733 and parameters: {'eta': 0.014301288633683124, 'max_depth': 3, 'subsample': 0.6752283510571332, 'colsample_bytree': 0.7926309287063505, 'lambda': 24.621384457394647, 'alpha': 16.047740158151857}. Best is trial 2 with value: 61160.35264782224.\n",
      "[I 2025-07-24 22:51:42,793] Trial 10 finished with value: 61271.33267399839 and parameters: {'eta': 0.017445041665245697, 'max_depth': 6, 'subsample': 0.896978628354987, 'colsample_bytree': 0.899174247761839, 'lambda': 3.328787102765686, 'alpha': 41.24357659655457}. Best is trial 2 with value: 61160.35264782224.\n",
      "[I 2025-07-24 22:51:52,084] Trial 11 finished with value: 61339.98760508923 and parameters: {'eta': 0.04904008989948763, 'max_depth': 6, 'subsample': 0.8812036162083138, 'colsample_bytree': 0.6203760323694698, 'lambda': 23.734363334304547, 'alpha': 11.913506766479927}. Best is trial 2 with value: 61160.35264782224.\n",
      "[I 2025-07-24 22:52:24,115] Trial 12 finished with value: 61233.89331190455 and parameters: {'eta': 0.010367027820631054, 'max_depth': 6, 'subsample': 0.8368908499774862, 'colsample_bytree': 0.7025217145738032, 'lambda': 47.92738545213844, 'alpha': 9.012373743759717}. Best is trial 2 with value: 61160.35264782224.\n",
      "[I 2025-07-24 22:52:41,271] Trial 13 finished with value: 61365.5705967498 and parameters: {'eta': 0.018392371917509594, 'max_depth': 5, 'subsample': 0.8320449324403957, 'colsample_bytree': 0.8350787512308959, 'lambda': 18.73162879823963, 'alpha': 20.989011594642342}. Best is trial 2 with value: 61160.35264782224.\n",
      "[I 2025-07-24 22:52:59,029] Trial 14 finished with value: 61220.75949746446 and parameters: {'eta': 0.018374862482357715, 'max_depth': 6, 'subsample': 0.7731188015655333, 'colsample_bytree': 0.6009943380442548, 'lambda': 16.39043918366812, 'alpha': 4.126654653522478}. Best is trial 2 with value: 61160.35264782224.\n",
      "[I 2025-07-24 22:53:27,780] Trial 15 finished with value: 61247.98259990254 and parameters: {'eta': 0.013738551835621532, 'max_depth': 5, 'subsample': 0.8451294656780268, 'colsample_bytree': 0.6958726353158873, 'lambda': 34.165298982621266, 'alpha': 9.81495360460317}. Best is trial 2 with value: 61160.35264782224.\n",
      "[I 2025-07-24 22:53:47,396] Trial 16 finished with value: 61207.11745741882 and parameters: {'eta': 0.020952123216691113, 'max_depth': 6, 'subsample': 0.7422049373713479, 'colsample_bytree': 0.7383162073003713, 'lambda': 12.206580812659475, 'alpha': 14.483841639574361}. Best is trial 2 with value: 61160.35264782224.\n",
      "[I 2025-07-24 22:53:56,360] Trial 17 finished with value: 61323.94727515607 and parameters: {'eta': 0.04833126353089821, 'max_depth': 6, 'subsample': 0.8648032560198894, 'colsample_bytree': 0.8552130187032333, 'lambda': 31.135166546072828, 'alpha': 7.211334420879779}. Best is trial 2 with value: 61160.35264782224.\n",
      "[I 2025-07-24 22:54:03,351] Trial 18 finished with value: 61485.96495530718 and parameters: {'eta': 0.036313665721534674, 'max_depth': 5, 'subsample': 0.6966746451002948, 'colsample_bytree': 0.7800527097856941, 'lambda': 4.34967632052209, 'alpha': 3.3352103308109946}. Best is trial 2 with value: 61160.35264782224.\n",
      "[I 2025-07-24 22:54:32,656] Trial 19 finished with value: 61232.954541178064 and parameters: {'eta': 0.014788034793522545, 'max_depth': 6, 'subsample': 0.8115760591569443, 'colsample_bytree': 0.6667039292190544, 'lambda': 13.075746639179592, 'alpha': 23.56858098936485}. Best is trial 2 with value: 61160.35264782224.\n",
      "[I 2025-07-24 22:55:02,318] Trial 20 finished with value: 61313.4969144012 and parameters: {'eta': 0.010472512868865674, 'max_depth': 5, 'subsample': 0.7605866710163142, 'colsample_bytree': 0.8694664403304198, 'lambda': 30.49378626913603, 'alpha': 1.326564947669313}. Best is trial 2 with value: 61160.35264782224.\n",
      "[I 2025-07-24 22:55:23,302] Trial 21 finished with value: 61201.06279792774 and parameters: {'eta': 0.02013303193870269, 'max_depth': 6, 'subsample': 0.738376852239972, 'colsample_bytree': 0.7432779928104778, 'lambda': 10.551544341914806, 'alpha': 13.838270204787236}. Best is trial 2 with value: 61160.35264782224.\n",
      "[I 2025-07-24 22:55:40,194] Trial 22 finished with value: 61309.85952348358 and parameters: {'eta': 0.021874143561014695, 'max_depth': 6, 'subsample': 0.7378862350658176, 'colsample_bytree': 0.8179533462607053, 'lambda': 19.851133556200093, 'alpha': 7.90314364462185}. Best is trial 2 with value: 61160.35264782224.\n",
      "[I 2025-07-24 22:56:02,193] Trial 23 finished with value: 61252.96853527574 and parameters: {'eta': 0.01639521719516092, 'max_depth': 6, 'subsample': 0.6989999451297502, 'colsample_bytree': 0.7623211638498626, 'lambda': 9.431853917625467, 'alpha': 15.157757488060673}. Best is trial 2 with value: 61160.35264782224.\n",
      "[I 2025-07-24 22:56:13,355] Trial 24 finished with value: 61337.51583299552 and parameters: {'eta': 0.0270752021777788, 'max_depth': 6, 'subsample': 0.7970009067389255, 'colsample_bytree': 0.675213328352944, 'lambda': 37.300793031727686, 'alpha': 32.73316842934541}. Best is trial 2 with value: 61160.35264782224.\n",
      "[I 2025-07-24 22:56:42,034] Trial 25 finished with value: 61239.54140283877 and parameters: {'eta': 0.012277897699573262, 'max_depth': 5, 'subsample': 0.7619442291099763, 'colsample_bytree': 0.7225941270692204, 'lambda': 16.362541148233205, 'alpha': 11.048107807401845}. Best is trial 2 with value: 61160.35264782224.\n",
      "[I 2025-07-24 22:56:57,975] Trial 26 finished with value: 61245.41910821477 and parameters: {'eta': 0.019744534956170578, 'max_depth': 6, 'subsample': 0.8596409645730051, 'colsample_bytree': 0.6420282863106508, 'lambda': 5.739059252830638, 'alpha': 5.794701021924283}. Best is trial 2 with value: 61160.35264782224.\n",
      "[I 2025-07-24 22:57:12,231] Trial 27 finished with value: 61292.79857409479 and parameters: {'eta': 0.024121425525206987, 'max_depth': 5, 'subsample': 0.8169961115295773, 'colsample_bytree': 0.8205518382662669, 'lambda': 26.201889795910866, 'alpha': 27.501386827898596}. Best is trial 2 with value: 61160.35264782224.\n",
      "[I 2025-07-24 22:57:25,725] Trial 28 finished with value: 61320.102234835795 and parameters: {'eta': 0.016397182081456137, 'max_depth': 6, 'subsample': 0.682644183594656, 'colsample_bytree': 0.7759577163756537, 'lambda': 14.82261664965735, 'alpha': 17.69963783017625}. Best is trial 2 with value: 61160.35264782224.\n",
      "[I 2025-07-24 22:57:51,657] Trial 29 finished with value: 61532.768960302375 and parameters: {'eta': 0.012375099691121204, 'max_depth': 4, 'subsample': 0.7197629209862287, 'colsample_bytree': 0.7484103251012955, 'lambda': 2.9391397940990855, 'alpha': 5.1214125520573965}. Best is trial 2 with value: 61160.35264782224.\n",
      "[I 2025-07-24 22:58:00,335] Trial 30 finished with value: 61458.267314212 and parameters: {'eta': 0.03322085324845627, 'max_depth': 5, 'subsample': 0.7805391661774526, 'colsample_bytree': 0.8690786232924742, 'lambda': 1.1013725302522097, 'alpha': 12.301318857932465}. Best is trial 2 with value: 61160.35264782224.\n",
      "[I 2025-07-24 22:58:21,725] Trial 31 finished with value: 61290.618272875734 and parameters: {'eta': 0.0211514638909963, 'max_depth': 6, 'subsample': 0.744333684090816, 'colsample_bytree': 0.7333611992670978, 'lambda': 12.063184628458538, 'alpha': 13.7035924460165}. Best is trial 2 with value: 61160.35264782224.\n",
      "[I 2025-07-24 22:58:32,902] Trial 32 finished with value: 61346.31163833621 and parameters: {'eta': 0.02029557233232633, 'max_depth': 6, 'subsample': 0.7308925979806379, 'colsample_bytree': 0.745050216032235, 'lambda': 9.704246888380267, 'alpha': 8.8100544752418}. Best is trial 2 with value: 61160.35264782224.\n",
      "[I 2025-07-24 22:58:48,197] Trial 33 finished with value: 61307.878914966255 and parameters: {'eta': 0.025552359358821455, 'max_depth': 6, 'subsample': 0.706787588708453, 'colsample_bytree': 0.6878534316783682, 'lambda': 21.381974680423525, 'alpha': 18.87828857025482}. Best is trial 2 with value: 61160.35264782224.\n",
      "[I 2025-07-24 22:58:58,454] Trial 34 finished with value: 61266.976393008146 and parameters: {'eta': 0.02759002664978014, 'max_depth': 6, 'subsample': 0.7542270640005723, 'colsample_bytree': 0.8005220877524483, 'lambda': 6.540036621463852, 'alpha': 29.014577253149163}. Best is trial 2 with value: 61160.35264782224.\n",
      "[I 2025-07-24 22:59:16,359] Trial 35 finished with value: 61324.3667858256 and parameters: {'eta': 0.02361115582076021, 'max_depth': 5, 'subsample': 0.66207030321884, 'colsample_bytree': 0.7129231779620898, 'lambda': 39.75207330574619, 'alpha': 6.683497426828888}. Best is trial 2 with value: 61160.35264782224.\n",
      "[I 2025-07-24 22:59:26,227] Trial 36 finished with value: 61205.093073316515 and parameters: {'eta': 0.04225625435829429, 'max_depth': 6, 'subsample': 0.6008516506967732, 'colsample_bytree': 0.6485632104073574, 'lambda': 9.1675235361855, 'alpha': 48.757663696980046}. Best is trial 2 with value: 61160.35264782224.\n",
      "[I 2025-07-24 22:59:34,999] Trial 37 finished with value: 61367.79248164406 and parameters: {'eta': 0.041472488721619696, 'max_depth': 6, 'subsample': 0.6306339984276991, 'colsample_bytree': 0.6412450632771841, 'lambda': 4.4995063042695564, 'alpha': 46.128366542528404}. Best is trial 2 with value: 61160.35264782224.\n",
      "[I 2025-07-24 22:59:41,756] Trial 38 finished with value: 61802.02188388598 and parameters: {'eta': 0.04286293523692143, 'max_depth': 4, 'subsample': 0.602465124577209, 'colsample_bytree': 0.6030842050048264, 'lambda': 9.04135384857796, 'alpha': 37.28870087429392}. Best is trial 2 with value: 61160.35264782224.\n",
      "[I 2025-07-24 22:59:50,198] Trial 39 finished with value: 61544.3897302181 and parameters: {'eta': 0.0383144068217907, 'max_depth': 5, 'subsample': 0.646946666532602, 'colsample_bytree': 0.6531528863955134, 'lambda': 1.7018675537185302, 'alpha': 4.132491394611333}. Best is trial 2 with value: 61160.35264782224.\n",
      "[I 2025-07-24 23:00:02,595] Trial 40 finished with value: 61233.10732428531 and parameters: {'eta': 0.030250983470483963, 'max_depth': 6, 'subsample': 0.8778123301872891, 'colsample_bytree': 0.6189393243861714, 'lambda': 7.451724502069095, 'alpha': 2.419217768441598}. Best is trial 2 with value: 61160.35264782224.\n",
      "[I 2025-07-24 23:00:11,606] Trial 41 finished with value: 61214.89973052279 and parameters: {'eta': 0.04455219707406647, 'max_depth': 6, 'subsample': 0.7173276046009177, 'colsample_bytree': 0.6243217207602284, 'lambda': 11.697707215679396, 'alpha': 10.576863130119579}. Best is trial 2 with value: 61160.35264782224.\n",
      "[I 2025-07-24 23:00:24,447] Trial 42 finished with value: 61183.74971337263 and parameters: {'eta': 0.033202036524551726, 'max_depth': 6, 'subsample': 0.7888897867292846, 'colsample_bytree': 0.7657262630837732, 'lambda': 26.75896611960572, 'alpha': 14.332431686675744}. Best is trial 2 with value: 61160.35264782224.\n",
      "[I 2025-07-24 23:00:40,216] Trial 43 finished with value: 61299.69079250049 and parameters: {'eta': 0.03357749517838638, 'max_depth': 6, 'subsample': 0.7910708204077921, 'colsample_bytree': 0.8073949450063084, 'lambda': 49.68520428757301, 'alpha': 8.059417741679292}. Best is trial 2 with value: 61160.35264782224.\n",
      "[I 2025-07-24 23:00:51,326] Trial 44 finished with value: 61328.85980388491 and parameters: {'eta': 0.03842907475467077, 'max_depth': 6, 'subsample': 0.8150212605058745, 'colsample_bytree': 0.7798992799050312, 'lambda': 39.455513516262286, 'alpha': 12.751739937866134}. Best is trial 2 with value: 61160.35264782224.\n",
      "[I 2025-07-24 23:01:04,952] Trial 45 finished with value: 61656.09682592184 and parameters: {'eta': 0.045832534848974996, 'max_depth': 3, 'subsample': 0.7719274292257329, 'colsample_bytree': 0.7627267101958461, 'lambda': 27.779843594131965, 'alpha': 23.895276455655676}. Best is trial 2 with value: 61160.35264782224.\n",
      "[I 2025-07-24 23:01:16,423] Trial 46 finished with value: 61228.11513761261 and parameters: {'eta': 0.039480715890353596, 'max_depth': 6, 'subsample': 0.8277983582403332, 'colsample_bytree': 0.6319150223367087, 'lambda': 19.565602707627086, 'alpha': 18.94273218788402}. Best is trial 2 with value: 61160.35264782224.\n",
      "[I 2025-07-24 23:01:22,168] Trial 47 finished with value: 61569.76410259344 and parameters: {'eta': 0.046330902861819616, 'max_depth': 6, 'subsample': 0.6796897198262577, 'colsample_bytree': 0.6572387544339902, 'lambda': 23.460323449152465, 'alpha': 1.4543971463126653}. Best is trial 2 with value: 61160.35264782224.\n",
      "[I 2025-07-24 23:01:30,937] Trial 48 finished with value: 61255.02184466434 and parameters: {'eta': 0.03440036516896202, 'max_depth': 6, 'subsample': 0.8924922446970941, 'colsample_bytree': 0.8438881194087642, 'lambda': 15.289472681905243, 'alpha': 49.78925417391758}. Best is trial 2 with value: 61160.35264782224.\n",
      "[I 2025-07-24 23:01:48,248] Trial 49 finished with value: 61206.34655824931 and parameters: {'eta': 0.031679145621838455, 'max_depth': 6, 'subsample': 0.8466408939705481, 'colsample_bytree': 0.6885454121156648, 'lambda': 32.4486318253793, 'alpha': 16.5422194471656}. Best is trial 2 with value: 61160.35264782224.\n",
      "[I 2025-07-24 23:02:00,144] Trial 50 finished with value: 61337.623229889294 and parameters: {'eta': 0.03651500688991762, 'max_depth': 5, 'subsample': 0.6229499277686952, 'colsample_bytree': 0.8982265248544347, 'lambda': 41.38819577384866, 'alpha': 9.488783590961654}. Best is trial 2 with value: 61160.35264782224.\n",
      "[I 2025-07-24 23:02:17,827] Trial 51 finished with value: 61174.95833892403 and parameters: {'eta': 0.030528623432152253, 'max_depth': 6, 'subsample': 0.8541835943471769, 'colsample_bytree': 0.6829183754822922, 'lambda': 33.88926899191066, 'alpha': 16.417460935496237}. Best is trial 2 with value: 61160.35264782224.\n",
      "[I 2025-07-24 23:02:26,231] Trial 52 finished with value: 61298.03024801107 and parameters: {'eta': 0.04888535007025972, 'max_depth': 6, 'subsample': 0.8680084088435992, 'colsample_bytree': 0.6123946543857076, 'lambda': 27.607622010667708, 'alpha': 14.239670958413267}. Best is trial 2 with value: 61160.35264782224.\n",
      "[I 2025-07-24 23:02:39,572] Trial 53 finished with value: 61197.84709475213 and parameters: {'eta': 0.030050080704532845, 'max_depth': 6, 'subsample': 0.8512522424582416, 'colsample_bytree': 0.6731231792490693, 'lambda': 34.203924990268725, 'alpha': 20.33740096577164}. Best is trial 2 with value: 61160.35264782224.\n",
      "[I 2025-07-24 23:02:52,900] Trial 54 finished with value: 61332.251194563694 and parameters: {'eta': 0.028828216078671663, 'max_depth': 6, 'subsample': 0.8905003537088347, 'colsample_bytree': 0.7091203966174993, 'lambda': 34.02401735371144, 'alpha': 21.78993795824734}. Best is trial 2 with value: 61160.35264782224.\n",
      "[I 2025-07-24 23:03:09,814] Trial 55 finished with value: 61152.09796041988 and parameters: {'eta': 0.02568463423147262, 'max_depth': 6, 'subsample': 0.8489061650107588, 'colsample_bytree': 0.725843698854554, 'lambda': 23.463899297827314, 'alpha': 12.190817440946875}. Best is trial 55 with value: 61152.09796041988.\n",
      "[I 2025-07-24 23:03:27,648] Trial 56 finished with value: 61199.30940628601 and parameters: {'eta': 0.025545304201485382, 'max_depth': 6, 'subsample': 0.8522764025113262, 'colsample_bytree': 0.7220543001644234, 'lambda': 23.90992740265565, 'alpha': 11.36239261529319}. Best is trial 55 with value: 61152.09796041988.\n",
      "[I 2025-07-24 23:03:42,604] Trial 57 finished with value: 61186.65617013734 and parameters: {'eta': 0.031065218556860806, 'max_depth': 6, 'subsample': 0.8363572468894376, 'colsample_bytree': 0.6678604616143822, 'lambda': 42.51686578640902, 'alpha': 16.49584679139061}. Best is trial 55 with value: 61152.09796041988.\n",
      "[I 2025-07-24 23:04:03,077] Trial 58 finished with value: 61638.216155138 and parameters: {'eta': 0.03134248133466599, 'max_depth': 3, 'subsample': 0.8351246249938552, 'colsample_bytree': 0.6995300868804345, 'lambda': 46.963121620486916, 'alpha': 15.802253108000714}. Best is trial 55 with value: 61152.09796041988.\n",
      "[I 2025-07-24 23:04:19,286] Trial 59 finished with value: 61325.030108191764 and parameters: {'eta': 0.027801656326756366, 'max_depth': 5, 'subsample': 0.8037003132478825, 'colsample_bytree': 0.6829131741142905, 'lambda': 28.547550264859176, 'alpha': 9.884512565755585}. Best is trial 55 with value: 61152.09796041988.\n",
      "[I 2025-07-24 23:04:41,665] Trial 60 finished with value: 61114.43542401074 and parameters: {'eta': 0.024609813623433893, 'max_depth': 6, 'subsample': 0.8763206381911133, 'colsample_bytree': 0.7332949464147935, 'lambda': 18.15557818566877, 'alpha': 7.210843804685465}. Best is trial 60 with value: 61114.43542401074.\n",
      "[I 2025-07-24 23:04:58,700] Trial 61 finished with value: 61226.84884678597 and parameters: {'eta': 0.023142261975421262, 'max_depth': 6, 'subsample': 0.8738117050311771, 'colsample_bytree': 0.7282651509808673, 'lambda': 22.67725648254854, 'alpha': 7.6595412659637825}. Best is trial 60 with value: 61114.43542401074.\n",
      "[I 2025-07-24 23:05:13,798] Trial 62 finished with value: 61203.61862377977 and parameters: {'eta': 0.024471623539060242, 'max_depth': 6, 'subsample': 0.8855705846207211, 'colsample_bytree': 0.766950925559383, 'lambda': 16.997410053918664, 'alpha': 6.50625895356981}. Best is trial 60 with value: 61114.43542401074.\n",
      "[I 2025-07-24 23:05:34,836] Trial 63 finished with value: 61135.38645832823 and parameters: {'eta': 0.027013394810892224, 'max_depth': 6, 'subsample': 0.824165423659943, 'colsample_bytree': 0.7061239436603909, 'lambda': 36.62762960554429, 'alpha': 5.303486255326515}. Best is trial 60 with value: 61114.43542401074.\n",
      "[I 2025-07-24 23:05:48,240] Trial 64 finished with value: 61306.32690517914 and parameters: {'eta': 0.02671398799337241, 'max_depth': 6, 'subsample': 0.8236333202405008, 'colsample_bytree': 0.7545332904322227, 'lambda': 35.4011716968533, 'alpha': 5.3329799953219545}. Best is trial 60 with value: 61114.43542401074.\n",
      "[I 2025-07-24 23:06:08,258] Trial 65 finished with value: 61132.58588764026 and parameters: {'eta': 0.028531114276019613, 'max_depth': 6, 'subsample': 0.8727903726574969, 'colsample_bytree': 0.7901456823450319, 'lambda': 18.85040533292707, 'alpha': 4.285835241375744}. Best is trial 60 with value: 61114.43542401074.\n",
      "[I 2025-07-24 23:06:20,014] Trial 66 finished with value: 61135.60850596821 and parameters: {'eta': 0.025065125679766462, 'max_depth': 6, 'subsample': 0.8602439719063582, 'colsample_bytree': 0.7876001430197042, 'lambda': 13.83270806072649, 'alpha': 4.35029923792602}. Best is trial 60 with value: 61114.43542401074.\n",
      "[I 2025-07-24 23:06:36,556] Trial 67 finished with value: 61165.675430781244 and parameters: {'eta': 0.022489283555086365, 'max_depth': 6, 'subsample': 0.8597080263744966, 'colsample_bytree': 0.7917662369462634, 'lambda': 13.75118562799462, 'alpha': 4.3237107745113805}. Best is trial 60 with value: 61114.43542401074.\n",
      "[I 2025-07-24 23:06:56,003] Trial 68 finished with value: 61228.55116503945 and parameters: {'eta': 0.022442494913847876, 'max_depth': 6, 'subsample': 0.8990178631307654, 'colsample_bytree': 0.795385179465961, 'lambda': 14.329424634063537, 'alpha': 3.1256589037921185}. Best is trial 60 with value: 61114.43542401074.\n",
      "[I 2025-07-24 23:07:17,350] Trial 69 finished with value: 61239.65682626029 and parameters: {'eta': 0.018591278018382514, 'max_depth': 6, 'subsample': 0.8656233971786945, 'colsample_bytree': 0.8272323521181865, 'lambda': 13.360785272979761, 'alpha': 4.304768331989374}. Best is trial 60 with value: 61114.43542401074.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Error model tuning complete. Best validation RMSE: $61,114.44\n",
      "\n",
      "--- K-Fold training the final error model... ---\n",
      "  Training XGB error model for fold 1/5...\n",
      "  Training XGB error model for fold 2/5...\n",
      "  Training XGB error model for fold 3/5...\n",
      "  Training XGB error model for fold 4/5...\n",
      "  Training XGB error model for fold 5/5...\n",
      "\n",
      "Final Error Model OOF RMSE: $60,484.58\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# BLOCK 5: TUNE AND TRAIN THE XGBOOST ERROR MODEL\n",
    "# =============================================================================\n",
    "\n",
    "# --- 1. Prepare Features and Target for the Error Model ---\n",
    "print(\"\\n--- Preparing features for the XGBoost error model ---\")\n",
    "# The error target is the absolute difference between the blended mean and the true price\n",
    "error_target = np.abs(y_true - oof_ensemble_mean)\n",
    "\n",
    "# The features are the full feature set PLUS the OOF predictions of the mean models\n",
    "X_for_error = X.copy()\n",
    "X_for_error['oof_xgb'] = oof_xgb_preds\n",
    "X_for_error['oof_cb'] = oof_cb_preds\n",
    "X_for_error['oof_lgbm'] = oof_lgbm_preds\n",
    "X_for_error['oof_nn'] = oof_nn_preds\n",
    "\n",
    "# Do the same for the test set, ensuring column names are consistent\n",
    "X_test_for_error = X_test.copy()\n",
    "X_test_for_error['oof_xgb'] = test_xgb_preds\n",
    "X_test_for_error['oof_cb'] = test_cb_preds\n",
    "X_test_for_error['oof_lgbm'] = test_lgbm_preds\n",
    "X_test_for_error['oof_nn'] = test_nn_preds\n",
    "\n",
    "# --- 2. Tune the Error Model with Optuna ---\n",
    "print(\"\\n--- Tuning the XGBoost Error Model with Optuna... ---\")\n",
    "X_train_err_opt, X_val_err_opt, y_train_err_opt, y_val_err_opt = train_test_split(\n",
    "    X_for_error, error_target, test_size=0.2, random_state=RANDOM_STATE\n",
    ")\n",
    "dtrain_err_opt = xgb.DMatrix(X_train_err_opt, label=y_train_err_opt)\n",
    "dval_err_opt = xgb.DMatrix(X_val_err_opt, label=y_val_err_opt)\n",
    "\n",
    "def objective_error_model(trial):\n",
    "    params = {\n",
    "        'objective': 'reg:squarederror', 'eval_metric': 'rmse', 'tree_method': 'hist',\n",
    "        'n_jobs': -1, 'seed': RANDOM_STATE,\n",
    "        'eta': trial.suggest_float('eta', 0.01, 0.05, log=True),\n",
    "        'max_depth': trial.suggest_int('max_depth', 3, 6),\n",
    "        'subsample': trial.suggest_float('subsample', 0.6, 0.9),\n",
    "        'colsample_bytree': trial.suggest_float('colsample_bytree', 0.6, 0.9),\n",
    "        'lambda': trial.suggest_float('lambda', 1.0, 50.0, log=True),\n",
    "        'alpha': trial.suggest_float('alpha', 1.0, 50.0, log=True)\n",
    "    }\n",
    "    bst = xgb.train(params, dtrain_err_opt, num_boost_round=2000, \n",
    "                    evals=[(dval_err_opt, 'val')], early_stopping_rounds=75, verbose_eval=False)\n",
    "    preds = bst.predict(dval_err_opt, iteration_range=(0, bst.best_iteration))\n",
    "    return np.sqrt(mean_squared_error(y_val_err_opt, preds))\n",
    "\n",
    "study_error = optuna.create_study(direction='minimize')\n",
    "study_error.optimize(objective_error_model, n_trials=70)\n",
    "best_params_error = study_error.best_params\n",
    "best_params_error['n_estimators'] = study_error.best_trial.user_attrs.get('best_iteration', 2000)\n",
    "print(f\"\\nError model tuning complete. Best validation RMSE: ${study_error.best_value:,.2f}\")\n",
    "\n",
    "# --- 3. Final K-Fold Training of the Error Model ---\n",
    "print(\"\\n--- K-Fold training the final error model... ---\")\n",
    "oof_error_preds = np.zeros(len(X))\n",
    "test_error_preds = np.zeros(len(X_test))\n",
    "skf = StratifiedKFold(n_splits=N_SPLITS, shuffle=True, random_state=RANDOM_STATE)\n",
    "\n",
    "for fold, (train_idx, val_idx) in enumerate(skf.split(X_for_error, grade_for_stratify)):\n",
    "    print(f\"  Training XGB error model for fold {fold+1}/{N_SPLITS}...\")\n",
    "    X_train_err, X_val_err = X_for_error.iloc[train_idx], X_for_error.iloc[val_idx]\n",
    "    y_train_err, y_val_err = error_target.iloc[train_idx], error_target.iloc[val_idx]\n",
    "\n",
    "    model_err = xgb.XGBRegressor(**best_params_error)\n",
    "    model_err.fit(X_train_err, y_train_err)\n",
    "    \n",
    "    oof_error_preds[val_idx] = np.clip(model_err.predict(X_val_err), 0, None)\n",
    "    test_error_preds += np.clip(model_err.predict(X_test_for_error), 0, None) / N_SPLITS\n",
    "\n",
    "print(f\"\\nFinal Error Model OOF RMSE: ${np.sqrt(mean_squared_error(error_target, oof_error_preds)):,.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "01f24825-8632-4af8-a9ea-1f9121437e7b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Calibrating the final intervals with an Optimizer ---\n",
      "\n",
      "============================================================\n",
      "FINAL RESULTS\n",
      "============================================================\n",
      "Final OOF Winkler Score (4 Mean + 1 XGB Error): $293,131.26\n",
      "Optimal Multipliers: a=1.9797, b=2.1747\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# BLOCK 6: FINAL CALIBRATION AND SUBMISSION\n",
    "# =============================================================================\n",
    "\n",
    "print(\"\\n--- Calibrating the final intervals with an Optimizer ---\")\n",
    "oof_error_final = np.clip(oof_error_preds, 0, None)\n",
    "\n",
    "def get_winkler_from_multipliers(multipliers):\n",
    "    a, b = multipliers[0], multipliers[1]\n",
    "    low = oof_ensemble_mean - oof_error_final * a\n",
    "    high = oof_ensemble_mean + oof_error_final * b\n",
    "    return winkler_score(y_true, low, high)\n",
    "\n",
    "initial_guess = [1.5, 1.5]\n",
    "bounds = [(0.5, 4.0), (0.5, 4.0)]\n",
    "result_calib = minimize(get_winkler_from_multipliers, initial_guess, method='L-BFGS-B', bounds=bounds)\n",
    "best_a, best_b = result_calib.x\n",
    "best_score = result_calib.fun\n",
    "\n",
    "print(\"\\n\" + \"=\"*60); print(\"FINAL RESULTS\"); print(\"=\"*60)\n",
    "print(f\"Final OOF Winkler Score (4 Mean + 1 XGB Error): ${best_score:,.2f}\")\n",
    "print(f\"Optimal Multipliers: a={best_a:.4f}, b={best_b:.4f}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31e53007-0d58-48b3-9634-0e4b4419ff25",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Create Submission File ---\n",
    "print(\"\\n--- Creating final submission file... ---\")\n",
    "final_lower = test_ensemble_mean - np.clip(test_error_preds, 0, None) * best_a\n",
    "final_upper = test_ensemble_mean + np.clip(test_error_preds, 0, None) * best_b\n",
    "final_upper = np.maximum(final_lower + 1, final_upper) # Safety check\n",
    "\n",
    "submission_df = pd.DataFrame({'id': pd.read_csv(os.path.join(DATA_PATH, 'test.csv'))['id'], \n",
    "                              'pi_lower': final_lower, \n",
    "                              'pi_upper': final_upper})\n",
    "submission_filename = f'submission_3M1E_XGB_{int(best_score)}.csv'\n",
    "submission_df.to_csv(submission_filename, index=False)\n",
    "print(f\"\\n'{submission_filename}' created successfully!\")\n",
    "display(submission_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e3a28a8f-b2d6-474f-98bd-e450b6159e9b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-07-24 23:09:37,443] A new study created in memory with name: no-name-ecfa461e-e897-430a-ac90-1a25ee78589f\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Preparing features for the CatBoost error model ---\n",
      "\n",
      "--- Tuning the CatBoost Error Model with Optuna... ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-07-24 23:09:54,845] Trial 0 finished with value: 61469.03336599517 and parameters: {'iterations': 2482, 'learning_rate': 0.019112171719833218, 'depth': 6, 'l2_leaf_reg': 5.678502599472514, 'subsample': 0.6367525143282068, 'random_strength': 5.478896729634377}. Best is trial 0 with value: 61469.03336599517.\n",
      "[I 2025-07-24 23:10:04,739] Trial 1 finished with value: 61959.377937245226 and parameters: {'iterations': 2071, 'learning_rate': 0.016222624946220735, 'depth': 4, 'l2_leaf_reg': 1.0917476416508312, 'subsample': 0.6107510045067848, 'random_strength': 2.917492128292178}. Best is trial 0 with value: 61469.03336599517.\n",
      "[I 2025-07-24 23:10:20,910] Trial 2 finished with value: 61523.53355184494 and parameters: {'iterations': 2412, 'learning_rate': 0.01689411019542091, 'depth': 6, 'l2_leaf_reg': 1.6872331844834152, 'subsample': 0.6030289135677183, 'random_strength': 2.719275110251027}. Best is trial 0 with value: 61469.03336599517.\n",
      "[I 2025-07-24 23:10:27,992] Trial 3 finished with value: 61657.03889062615 and parameters: {'iterations': 1124, 'learning_rate': 0.036197451289701026, 'depth': 5, 'l2_leaf_reg': 1.124874655516558, 'subsample': 0.7869063086015842, 'random_strength': 3.0918592538776912}. Best is trial 0 with value: 61469.03336599517.\n",
      "[I 2025-07-24 23:10:42,294] Trial 4 finished with value: 61572.45427443665 and parameters: {'iterations': 2232, 'learning_rate': 0.029416071162350996, 'depth': 5, 'l2_leaf_reg': 1.7161471264726307, 'subsample': 0.8959292874790539, 'random_strength': 2.7633686655009755}. Best is trial 0 with value: 61469.03336599517.\n",
      "[I 2025-07-24 23:10:51,881] Trial 5 finished with value: 61996.33701204171 and parameters: {'iterations': 2307, 'learning_rate': 0.026120440915345575, 'depth': 3, 'l2_leaf_reg': 1.0108786495877096, 'subsample': 0.7088626183268565, 'random_strength': 3.793927281597669}. Best is trial 0 with value: 61469.03336599517.\n",
      "[I 2025-07-24 23:11:00,480] Trial 6 finished with value: 61655.20985930973 and parameters: {'iterations': 1136, 'learning_rate': 0.02499593926473055, 'depth': 6, 'l2_leaf_reg': 2.582969239279483, 'subsample': 0.7186817721504642, 'random_strength': 9.57451348011744}. Best is trial 0 with value: 61469.03336599517.\n",
      "[I 2025-07-24 23:11:09,704] Trial 7 finished with value: 61943.44775070609 and parameters: {'iterations': 2058, 'learning_rate': 0.03720627020604288, 'depth': 3, 'l2_leaf_reg': 9.212035584423207, 'subsample': 0.7307731227324652, 'random_strength': 6.116525137790246}. Best is trial 0 with value: 61469.03336599517.\n",
      "[I 2025-07-24 23:11:27,911] Trial 8 finished with value: 61451.80996619189 and parameters: {'iterations': 2503, 'learning_rate': 0.026858568558315084, 'depth': 6, 'l2_leaf_reg': 17.455183544538606, 'subsample': 0.6301424963565913, 'random_strength': 1.863347396725377}. Best is trial 8 with value: 61451.80996619189.\n",
      "[I 2025-07-24 23:11:39,693] Trial 9 finished with value: 62074.49223290716 and parameters: {'iterations': 2843, 'learning_rate': 0.01595911637697987, 'depth': 3, 'l2_leaf_reg': 5.029147148744264, 'subsample': 0.6090923750520384, 'random_strength': 7.301509414075}. Best is trial 8 with value: 61451.80996619189.\n",
      "[I 2025-07-24 23:11:49,129] Trial 10 finished with value: 62523.554001432065 and parameters: {'iterations': 1707, 'learning_rate': 0.012204078549105636, 'depth': 5, 'l2_leaf_reg': 35.83670753284198, 'subsample': 0.8028339153324757, 'random_strength': 1.1547630322903468}. Best is trial 8 with value: 61451.80996619189.\n",
      "[I 2025-07-24 23:11:58,876] Trial 11 finished with value: 61424.72363532117 and parameters: {'iterations': 2821, 'learning_rate': 0.04950358391506606, 'depth': 6, 'l2_leaf_reg': 15.42108023802699, 'subsample': 0.6660256107223155, 'random_strength': 1.4538979701470558}. Best is trial 11 with value: 61424.72363532117.\n",
      "[I 2025-07-24 23:12:11,859] Trial 12 finished with value: 61415.61842270821 and parameters: {'iterations': 2952, 'learning_rate': 0.04910492380029703, 'depth': 6, 'l2_leaf_reg': 23.105377723552127, 'subsample': 0.6717577530177256, 'random_strength': 1.3919328384733565}. Best is trial 12 with value: 61415.61842270821.\n",
      "[I 2025-07-24 23:12:27,413] Trial 13 finished with value: 61655.53059397366 and parameters: {'iterations': 2959, 'learning_rate': 0.04980600962907879, 'depth': 4, 'l2_leaf_reg': 45.672171207438666, 'subsample': 0.6826451729838289, 'random_strength': 1.0387140661946592}. Best is trial 12 with value: 61415.61842270821.\n",
      "[I 2025-07-24 23:12:42,172] Trial 14 finished with value: 61522.81962572479 and parameters: {'iterations': 2748, 'learning_rate': 0.04338937667080371, 'depth': 5, 'l2_leaf_reg': 19.666219811568208, 'subsample': 0.6685587261270606, 'random_strength': 1.599646968709388}. Best is trial 12 with value: 61415.61842270821.\n",
      "[I 2025-07-24 23:12:53,139] Trial 15 finished with value: 61471.19420115495 and parameters: {'iterations': 1694, 'learning_rate': 0.049154087937075305, 'depth': 6, 'l2_leaf_reg': 15.03077527234707, 'subsample': 0.6706524527928607, 'random_strength': 1.676236285708154}. Best is trial 12 with value: 61415.61842270821.\n",
      "[I 2025-07-24 23:13:12,771] Trial 16 finished with value: 61373.12027808772 and parameters: {'iterations': 2710, 'learning_rate': 0.03530094730574954, 'depth': 6, 'l2_leaf_reg': 24.313212860413465, 'subsample': 0.7911126851158836, 'random_strength': 1.3285728029275963}. Best is trial 16 with value: 61373.12027808772.\n",
      "[I 2025-07-24 23:13:15,430] Trial 17 finished with value: 62653.71652710051 and parameters: {'iterations': 2625, 'learning_rate': 0.03395325515501928, 'depth': 4, 'l2_leaf_reg': 30.098588412927725, 'subsample': 0.779635814710892, 'random_strength': 2.1067184077212637}. Best is trial 16 with value: 61373.12027808772.\n",
      "[I 2025-07-24 23:13:27,137] Trial 18 finished with value: 61590.165051846096 and parameters: {'iterations': 2978, 'learning_rate': 0.040280556304580654, 'depth': 5, 'l2_leaf_reg': 9.230888086801999, 'subsample': 0.8460790454186561, 'random_strength': 1.2773010099104252}. Best is trial 16 with value: 61373.12027808772.\n",
      "[I 2025-07-24 23:13:31,151] Trial 19 finished with value: 62284.38958712442 and parameters: {'iterations': 1742, 'learning_rate': 0.03223796010290549, 'depth': 6, 'l2_leaf_reg': 25.87434248491084, 'subsample': 0.7534089821596426, 'random_strength': 2.1587607000925453}. Best is trial 16 with value: 61373.12027808772.\n",
      "[I 2025-07-24 23:13:35,801] Trial 20 finished with value: 62415.4131650204 and parameters: {'iterations': 2654, 'learning_rate': 0.02126470426750988, 'depth': 5, 'l2_leaf_reg': 48.50757593694928, 'subsample': 0.8359288300124588, 'random_strength': 3.8923782665955304}. Best is trial 16 with value: 61373.12027808772.\n",
      "[I 2025-07-24 23:13:44,892] Trial 21 finished with value: 61497.46624183259 and parameters: {'iterations': 2806, 'learning_rate': 0.04364517024128607, 'depth': 6, 'l2_leaf_reg': 12.46845203469922, 'subsample': 0.6917787772864626, 'random_strength': 1.3911949218567956}. Best is trial 16 with value: 61373.12027808772.\n",
      "[I 2025-07-24 23:14:01,395] Trial 22 finished with value: 61465.80341205621 and parameters: {'iterations': 3000, 'learning_rate': 0.042833062875448, 'depth': 6, 'l2_leaf_reg': 24.871338141972842, 'subsample': 0.7544329839931692, 'random_strength': 1.4488439343438315}. Best is trial 16 with value: 61373.12027808772.\n",
      "[I 2025-07-24 23:14:12,739] Trial 23 finished with value: 61414.500742668926 and parameters: {'iterations': 2626, 'learning_rate': 0.049163772761116234, 'depth': 6, 'l2_leaf_reg': 12.933795266791215, 'subsample': 0.6443180376825673, 'random_strength': 1.1148633257940819}. Best is trial 16 with value: 61373.12027808772.\n",
      "[I 2025-07-24 23:14:23,074] Trial 24 finished with value: 61431.85306614776 and parameters: {'iterations': 2644, 'learning_rate': 0.038572842070725685, 'depth': 6, 'l2_leaf_reg': 10.664118248200447, 'subsample': 0.6495763863332578, 'random_strength': 1.0207723947027312}. Best is trial 16 with value: 61373.12027808772.\n",
      "[I 2025-07-24 23:14:36,667] Trial 25 finished with value: 61589.55223570526 and parameters: {'iterations': 2288, 'learning_rate': 0.03040452204012501, 'depth': 5, 'l2_leaf_reg': 6.555608307201707, 'subsample': 0.8310982706687804, 'random_strength': 2.2314613483213024}. Best is trial 16 with value: 61373.12027808772.\n",
      "[I 2025-07-24 23:14:42,746] Trial 26 finished with value: 61633.54752097532 and parameters: {'iterations': 1486, 'learning_rate': 0.044921356177097325, 'depth': 6, 'l2_leaf_reg': 4.092718226267149, 'subsample': 0.7365800088649593, 'random_strength': 1.1799876763448438}. Best is trial 16 with value: 61373.12027808772.\n",
      "[I 2025-07-24 23:14:51,459] Trial 27 finished with value: 62427.40268563285 and parameters: {'iterations': 2624, 'learning_rate': 0.01022607721293146, 'depth': 5, 'l2_leaf_reg': 19.539095766654146, 'subsample': 0.7034466855794957, 'random_strength': 1.7524438708271466}. Best is trial 16 with value: 61373.12027808772.\n",
      "[I 2025-07-24 23:15:04,892] Trial 28 finished with value: 61638.00436759579 and parameters: {'iterations': 1898, 'learning_rate': 0.03473926703109411, 'depth': 6, 'l2_leaf_reg': 35.22512444560973, 'subsample': 0.8700502866074266, 'random_strength': 1.2238669453945084}. Best is trial 16 with value: 61373.12027808772.\n",
      "[I 2025-07-24 23:15:17,181] Trial 29 finished with value: 61729.5099606622 and parameters: {'iterations': 2475, 'learning_rate': 0.03977304954565254, 'depth': 4, 'l2_leaf_reg': 23.060019079439748, 'subsample': 0.63790959528982, 'random_strength': 4.051241294412428}. Best is trial 16 with value: 61373.12027808772.\n",
      "[I 2025-07-24 23:15:35,958] Trial 30 finished with value: 61509.39089697231 and parameters: {'iterations': 2530, 'learning_rate': 0.022227902523460696, 'depth': 6, 'l2_leaf_reg': 12.448992330636365, 'subsample': 0.8076413072464727, 'random_strength': 1.9741711683008236}. Best is trial 16 with value: 61373.12027808772.\n",
      "[I 2025-07-24 23:15:48,776] Trial 31 finished with value: 61399.539132600956 and parameters: {'iterations': 2833, 'learning_rate': 0.04906766849167502, 'depth': 6, 'l2_leaf_reg': 14.866415997790506, 'subsample': 0.6565473699156491, 'random_strength': 1.515021215478073}. Best is trial 16 with value: 61373.12027808772.\n",
      "[I 2025-07-24 23:15:59,414] Trial 32 finished with value: 61525.67933435342 and parameters: {'iterations': 2890, 'learning_rate': 0.046043471125804813, 'depth': 6, 'l2_leaf_reg': 8.890822425167485, 'subsample': 0.6472615951112414, 'random_strength': 1.5401491253493291}. Best is trial 16 with value: 61373.12027808772.\n",
      "[I 2025-07-24 23:16:10,923] Trial 33 finished with value: 61382.09390322347 and parameters: {'iterations': 2703, 'learning_rate': 0.0403573772528245, 'depth': 6, 'l2_leaf_reg': 14.443254109716639, 'subsample': 0.6237030011984266, 'random_strength': 1.29972548366113}. Best is trial 16 with value: 61373.12027808772.\n",
      "[I 2025-07-24 23:16:24,188] Trial 34 finished with value: 61549.368207765445 and parameters: {'iterations': 2402, 'learning_rate': 0.03987456711469152, 'depth': 6, 'l2_leaf_reg': 14.676602893870527, 'subsample': 0.6244751575710158, 'random_strength': 2.4193778127374395}. Best is trial 16 with value: 61373.12027808772.\n",
      "[I 2025-07-24 23:16:39,110] Trial 35 finished with value: 61334.271199711555 and parameters: {'iterations': 2679, 'learning_rate': 0.03137420472961067, 'depth': 6, 'l2_leaf_reg': 7.194787912270065, 'subsample': 0.614891767617751, 'random_strength': 1.0504907474803682}. Best is trial 35 with value: 61334.271199711555.\n",
      "[I 2025-07-24 23:16:52,563] Trial 36 finished with value: 61577.100581529376 and parameters: {'iterations': 2190, 'learning_rate': 0.029286958503472987, 'depth': 5, 'l2_leaf_reg': 3.664411387991481, 'subsample': 0.6177044542305048, 'random_strength': 1.0098597803547638}. Best is trial 35 with value: 61334.271199711555.\n",
      "[I 2025-07-24 23:17:06,907] Trial 37 finished with value: 61409.23181889535 and parameters: {'iterations': 2737, 'learning_rate': 0.032351997960071495, 'depth': 6, 'l2_leaf_reg': 6.3947252849389224, 'subsample': 0.6023383803448792, 'random_strength': 1.307435277393916}. Best is trial 35 with value: 61334.271199711555.\n",
      "[I 2025-07-24 23:17:21,592] Trial 38 finished with value: 61599.96709198443 and parameters: {'iterations': 2394, 'learning_rate': 0.024636503635392524, 'depth': 5, 'l2_leaf_reg': 5.115608158511218, 'subsample': 0.7704755964168256, 'random_strength': 3.41063220455276}. Best is trial 35 with value: 61334.271199711555.\n",
      "[I 2025-07-24 23:17:38,377] Trial 39 finished with value: 61402.37072939004 and parameters: {'iterations': 2145, 'learning_rate': 0.028255193303625855, 'depth': 6, 'l2_leaf_reg': 8.04102091026863, 'subsample': 0.6249750458046622, 'random_strength': 1.7398899038703786}. Best is trial 35 with value: 61334.271199711555.\n",
      "[I 2025-07-24 23:17:48,205] Trial 40 finished with value: 61579.2738763848 and parameters: {'iterations': 2727, 'learning_rate': 0.03559422994624786, 'depth': 6, 'l2_leaf_reg': 2.690216841641949, 'subsample': 0.6015468541952019, 'random_strength': 2.480421551137907}. Best is trial 35 with value: 61334.271199711555.\n",
      "[I 2025-07-24 23:18:03,622] Trial 41 finished with value: 61430.46402282271 and parameters: {'iterations': 2557, 'learning_rate': 0.02823275950493406, 'depth': 6, 'l2_leaf_reg': 10.113970864409685, 'subsample': 0.6269252065700336, 'random_strength': 1.7334503505692316}. Best is trial 35 with value: 61334.271199711555.\n",
      "[I 2025-07-24 23:18:18,963] Trial 42 finished with value: 61323.22388172411 and parameters: {'iterations': 2153, 'learning_rate': 0.03208660109215744, 'depth': 6, 'l2_leaf_reg': 7.70084793504239, 'subsample': 0.6578338362139209, 'random_strength': 1.2695358553261544}. Best is trial 42 with value: 61323.22388172411.\n",
      "[I 2025-07-24 23:18:37,199] Trial 43 finished with value: 61440.471750482146 and parameters: {'iterations': 2343, 'learning_rate': 0.031872529532506196, 'depth': 6, 'l2_leaf_reg': 17.705255830534142, 'subsample': 0.6584576688528678, 'random_strength': 1.2661676222509832}. Best is trial 42 with value: 61323.22388172411.\n",
      "[I 2025-07-24 23:18:58,091] Trial 44 finished with value: 61436.012150853116 and parameters: {'iterations': 2882, 'learning_rate': 0.01751945090387651, 'depth': 6, 'l2_leaf_reg': 7.5338672133380875, 'subsample': 0.6880127366043639, 'random_strength': 1.1233926090202582}. Best is trial 42 with value: 61323.22388172411.\n",
      "[I 2025-07-24 23:19:09,704] Trial 45 finished with value: 61511.952157140615 and parameters: {'iterations': 1921, 'learning_rate': 0.036712666401272756, 'depth': 6, 'l2_leaf_reg': 5.80101788415408, 'subsample': 0.8013269974509505, 'random_strength': 1.5410681723844972}. Best is trial 42 with value: 61323.22388172411.\n",
      "[I 2025-07-24 23:19:16,750] Trial 46 finished with value: 61834.008365525086 and parameters: {'iterations': 1206, 'learning_rate': 0.025716884788244104, 'depth': 5, 'l2_leaf_reg': 11.316580178042365, 'subsample': 0.6550047085212497, 'random_strength': 4.66097931038905}. Best is trial 42 with value: 61323.22388172411.\n",
      "[I 2025-07-24 23:19:25,017] Trial 47 finished with value: 62032.41649645958 and parameters: {'iterations': 2768, 'learning_rate': 0.023638726367147204, 'depth': 3, 'l2_leaf_reg': 1.5694689755280844, 'subsample': 0.614179918138209, 'random_strength': 9.557741824881731}. Best is trial 42 with value: 61323.22388172411.\n",
      "[I 2025-07-24 23:19:43,868] Trial 48 finished with value: 61460.50826062708 and parameters: {'iterations': 2502, 'learning_rate': 0.042121077586813827, 'depth': 6, 'l2_leaf_reg': 30.43001273773265, 'subsample': 0.7052522877480308, 'random_strength': 1.3420914532400359}. Best is trial 42 with value: 61323.22388172411.\n",
      "[I 2025-07-24 23:19:51,999] Trial 49 finished with value: 61668.68353365819 and parameters: {'iterations': 2047, 'learning_rate': 0.037381449332817625, 'depth': 5, 'l2_leaf_reg': 4.34218733324949, 'subsample': 0.7214294207451222, 'random_strength': 1.110765822652124}. Best is trial 42 with value: 61323.22388172411.\n",
      "[I 2025-07-24 23:20:09,221] Trial 50 finished with value: 61480.13819989755 and parameters: {'iterations': 2877, 'learning_rate': 0.020006773163899382, 'depth': 6, 'l2_leaf_reg': 3.238477941340838, 'subsample': 0.6341697410083744, 'random_strength': 1.8919898905188326}. Best is trial 42 with value: 61323.22388172411.\n",
      "[I 2025-07-24 23:20:25,590] Trial 51 finished with value: 61442.74400592719 and parameters: {'iterations': 2141, 'learning_rate': 0.02657217846845569, 'depth': 6, 'l2_leaf_reg': 8.084813166793175, 'subsample': 0.6263974975827448, 'random_strength': 1.546367802248511}. Best is trial 42 with value: 61323.22388172411.\n",
      "[I 2025-07-24 23:20:40,548] Trial 52 finished with value: 61376.51193177412 and parameters: {'iterations': 2127, 'learning_rate': 0.027948279375058907, 'depth': 6, 'l2_leaf_reg': 7.144974305394028, 'subsample': 0.616566448727728, 'random_strength': 1.1988423242244104}. Best is trial 42 with value: 61323.22388172411.\n",
      "[I 2025-07-24 23:20:54,745] Trial 53 finished with value: 61378.446474058626 and parameters: {'iterations': 1895, 'learning_rate': 0.03353548270467612, 'depth': 6, 'l2_leaf_reg': 5.533864818886779, 'subsample': 0.6139630052993699, 'random_strength': 1.2816048755529945}. Best is trial 42 with value: 61323.22388172411.\n",
      "[I 2025-07-24 23:21:06,756] Trial 54 finished with value: 61468.65896546683 and parameters: {'iterations': 1850, 'learning_rate': 0.03188981448386424, 'depth': 6, 'l2_leaf_reg': 5.162642188140892, 'subsample': 0.6128136398553857, 'random_strength': 1.1887518963990362}. Best is trial 42 with value: 61323.22388172411.\n",
      "[I 2025-07-24 23:21:19,663] Trial 55 finished with value: 61397.82413492331 and parameters: {'iterations': 1972, 'learning_rate': 0.0336694791819937, 'depth': 6, 'l2_leaf_reg': 7.015859089650101, 'subsample': 0.6383510410035891, 'random_strength': 1.364591003829076}. Best is trial 42 with value: 61323.22388172411.\n",
      "[I 2025-07-24 23:21:31,338] Trial 56 finished with value: 61464.83530030034 and parameters: {'iterations': 1770, 'learning_rate': 0.02992857368730146, 'depth': 6, 'l2_leaf_reg': 5.740195707260853, 'subsample': 0.6008878079147972, 'random_strength': 1.0810142166081829}. Best is trial 42 with value: 61323.22388172411.\n",
      "[I 2025-07-24 23:21:42,582] Trial 57 finished with value: 61490.46966425755 and parameters: {'iterations': 1590, 'learning_rate': 0.027623269033415984, 'depth': 6, 'l2_leaf_reg': 4.48245953730155, 'subsample': 0.618781081502277, 'random_strength': 1.2252496227610117}. Best is trial 42 with value: 61323.22388172411.\n",
      "[I 2025-07-24 23:21:55,825] Trial 58 finished with value: 61419.874250714325 and parameters: {'iterations': 2084, 'learning_rate': 0.03416254244695816, 'depth': 6, 'l2_leaf_reg': 3.1198791887313084, 'subsample': 0.610995738228925, 'random_strength': 1.3831317033221646}. Best is trial 42 with value: 61323.22388172411.\n",
      "[I 2025-07-24 23:22:07,437] Trial 59 finished with value: 61580.66960446182 and parameters: {'iterations': 2274, 'learning_rate': 0.03013247691234283, 'depth': 5, 'l2_leaf_reg': 6.328252022199715, 'subsample': 0.6755534762900935, 'random_strength': 1.0463313961830132}. Best is trial 42 with value: 61323.22388172411.\n",
      "[I 2025-07-24 23:22:17,205] Trial 60 finished with value: 61547.390165585 and parameters: {'iterations': 1421, 'learning_rate': 0.03623296850913379, 'depth': 6, 'l2_leaf_reg': 8.92769107700746, 'subsample': 0.7661901519660335, 'random_strength': 2.953934541777939}. Best is trial 42 with value: 61323.22388172411.\n",
      "[I 2025-07-24 23:22:30,049] Trial 61 finished with value: 61447.87252262587 and parameters: {'iterations': 2008, 'learning_rate': 0.03331751735010217, 'depth': 6, 'l2_leaf_reg': 7.673681248851276, 'subsample': 0.6410867985307369, 'random_strength': 1.3356277554309863}. Best is trial 42 with value: 61323.22388172411.\n",
      "[I 2025-07-24 23:22:38,729] Trial 62 finished with value: 61586.191894403644 and parameters: {'iterations': 1947, 'learning_rate': 0.03077735954973574, 'depth': 6, 'l2_leaf_reg': 9.784800410097303, 'subsample': 0.6330369978786763, 'random_strength': 1.2059551754212792}. Best is trial 42 with value: 61323.22388172411.\n",
      "[I 2025-07-24 23:22:48,816] Trial 63 finished with value: 61461.65270998584 and parameters: {'iterations': 1791, 'learning_rate': 0.038347919082514725, 'depth': 6, 'l2_leaf_reg': 6.9380568760769705, 'subsample': 0.6627978341648295, 'random_strength': 1.4274122439750958}. Best is trial 42 with value: 61323.22388172411.\n",
      "[I 2025-07-24 23:22:59,353] Trial 64 finished with value: 61498.18388789071 and parameters: {'iterations': 1655, 'learning_rate': 0.0345878570111586, 'depth': 6, 'l2_leaf_reg': 4.750220165626432, 'subsample': 0.6383956516716627, 'random_strength': 1.6109240846914061}. Best is trial 42 with value: 61323.22388172411.\n",
      "[I 2025-07-24 23:23:17,228] Trial 65 finished with value: 61634.22736307241 and parameters: {'iterations': 2192, 'learning_rate': 0.014353650495494629, 'depth': 6, 'l2_leaf_reg': 11.427356130340462, 'subsample': 0.7946958208024767, 'random_strength': 1.1137537152832395}. Best is trial 42 with value: 61323.22388172411.\n",
      "[I 2025-07-24 23:23:29,189] Trial 66 finished with value: 61576.52281957165 and parameters: {'iterations': 1870, 'learning_rate': 0.04122286440373574, 'depth': 6, 'l2_leaf_reg': 40.3003575588606, 'subsample': 0.6472122808273558, 'random_strength': 1.284428522627171}. Best is trial 42 with value: 61323.22388172411.\n",
      "[I 2025-07-24 23:23:40,009] Trial 67 finished with value: 61938.25997253559 and parameters: {'iterations': 1991, 'learning_rate': 0.033186194294994725, 'depth': 3, 'l2_leaf_reg': 3.7162692354192775, 'subsample': 0.8146314364570403, 'random_strength': 1.4503195876611652}. Best is trial 42 with value: 61323.22388172411.\n",
      "[I 2025-07-24 23:23:51,548] Trial 68 finished with value: 61449.15119398339 and parameters: {'iterations': 2686, 'learning_rate': 0.038574837889033195, 'depth': 6, 'l2_leaf_reg': 13.066224622192598, 'subsample': 0.6097314591718817, 'random_strength': 1.1686693321053465}. Best is trial 42 with value: 61323.22388172411.\n",
      "[I 2025-07-24 23:23:59,037] Trial 69 finished with value: 61807.33483091137 and parameters: {'iterations': 2572, 'learning_rate': 0.028936383999445217, 'depth': 4, 'l2_leaf_reg': 5.459345760733341, 'subsample': 0.7399688536536044, 'random_strength': 1.001803832030777}. Best is trial 42 with value: 61323.22388172411.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "CatBoost Error model tuning complete. Best validation RMSE: $61,323.22\n",
      "\n",
      "--- K-Fold training the final CatBoost error model... ---\n",
      "  Training CatBoost error model for fold 1/5...\n",
      "  Training CatBoost error model for fold 2/5...\n",
      "  Training CatBoost error model for fold 3/5...\n",
      "  Training CatBoost error model for fold 4/5...\n",
      "  Training CatBoost error model for fold 5/5...\n",
      "\n",
      "Final CatBoost Error Model OOF RMSE: $60,550.52\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# BLOCK 5 (CATBOOST): TUNE AND TRAIN THE CATBOOST ERROR MODEL\n",
    "# =============================================================================\n",
    "\n",
    "# --- 1. Prepare Features and Target for the Error Model ---\n",
    "print(\"\\n--- Preparing features for the CatBoost error model ---\")\n",
    "# The error target is the same for all error models\n",
    "# The features are also the same (full feature set + OOF mean preds)\n",
    "\n",
    "# --- 2. Tune the Error Model with Optuna ---\n",
    "print(\"\\n--- Tuning the CatBoost Error Model with Optuna... ---\")\n",
    "# We can use the same train/validation split for tuning efficiency\n",
    "# X_train_err_opt, X_val_err_opt, y_train_err_opt, y_val_err_opt are already defined from the XGBoost block\n",
    "\n",
    "def objective_error_model_cb(trial):\n",
    "    params = {\n",
    "        'loss_function': 'RMSE',\n",
    "        'eval_metric': 'RMSE',\n",
    "        'random_seed': RANDOM_STATE,\n",
    "        'verbose': 0,\n",
    "        'iterations': trial.suggest_int('iterations', 1000, 3000),\n",
    "        'learning_rate': trial.suggest_float('learning_rate', 0.01, 0.05, log=True),\n",
    "        'depth': trial.suggest_int('depth', 3, 6),\n",
    "        'l2_leaf_reg': trial.suggest_float('l2_leaf_reg', 1.0, 50.0, log=True),\n",
    "        'subsample': trial.suggest_float('subsample', 0.6, 0.9),\n",
    "        'random_strength': trial.suggest_float('random_strength', 1.0, 10.0, log=True)\n",
    "    }\n",
    "    \n",
    "    model = cb.CatBoostRegressor(**params)\n",
    "    model.fit(\n",
    "        X_train_err_opt, y_train_err_opt,\n",
    "        eval_set=[(X_val_err_opt, y_val_err_opt)],\n",
    "        early_stopping_rounds=75,\n",
    "        use_best_model=True\n",
    "    )\n",
    "    \n",
    "    preds = model.predict(X_val_err_opt)\n",
    "    rmse = np.sqrt(mean_squared_error(y_val_err_opt, preds))\n",
    "    return rmse\n",
    "\n",
    "study_error_cb = optuna.create_study(direction='minimize')\n",
    "study_error_cb.optimize(objective_error_model_cb, n_trials=70)\n",
    "best_params_error_cb = study_error_cb.best_params\n",
    "print(f\"\\nCatBoost Error model tuning complete. Best validation RMSE: ${study_error_cb.best_value:,.2f}\")\n",
    "\n",
    "# --- 3. Final K-Fold Training of the Error Model ---\n",
    "print(\"\\n--- K-Fold training the final CatBoost error model... ---\")\n",
    "oof_error_preds_cb = np.zeros(len(X))\n",
    "test_error_preds_cb = np.zeros(len(X_test))\n",
    "skf_cb = StratifiedKFold(n_splits=N_SPLITS, shuffle=True, random_state=RANDOM_STATE)\n",
    "\n",
    "for fold, (train_idx, val_idx) in enumerate(skf_cb.split(X_for_error, grade_for_stratify)):\n",
    "    print(f\"  Training CatBoost error model for fold {fold+1}/{N_SPLITS}...\")\n",
    "    X_train_err, X_val_err = X_for_error.iloc[train_idx], X_for_error.iloc[val_idx]\n",
    "    y_train_err, y_val_err = error_target.iloc[train_idx], error_target.iloc[val_idx]\n",
    "\n",
    "    model_err_cb = cb.CatBoostRegressor(**best_params_error_cb, early_stopping_rounds=75, verbose=0)\n",
    "    model_err_cb.fit(X_train_err, y_train_err, eval_set=[(X_val_err, y_val_err)])\n",
    "    \n",
    "    oof_error_preds_cb[val_idx] = np.clip(model_err_cb.predict(X_val_err), 0, None)\n",
    "    test_error_preds_cb += np.clip(model_err_cb.predict(X_test_for_error), 0, None) / N_SPLITS\n",
    "\n",
    "print(f\"\\nFinal CatBoost Error Model OOF RMSE: ${np.sqrt(mean_squared_error(error_target, oof_error_preds_cb)):,.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8c945b98-9ec2-4066-afb2-2eee49625898",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-07-24 17:14:55,327] A new study created in memory with name: no-name-ba92639c-e5cd-4056-9d88-270b2c0882ee\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Preparing features for the LightGBM error model ---\n",
      "\n",
      "--- Tuning the LightGBM Error Model with Optuna... ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-07-24 17:15:12,321] Trial 0 finished with value: 63094.69733961664 and parameters: {'learning_rate': 0.034793142258152235, 'n_estimators': 2339, 'num_leaves': 37, 'max_depth': 5, 'lambda_l1': 2.4131530218075854, 'lambda_l2': 7.969999565684957, 'feature_fraction': 0.7255296573441212, 'bagging_fraction': 0.8051794666918468, 'bagging_freq': 2}. Best is trial 0 with value: 63094.69733961664.\n",
      "[I 2025-07-24 17:15:32,145] Trial 1 finished with value: 63243.20911394577 and parameters: {'learning_rate': 0.030072630780943625, 'n_estimators': 2389, 'num_leaves': 31, 'max_depth': 6, 'lambda_l1': 14.575271598553988, 'lambda_l2': 8.70101780320695, 'feature_fraction': 0.7608235405531482, 'bagging_fraction': 0.7987340416954959, 'bagging_freq': 4}. Best is trial 0 with value: 63094.69733961664.\n",
      "[I 2025-07-24 17:15:52,071] Trial 2 finished with value: 63157.0332211606 and parameters: {'learning_rate': 0.025100431663170962, 'n_estimators': 2711, 'num_leaves': 35, 'max_depth': 5, 'lambda_l1': 5.1602327333263505, 'lambda_l2': 4.4744240200803285, 'feature_fraction': 0.6355570100055112, 'bagging_fraction': 0.7143138442682604, 'bagging_freq': 3}. Best is trial 0 with value: 63094.69733961664.\n",
      "[I 2025-07-24 17:15:58,535] Trial 3 finished with value: 63468.36951338092 and parameters: {'learning_rate': 0.04008250502533983, 'n_estimators': 1908, 'num_leaves': 19, 'max_depth': 5, 'lambda_l1': 1.641247090307387, 'lambda_l2': 9.082389709179097, 'feature_fraction': 0.6206160371026568, 'bagging_fraction': 0.6197383123495145, 'bagging_freq': 5}. Best is trial 0 with value: 63094.69733961664.\n",
      "[I 2025-07-24 17:16:11,269] Trial 4 finished with value: 63438.3800311932 and parameters: {'learning_rate': 0.029518737064993687, 'n_estimators': 1574, 'num_leaves': 36, 'max_depth': 6, 'lambda_l1': 14.274038697502057, 'lambda_l2': 2.0570347177507795, 'feature_fraction': 0.6322642074771909, 'bagging_fraction': 0.6430004070059273, 'bagging_freq': 5}. Best is trial 0 with value: 63094.69733961664.\n",
      "[I 2025-07-24 17:16:20,655] Trial 5 finished with value: 63902.75545295489 and parameters: {'learning_rate': 0.01844772259731284, 'n_estimators': 1292, 'num_leaves': 16, 'max_depth': 5, 'lambda_l1': 1.5237485901696142, 'lambda_l2': 3.4557211875803238, 'feature_fraction': 0.7701281878359707, 'bagging_fraction': 0.8817627039461247, 'bagging_freq': 3}. Best is trial 0 with value: 63094.69733961664.\n",
      "[I 2025-07-24 17:16:34,649] Trial 6 finished with value: 63282.85240242327 and parameters: {'learning_rate': 0.0366056530883761, 'n_estimators': 2397, 'num_leaves': 22, 'max_depth': 4, 'lambda_l1': 4.613853180455433, 'lambda_l2': 3.3598850120026422, 'feature_fraction': 0.7845688086177285, 'bagging_fraction': 0.8694634114863122, 'bagging_freq': 7}. Best is trial 0 with value: 63094.69733961664.\n",
      "[I 2025-07-24 17:16:49,891] Trial 7 finished with value: 63818.51257542186 and parameters: {'learning_rate': 0.010264288743509879, 'n_estimators': 1883, 'num_leaves': 34, 'max_depth': 6, 'lambda_l1': 18.524551066533952, 'lambda_l2': 23.1843167260933, 'feature_fraction': 0.6919953560296707, 'bagging_fraction': 0.804999739140085, 'bagging_freq': 1}. Best is trial 0 with value: 63094.69733961664.\n",
      "[I 2025-07-24 17:17:02,275] Trial 8 finished with value: 63673.44480467395 and parameters: {'learning_rate': 0.03912649263353702, 'n_estimators': 1874, 'num_leaves': 14, 'max_depth': 5, 'lambda_l1': 35.05207653892775, 'lambda_l2': 1.542828633828302, 'feature_fraction': 0.8895985083585273, 'bagging_fraction': 0.8298792356144324, 'bagging_freq': 4}. Best is trial 0 with value: 63094.69733961664.\n",
      "[I 2025-07-24 17:17:10,571] Trial 9 finished with value: 63572.81338688303 and parameters: {'learning_rate': 0.021218641637461615, 'n_estimators': 1462, 'num_leaves': 25, 'max_depth': 5, 'lambda_l1': 4.671064702597409, 'lambda_l2': 2.1628211039091947, 'feature_fraction': 0.6078893622921143, 'bagging_fraction': 0.6383266202301608, 'bagging_freq': 4}. Best is trial 0 with value: 63094.69733961664.\n",
      "[I 2025-07-24 17:17:26,782] Trial 10 finished with value: 63920.38301662622 and parameters: {'learning_rate': 0.016771776906458247, 'n_estimators': 2911, 'num_leaves': 40, 'max_depth': 3, 'lambda_l1': 1.0720795024682261, 'lambda_l2': 44.75844750232675, 'feature_fraction': 0.8485964597760469, 'bagging_fraction': 0.7254357486424421, 'bagging_freq': 1}. Best is trial 0 with value: 63094.69733961664.\n",
      "[I 2025-07-24 17:17:34,554] Trial 11 finished with value: 63421.3892335355 and parameters: {'learning_rate': 0.0491471830465828, 'n_estimators': 2825, 'num_leaves': 30, 'max_depth': 4, 'lambda_l1': 3.5544781778266787, 'lambda_l2': 5.7731953804450304, 'feature_fraction': 0.6956218356686343, 'bagging_fraction': 0.7141487888234115, 'bagging_freq': 2}. Best is trial 0 with value: 63094.69733961664.\n",
      "[I 2025-07-24 17:17:49,357] Trial 12 finished with value: 63392.21875043384 and parameters: {'learning_rate': 0.026566001910164082, 'n_estimators': 2488, 'num_leaves': 40, 'max_depth': 4, 'lambda_l1': 2.5436693365116962, 'lambda_l2': 14.877890774639255, 'feature_fraction': 0.693058449561369, 'bagging_fraction': 0.7518211508500713, 'bagging_freq': 2}. Best is trial 0 with value: 63094.69733961664.\n",
      "[I 2025-07-24 17:18:04,602] Trial 13 finished with value: 64117.333612491275 and parameters: {'learning_rate': 0.012548213094047564, 'n_estimators': 2645, 'num_leaves': 29, 'max_depth': 3, 'lambda_l1': 6.578209206708469, 'lambda_l2': 4.7134510992118805, 'feature_fraction': 0.6618528884597717, 'bagging_fraction': 0.6950962298155957, 'bagging_freq': 2}. Best is trial 0 with value: 63094.69733961664.\n",
      "[I 2025-07-24 17:18:19,518] Trial 14 finished with value: 63309.56232048446 and parameters: {'learning_rate': 0.025078090691969753, 'n_estimators': 2152, 'num_leaves': 35, 'max_depth': 5, 'lambda_l1': 9.325931612917211, 'lambda_l2': 1.0807714856861264, 'feature_fraction': 0.7347255258447033, 'bagging_fraction': 0.773424922064964, 'bagging_freq': 3}. Best is trial 0 with value: 63094.69733961664.\n",
      "[I 2025-07-24 17:18:32,929] Trial 15 finished with value: 63279.445158517956 and parameters: {'learning_rate': 0.033917491032570114, 'n_estimators': 2175, 'num_leaves': 26, 'max_depth': 4, 'lambda_l1': 2.3964486640357685, 'lambda_l2': 14.403539399984645, 'feature_fraction': 0.819887316750445, 'bagging_fraction': 0.6774987632247064, 'bagging_freq': 3}. Best is trial 0 with value: 63094.69733961664.\n",
      "[I 2025-07-24 17:18:46,995] Trial 16 finished with value: 63008.046216028415 and parameters: {'learning_rate': 0.04967623217499028, 'n_estimators': 2653, 'num_leaves': 36, 'max_depth': 6, 'lambda_l1': 7.972816399005995, 'lambda_l2': 12.933271498661698, 'feature_fraction': 0.7214563879929922, 'bagging_fraction': 0.8441199586595552, 'bagging_freq': 7}. Best is trial 16 with value: 63008.046216028415.\n",
      "[I 2025-07-24 17:19:02,413] Trial 17 finished with value: 63053.40026415354 and parameters: {'learning_rate': 0.04982670203493395, 'n_estimators': 2982, 'num_leaves': 39, 'max_depth': 6, 'lambda_l1': 8.223939654058768, 'lambda_l2': 28.142058251834772, 'feature_fraction': 0.7297456778269419, 'bagging_fraction': 0.8448014014734647, 'bagging_freq': 7}. Best is trial 16 with value: 63008.046216028415.\n",
      "[I 2025-07-24 17:19:17,260] Trial 18 finished with value: 63392.98374635538 and parameters: {'learning_rate': 0.049446865759079986, 'n_estimators': 2963, 'num_leaves': 10, 'max_depth': 6, 'lambda_l1': 8.58905291275253, 'lambda_l2': 47.06782639868541, 'feature_fraction': 0.7226715283483399, 'bagging_fraction': 0.8523950385975548, 'bagging_freq': 7}. Best is trial 16 with value: 63008.046216028415.\n",
      "[I 2025-07-24 17:19:33,952] Trial 19 finished with value: 63485.35944484732 and parameters: {'learning_rate': 0.04414641200971131, 'n_estimators': 2635, 'num_leaves': 32, 'max_depth': 6, 'lambda_l1': 34.44228031938613, 'lambda_l2': 27.83509265561365, 'feature_fraction': 0.8024887521855901, 'bagging_fraction': 0.8893989657636784, 'bagging_freq': 6}. Best is trial 16 with value: 63008.046216028415.\n",
      "[I 2025-07-24 17:20:25,960] Trial 20 finished with value: 63152.612215969544 and parameters: {'learning_rate': 0.04204041582856716, 'n_estimators': 2997, 'num_leaves': 38, 'max_depth': 6, 'lambda_l1': 20.750862272820676, 'lambda_l2': 15.304791619072496, 'feature_fraction': 0.6634779373173963, 'bagging_fraction': 0.8421465078938832, 'bagging_freq': 6}. Best is trial 16 with value: 63008.046216028415.\n",
      "[I 2025-07-24 17:20:44,565] Trial 21 finished with value: 62960.3741566168 and parameters: {'learning_rate': 0.04688545056677072, 'n_estimators': 2213, 'num_leaves': 38, 'max_depth': 6, 'lambda_l1': 7.706078894719996, 'lambda_l2': 24.023153706694167, 'feature_fraction': 0.7202704523903461, 'bagging_fraction': 0.813016732503191, 'bagging_freq': 6}. Best is trial 21 with value: 62960.3741566168.\n",
      "[I 2025-07-24 17:21:02,633] Trial 22 finished with value: 63044.36158052028 and parameters: {'learning_rate': 0.04946280758701938, 'n_estimators': 2744, 'num_leaves': 40, 'max_depth': 6, 'lambda_l1': 11.233945280445615, 'lambda_l2': 25.785302758337224, 'feature_fraction': 0.7508434860850935, 'bagging_fraction': 0.8210559776523055, 'bagging_freq': 6}. Best is trial 21 with value: 62960.3741566168.\n",
      "[I 2025-07-24 17:21:19,672] Trial 23 finished with value: 63204.15444371771 and parameters: {'learning_rate': 0.04395749324562813, 'n_estimators': 2139, 'num_leaves': 33, 'max_depth': 6, 'lambda_l1': 12.891144234671044, 'lambda_l2': 20.53046495902671, 'feature_fraction': 0.7506112144514282, 'bagging_fraction': 0.7741647429798132, 'bagging_freq': 6}. Best is trial 21 with value: 62960.3741566168.\n",
      "[I 2025-07-24 17:21:42,089] Trial 24 finished with value: 63329.65778091321 and parameters: {'learning_rate': 0.032651822230178346, 'n_estimators': 2547, 'num_leaves': 37, 'max_depth': 6, 'lambda_l1': 25.453698508828214, 'lambda_l2': 11.50042943782441, 'feature_fraction': 0.7050232017091298, 'bagging_fraction': 0.824250577816632, 'bagging_freq': 5}. Best is trial 21 with value: 62960.3741566168.\n",
      "[I 2025-07-24 17:21:53,647] Trial 25 finished with value: 63357.47812398482 and parameters: {'learning_rate': 0.04432728205181521, 'n_estimators': 2763, 'num_leaves': 28, 'max_depth': 6, 'lambda_l1': 11.86889531765298, 'lambda_l2': 34.186436480802165, 'feature_fraction': 0.6681262601325012, 'bagging_fraction': 0.7789697590920241, 'bagging_freq': 6}. Best is trial 21 with value: 62960.3741566168.\n",
      "[I 2025-07-24 17:22:12,675] Trial 26 finished with value: 63055.4194881442 and parameters: {'learning_rate': 0.03814331007330846, 'n_estimators': 2279, 'num_leaves': 40, 'max_depth': 6, 'lambda_l1': 6.25034235314152, 'lambda_l2': 19.89809375576607, 'feature_fraction': 0.7905662559154606, 'bagging_fraction': 0.8613789464710984, 'bagging_freq': 7}. Best is trial 21 with value: 62960.3741566168.\n",
      "[I 2025-07-24 17:22:28,954] Trial 27 finished with value: 63223.484558098 and parameters: {'learning_rate': 0.04964589306190387, 'n_estimators': 2548, 'num_leaves': 33, 'max_depth': 5, 'lambda_l1': 10.896577647766732, 'lambda_l2': 36.51063628218933, 'feature_fraction': 0.8315090563654779, 'bagging_fraction': 0.8237053572619696, 'bagging_freq': 6}. Best is trial 21 with value: 62960.3741566168.\n",
      "[I 2025-07-24 17:22:45,402] Trial 28 finished with value: 63865.17486135632 and parameters: {'learning_rate': 0.03032447038256551, 'n_estimators': 1745, 'num_leaves': 37, 'max_depth': 6, 'lambda_l1': 47.82064162567042, 'lambda_l2': 11.166334292874962, 'feature_fraction': 0.7495132886190248, 'bagging_fraction': 0.7929116089826378, 'bagging_freq': 5}. Best is trial 21 with value: 62960.3741566168.\n",
      "[I 2025-07-24 17:23:30,398] Trial 29 finished with value: 63428.39676362205 and parameters: {'learning_rate': 0.03565240364253416, 'n_estimators': 1056, 'num_leaves': 37, 'max_depth': 5, 'lambda_l1': 3.1963294978129815, 'lambda_l2': 21.900361787277518, 'feature_fraction': 0.7054623619704049, 'bagging_fraction': 0.8992325761984531, 'bagging_freq': 7}. Best is trial 21 with value: 62960.3741566168.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "LightGBM Error model tuning complete. Best validation RMSE: $62,960.37\n",
      "\n",
      "--- K-Fold training the final LightGBM error model... ---\n",
      "  Training LightGBM error model for fold 1/5...\n",
      "Training until validation scores don't improve for 75 rounds\n",
      "  Training LightGBM error model for fold 2/5...\n",
      "Training until validation scores don't improve for 75 rounds\n",
      "Early stopping, best iteration is:\n",
      "[471]\tvalid_0's l2: 3.61934e+09\n",
      "  Training LightGBM error model for fold 3/5...\n",
      "Training until validation scores don't improve for 75 rounds\n",
      "  Training LightGBM error model for fold 4/5...\n",
      "Training until validation scores don't improve for 75 rounds\n",
      "  Training LightGBM error model for fold 5/5...\n",
      "Training until validation scores don't improve for 75 rounds\n",
      "Early stopping, best iteration is:\n",
      "[651]\tvalid_0's l2: 3.6831e+09\n",
      "\n",
      "Final LightGBM Error Model OOF RMSE: $60,583.05\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# BLOCK 5.5 (LIGHTGBM): TUNE AND TRAIN THE LIGHTGBM ERROR MODEL\n",
    "# =============================================================================\n",
    "import lightgbm as lgb\n",
    "\n",
    "# --- 1. Prepare Features and Target for the Error Model ---\n",
    "print(\"\\n--- Preparing features for the LightGBM error model ---\")\n",
    "# The error target and features are the same as the other error models.\n",
    "\n",
    "# --- 2. Tune the Error Model with Optuna ---\n",
    "print(\"\\n--- Tuning the LightGBM Error Model with Optuna... ---\")\n",
    "# We use the same train/validation split for tuning efficiency.\n",
    "# X_train_err_opt, X_val_err_opt, y_train_err_opt, y_val_err_opt are already defined.\n",
    "\n",
    "def objective_error_model_lgbm(trial):\n",
    "    params = {\n",
    "        'objective': 'regression_l1', # MAE is robust for error modeling\n",
    "        'metric': 'rmse',\n",
    "        'random_state': RANDOM_STATE,\n",
    "        'n_jobs': -1,\n",
    "        'boosting_type': 'gbdt',\n",
    "        'verbosity': -1, # Suppress warnings\n",
    "        'learning_rate': trial.suggest_float('learning_rate', 0.01, 0.05, log=True),\n",
    "        'n_estimators': trial.suggest_int('n_estimators', 1000, 3000),\n",
    "        'num_leaves': trial.suggest_int('num_leaves', 10, 40), # Shallower for error model\n",
    "        'max_depth': trial.suggest_int('max_depth', 3, 6),\n",
    "        'lambda_l1': trial.suggest_float('lambda_l1', 1.0, 50.0, log=True),\n",
    "        'lambda_l2': trial.suggest_float('lambda_l2', 1.0, 50.0, log=True),\n",
    "        'feature_fraction': trial.suggest_float('feature_fraction', 0.6, 0.9),\n",
    "        'bagging_fraction': trial.suggest_float('bagging_fraction', 0.6, 0.9),\n",
    "        'bagging_freq': trial.suggest_int('bagging_freq', 1, 7)\n",
    "    }\n",
    "    \n",
    "    model = lgb.LGBMRegressor(**params)\n",
    "    model.fit(\n",
    "        X_train_err_opt, y_train_err_opt,\n",
    "        eval_set=[(X_val_err_opt, y_val_err_opt)],\n",
    "        callbacks=[lgb.early_stopping(75, verbose=False)]\n",
    "    )\n",
    "    \n",
    "    preds = model.predict(X_val_err_opt)\n",
    "    rmse = np.sqrt(mean_squared_error(y_val_err_opt, preds))\n",
    "    return rmse\n",
    "\n",
    "study_error_lgbm = optuna.create_study(direction='minimize')\n",
    "study_error_lgbm.optimize(objective_error_model_lgbm, n_trials=30) # 30 trials is a good balance\n",
    "best_params_error_lgbm = study_error_lgbm.best_params\n",
    "print(f\"\\nLightGBM Error model tuning complete. Best validation RMSE: ${study_error_lgbm.best_value:,.2f}\")\n",
    "\n",
    "# --- 3. Final K-Fold Training of the Error Model ---\n",
    "print(\"\\n--- K-Fold training the final LightGBM error model... ---\")\n",
    "oof_error_preds_lgbm = np.zeros(len(X))\n",
    "test_error_preds_lgbm = np.zeros(len(X_test))\n",
    "skf_lgbm = StratifiedKFold(n_splits=N_SPLITS, shuffle=True, random_state=RANDOM_STATE)\n",
    "\n",
    "for fold, (train_idx, val_idx) in enumerate(skf_lgbm.split(X_for_error, grade_for_stratify)):\n",
    "    print(f\"  Training LightGBM error model for fold {fold+1}/{N_SPLITS}...\")\n",
    "    X_train_err, X_val_err = X_for_error.iloc[train_idx], X_for_error.iloc[val_idx]\n",
    "    y_train_err, y_val_err = error_target.iloc[train_idx], error_target.iloc[val_idx]\n",
    "\n",
    "    model_err_lgbm = lgb.LGBMRegressor(**best_params_error_lgbm, early_stopping_rounds=75)\n",
    "    model_err_lgbm.fit(X_train_err, y_train_err,\n",
    "                       eval_set=[(X_val_err, y_val_err)],\n",
    "                       callbacks=[lgb.early_stopping(75, verbose=False)])\n",
    "    \n",
    "    oof_error_preds_lgbm[val_idx] = np.clip(model_err_lgbm.predict(X_val_err), 0, None)\n",
    "    test_error_preds_lgbm += np.clip(model_err_lgbm.predict(X_test_for_error), 0, None) / N_SPLITS\n",
    "\n",
    "print(f\"\\nFinal LightGBM Error Model OOF RMSE: ${np.sqrt(mean_squared_error(error_target, oof_error_preds_lgbm)):,.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9909782e-7944-4c9f-a684-e41bc5ae01ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Saving all error model prediction arrays to './error_models/' ---\n",
      "XGBoost error predictions saved successfully.\n",
      "CatBoost error predictions saved successfully.\n",
      "LightGBM error predictions saved successfully.\n",
      "\n",
      "All necessary prediction files are now ready for the final blending notebook.\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# BLOCK 7: SAVE ALL SPECIALIST ERROR MODEL PREDICTIONS\n",
    "# =============================================================================\n",
    "import os\n",
    "import numpy as np\n",
    "\n",
    "# --- Define a dedicated path for error model predictions ---\n",
    "ERROR_PREDS_PATH = './error_models/' \n",
    "os.makedirs(ERROR_PREDS_PATH, exist_ok=True) # Create the directory if it doesn't exist\n",
    "\n",
    "print(f\"--- Saving all error model prediction arrays to '{ERROR_PREDS_PATH}' ---\")\n",
    "\n",
    "# --- Save XGBoost Error Model Predictions ---\n",
    "# This assumes your variables are named 'oof_error_preds' and 'test_error_preds'\n",
    "try:\n",
    "    np.save(os.path.join(ERROR_PREDS_PATH, 'oof_error_preds_xgb.npy'), oof_error_preds)\n",
    "    np.save(os.path.join(ERROR_PREDS_PATH, 'test_error_preds_xgb.npy'), test_error_preds)\n",
    "    print(\"XGBoost error predictions saved successfully.\")\n",
    "except NameError:\n",
    "    print(\"ERROR: Could not find XGBoost error prediction variables ('oof_error_preds', 'test_error_preds'). Please ensure the XGBoost error model block was run.\")\n",
    "\n",
    "# --- Save CatBoost Error Model Predictions ---\n",
    "# This assumes your variables are named 'oof_error_preds_cb' and 'test_error_preds_cb'\n",
    "try:\n",
    "    np.save(os.path.join(ERROR_PREDS_PATH, 'oof_error_preds_cb.npy'), oof_error_preds_cb)\n",
    "    np.save(os.path.join(ERROR_PREDS_PATH, 'test_error_preds_cb.npy'), test_error_preds_cb)\n",
    "    print(\"CatBoost error predictions saved successfully.\")\n",
    "except NameError:\n",
    "    print(\"ERROR: Could not find CatBoost error prediction variables ('oof_error_preds_cb', 'test_error_preds_cb'). Please ensure the CatBoost error model block was run.\")\n",
    "\n",
    "try:\n",
    "    np.save(os.path.join(ERROR_PREDS_PATH, 'oof_error_preds_lgbm.npy'), oof_error_preds)\n",
    "    np.save(os.path.join(ERROR_PREDS_PATH, 'test_error_preds_lgbm.npy'), test_error_preds)\n",
    "    print(\"LightGBM error predictions saved successfully.\")\n",
    "except NameError:\n",
    "    print(\"ERROR: Could not find LightGBM error prediction variables ('oof_error_preds', 'test_error_preds'). Please ensure the LightGBM error model block was run.\")\n",
    "\n",
    "\n",
    "print(\"\\nAll necessary prediction files are now ready for the final blending notebook.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a1bcde59-f972-41b4-bae5-cfa1bc414045",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Searching for the optimal blend weight for the two error models... ---\n",
      "  Testing Blend (XGB/CB): 0.00/1.00  |  Score: $294,441.63 (a=1.977, b=2.172)\n",
      "  Testing Blend (XGB/CB): 0.05/0.95  |  Score: $294,103.50 (a=1.977, b=2.173)\n",
      "  Testing Blend (XGB/CB): 0.10/0.90  |  Score: $293,791.04 (a=1.977, b=2.175)\n",
      "  Testing Blend (XGB/CB): 0.15/0.85  |  Score: $293,510.02 (a=1.976, b=2.174)\n",
      "  Testing Blend (XGB/CB): 0.20/0.80  |  Score: $293,253.40 (a=1.978, b=2.175)\n",
      "  Testing Blend (XGB/CB): 0.25/0.75  |  Score: $293,025.20 (a=1.978, b=2.175)\n",
      "  Testing Blend (XGB/CB): 0.30/0.70  |  Score: $292,824.74 (a=1.977, b=2.175)\n",
      "  Testing Blend (XGB/CB): 0.35/0.65  |  Score: $292,649.75 (a=1.979, b=2.177)\n",
      "  Testing Blend (XGB/CB): 0.40/0.60  |  Score: $292,503.35 (a=1.978, b=2.178)\n",
      "  Testing Blend (XGB/CB): 0.45/0.55  |  Score: $292,390.86 (a=1.979, b=2.176)\n",
      "  Testing Blend (XGB/CB): 0.50/0.50  |  Score: $292,307.99 (a=1.979, b=2.176)\n",
      "  Testing Blend (XGB/CB): 0.55/0.45  |  Score: $292,251.11 (a=1.980, b=2.174)\n",
      "  Testing Blend (XGB/CB): 0.60/0.40  |  Score: $292,220.46 (a=1.980, b=2.175)\n",
      "  Testing Blend (XGB/CB): 0.65/0.35  |  Score: $292,223.32 (a=1.979, b=2.176)\n",
      "  Testing Blend (XGB/CB): 0.70/0.30  |  Score: $292,258.42 (a=1.979, b=2.177)\n",
      "  Testing Blend (XGB/CB): 0.75/0.25  |  Score: $292,325.63 (a=1.978, b=2.177)\n",
      "  Testing Blend (XGB/CB): 0.80/0.20  |  Score: $292,425.47 (a=1.979, b=2.174)\n",
      "  Testing Blend (XGB/CB): 0.85/0.15  |  Score: $292,553.36 (a=1.978, b=2.175)\n",
      "  Testing Blend (XGB/CB): 0.90/0.10  |  Score: $292,713.45 (a=1.978, b=2.172)\n",
      "  Testing Blend (XGB/CB): 0.95/0.05  |  Score: $292,907.00 (a=1.978, b=2.174)\n",
      "  Testing Blend (XGB/CB): 1.00/0.00  |  Score: $293,131.26 (a=1.980, b=2.175)\n",
      "\n",
      "--- Building the final error ensemble with the optimal blend... ---\n",
      "\n",
      "============================================================\n",
      "FINAL RESULTS & SUBMISSION\n",
      "============================================================\n",
      "Final OOF Winkler Score (Optimally Blended Ensemble of Ensembles): $292,220.46\n",
      "Optimal Error Blend (XGB/CB): 0.60 / 0.40\n",
      "Optimal Multipliers: a=1.9799, b=2.1755\n"
     ]
    }
   ],
   "source": [
    "# =======================================================================================================\n",
    "# FINAL BLOCK (DEFINITIVE): OPTIMAL ERROR BLEND, PRECISE CALIBRATION, AND SUBMISSION\n",
    "# =======================================================================================================\n",
    "from scipy.optimize import minimize\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "# --- 1. Search for the Optimal Blend of Error Models ---\n",
    "print(\"\\n--- Searching for the optimal blend weight for the two error models... ---\")\n",
    "\n",
    "best_blend_score = float('inf')\n",
    "best_blend_weight = 0.5 # Default to 50/50\n",
    "best_blend_a = 1.0\n",
    "best_blend_b = 1.0\n",
    "\n",
    "# We will test weights for the XGBoost error model from 0% to 100% in 5% increments\n",
    "for xgb_weight in np.arange(0, 1.05, 0.05):\n",
    "    cb_weight = 1.0 - xgb_weight\n",
    "    \n",
    "    # Create the blended error predictions for this specific weight\n",
    "    oof_error_blend = (oof_error_preds * xgb_weight) + (oof_error_preds_cb * cb_weight)\n",
    "    oof_error_final_ensemble = np.clip(oof_error_blend, 0, None)\n",
    "\n",
    "    # --- Calibrate THIS specific blend ---\n",
    "    def get_winkler_from_multipliers(multipliers):\n",
    "        a, b = multipliers[0], multipliers[1]\n",
    "        low = oof_ensemble_mean - oof_error_final_ensemble * a\n",
    "        high = oof_ensemble_mean + oof_error_final_ensemble * b\n",
    "        return winkler_score(y_true, low, high)\n",
    "\n",
    "    initial_guess = [1.9, 2.2]\n",
    "    bounds = [(1.0, 3.0), (1.0, 3.0)]\n",
    "    result_calib = minimize(get_winkler_from_multipliers, initial_guess, method='L-BFGS-B', bounds=bounds)\n",
    "    \n",
    "    current_score = result_calib.fun\n",
    "    current_a, current_b = result_calib.x\n",
    "    \n",
    "    print(f\"  Testing Blend (XGB/CB): {xgb_weight:.2f}/{cb_weight:.2f}  |  Score: ${current_score:,.2f} (a={current_a:.3f}, b={current_b:.3f})\")\n",
    "\n",
    "    # If this blend is the best so far, save its parameters\n",
    "    if current_score < best_blend_score:\n",
    "        best_blend_score = current_score\n",
    "        best_blend_weight = xgb_weight\n",
    "        best_blend_a = current_a\n",
    "        best_blend_b = current_b\n",
    "\n",
    "# --- 2. Build the Final Error Ensemble with the Best Weight ---\n",
    "print(\"\\n--- Building the final error ensemble with the optimal blend... ---\")\n",
    "final_xgb_weight = best_blend_weight\n",
    "final_cb_weight = 1.0 - final_xgb_weight\n",
    "test_error_final_ensemble = (test_error_preds * final_xgb_weight) + (test_error_preds_cb * final_cb_weight)\n",
    "\n",
    "# --- 3. Final Results and Submission ---\n",
    "print(\"\\n\" + \"=\"*60); print(\"FINAL RESULTS & SUBMISSION\"); print(\"=\"*60)\n",
    "print(f\"Final OOF Winkler Score (Optimally Blended Ensemble of Ensembles): ${best_blend_score:,.2f}\")\n",
    "print(f\"Optimal Error Blend (XGB/CB): {final_xgb_weight:.2f} / {final_cb_weight:.2f}\")\n",
    "print(f\"Optimal Multipliers: a={best_blend_a:.4f}, b={best_blend_b:.4f}\")\n",
    "\n",
    "# Create the final prediction intervals for the test set\n",
    "final_lower = test_ensemble_mean - np.clip(test_error_final_ensemble, 0, None) * best_blend_a\n",
    "final_upper = test_ensemble_mean + np.clip(test_error_final_ensemble, 0, None) * best_blend_b\n",
    "final_upper = np.maximum(final_lower + 1, final_upper)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "994c9181-ddc3-45ec-b899-ca392da40594",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "'submission_final_OptimalEoE_292680.csv' created successfully! Good luck on the leaderboard!\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>pi_lower</th>\n",
       "      <th>pi_upper</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>200000</td>\n",
       "      <td>818976.003200</td>\n",
       "      <td>1.012106e+06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>200001</td>\n",
       "      <td>578449.575873</td>\n",
       "      <td>7.983806e+05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>200002</td>\n",
       "      <td>450278.368874</td>\n",
       "      <td>6.537181e+05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>200003</td>\n",
       "      <td>294652.612316</td>\n",
       "      <td>4.240232e+05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>200004</td>\n",
       "      <td>346969.921670</td>\n",
       "      <td>7.985549e+05</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       id       pi_lower      pi_upper\n",
       "0  200000  818976.003200  1.012106e+06\n",
       "1  200001  578449.575873  7.983806e+05\n",
       "2  200002  450278.368874  6.537181e+05\n",
       "3  200003  294652.612316  4.240232e+05\n",
       "4  200004  346969.921670  7.985549e+05"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Create the submission DataFrame\n",
    "submission_df = pd.DataFrame({'id': pd.read_csv('./test.csv')['id'], \n",
    "                              'pi_lower': final_lower, \n",
    "                              'pi_upper': final_upper})\n",
    "\n",
    "# Save the submission file\n",
    "submission_filename = f'submission_final_OptimalEoE_{int(best_blend_score)}.csv'\n",
    "submission_df.to_csv(submission_filename, index=False)\n",
    "\n",
    "print(f\"\\n'{submission_filename}' created successfully! Good luck on the leaderboard!\")\n",
    "display(submission_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "bb8d4de6-1ee6-44bc-bfad-b9a7f2553910",
   "metadata": {},
   "outputs": [],
   "source": [
    "er_PATH = './error_models'\n",
    "oof_error_preds_lgbm = np.load(os.path.join(er_PATH, 'oof_error_preds_lgbm.npy'))\n",
    "test_error_preds_lgbm = np.load(os.path.join(er_PATH, 'test_error_preds_lgbm.npy'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d0ab5d8e-9e8b-4b66-b01a-0beb30ad2eb7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Searching for the optimal blend of 3 error models AND their calibration multipliers... ---\n",
      "\n",
      "--- Building the final test set predictions with the optimal blend... ---\n",
      "\n",
      "============================================================\n",
      "FINAL RESULTS & SUBMISSION\n",
      "============================================================\n",
      "Final OOF Winkler Score (4 Mean + 3 Error Ensemble): $292,217.24\n",
      "Optimal Error Blend (XGB/CB/LGBM): 0.3107 / 0.3786 / 0.3107\n",
      "Optimal Multipliers: a=1.9799, b=2.1764\n"
     ]
    }
   ],
   "source": [
    "# =======================================================================================================\n",
    "# FINAL BLOCK (DEFINITIVE): OPTIMAL 3-ERROR-MODEL BLEND, PRECISE CALIBRATION, AND SUBMISSION\n",
    "# =======================================================================================================\n",
    "from scipy.optimize import minimize\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "# --- 1. Define the Objective Function for the Optimizer ---\n",
    "# This master function will find the best blend AND the best calibration simultaneously.\n",
    "\n",
    "print(\"\\n--- Searching for the optimal blend of 3 error models AND their calibration multipliers... ---\")\n",
    "\n",
    "# Stack the OOF error predictions for easy matrix multiplication\n",
    "oof_error_stack = np.vstack([oof_error_preds, oof_error_preds_cb, oof_error_preds_lgbm]).T\n",
    "\n",
    "def get_winkler_from_blend_and_calibration(params):\n",
    "    \"\"\"\n",
    "    This function takes a single array of parameters and unpacks it:\n",
    "    - params[0]: weight for XGB error model\n",
    "    - params[1]: weight for CatBoost error model\n",
    "    - params[2]: weight for LightGBM error model\n",
    "    - params[3]: multiplier 'a' for the lower bound\n",
    "    - params[4]: multiplier 'b' for the upper bound\n",
    "    \"\"\"\n",
    "    # Unpack the parameters\n",
    "    weights = params[:3]\n",
    "    a = params[3]\n",
    "    b = params[4]\n",
    "    \n",
    "    # Create the blended error prediction using the given weights\n",
    "    oof_error_blend = np.dot(oof_error_stack, weights)\n",
    "    oof_error_final_ensemble = np.clip(oof_error_blend, 0, None)\n",
    "    \n",
    "    # Calculate the calibrated prediction intervals\n",
    "    low = oof_ensemble_mean - oof_error_final_ensemble * a\n",
    "    high = oof_ensemble_mean + oof_error_final_ensemble * b\n",
    "    \n",
    "    # Return the Winkler score for this specific combination\n",
    "    return winkler_score(y_true, low, high)\n",
    "\n",
    "# --- 2. Run the Optimizer ---\n",
    "# We need an initial guess and bounds for all 5 parameters (3 weights + 2 multipliers)\n",
    "initial_guess = [1/3, 1/3, 1/3, 1.9, 2.2]\n",
    "bounds = [(0, 1), (0, 1), (0, 1), (1.0, 3.0), (1.0, 3.0)]\n",
    "\n",
    "# Add a constraint that the sum of the weights must equal 1\n",
    "weights_constraint = {'type': 'eq', 'fun': lambda params: 1.0 - np.sum(params[:3])}\n",
    "\n",
    "result = minimize(\n",
    "    fun=get_winkler_from_blend_and_calibration,\n",
    "    x0=initial_guess,\n",
    "    bounds=bounds,\n",
    "    constraints=[weights_constraint],\n",
    "    method='SLSQP'\n",
    ")\n",
    "\n",
    "# Extract the best parameters found by the optimizer\n",
    "best_params = result.x\n",
    "best_score = result.fun\n",
    "best_error_weights = best_params[:3]\n",
    "best_a = best_params[3]\n",
    "best_b = best_params[4]\n",
    "\n",
    "# --- 3. Build the Final Test Set Predictions ---\n",
    "print(\"\\n--- Building the final test set predictions with the optimal blend... ---\")\n",
    "test_error_stack = np.vstack([test_error_preds, test_error_preds_cb, test_error_preds_lgbm]).T\n",
    "test_error_final_ensemble = np.dot(test_error_stack, best_error_weights)\n",
    "\n",
    "# --- 4. Final Results and Submission ---\n",
    "print(\"\\n\" + \"=\"*60); print(\"FINAL RESULTS & SUBMISSION\"); print(\"=\"*60)\n",
    "print(f\"Final OOF Winkler Score (4 Mean + 3 Error Ensemble): ${best_score:,.2f}\")\n",
    "print(f\"Optimal Error Blend (XGB/CB/LGBM): {best_error_weights[0]:.4f} / {best_error_weights[1]:.4f} / {best_error_weights[2]:.4f}\")\n",
    "print(f\"Optimal Multipliers: a={best_a:.4f}, b={best_b:.4f}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "067f2494-8dfa-414a-9842-97a0ce7ab30f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "'submission_final_4M3E_292217.csv' created successfully! Good luck on the leaderboard!\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>pi_lower</th>\n",
       "      <th>pi_upper</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>200000</td>\n",
       "      <td>820615.127941</td>\n",
       "      <td>1.010704e+06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>200001</td>\n",
       "      <td>579589.665406</td>\n",
       "      <td>7.975820e+05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>200002</td>\n",
       "      <td>452163.714092</td>\n",
       "      <td>6.520662e+05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>200003</td>\n",
       "      <td>293769.395373</td>\n",
       "      <td>4.252615e+05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>200004</td>\n",
       "      <td>354575.178634</td>\n",
       "      <td>7.911283e+05</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       id       pi_lower      pi_upper\n",
       "0  200000  820615.127941  1.010704e+06\n",
       "1  200001  579589.665406  7.975820e+05\n",
       "2  200002  452163.714092  6.520662e+05\n",
       "3  200003  293769.395373  4.252615e+05\n",
       "4  200004  354575.178634  7.911283e+05"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Create the final prediction intervals for the test set\n",
    "final_lower = test_ensemble_mean - np.clip(test_error_final_ensemble, 0, None) * best_a\n",
    "final_upper = test_ensemble_mean + np.clip(test_error_final_ensemble, 0, None) * best_b\n",
    "final_upper = np.maximum(final_lower + 1, final_upper)\n",
    "\n",
    "# Create the submission DataFrame\n",
    "submission_df = pd.DataFrame({'id': pd.read_csv('./test.csv')['id'], \n",
    "                              'pi_lower': final_lower, \n",
    "                              'pi_upper': final_upper})\n",
    "\n",
    "# Save the submission file\n",
    "submission_filename = f'submission_final_4M3E_{int(best_score)}.csv'\n",
    "submission_df.to_csv(submission_filename, index=False)\n",
    "\n",
    "print(f\"\\n'{submission_filename}' created successfully! Good luck on the leaderboard!\")\n",
    "display(submission_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "835379f2-4e26-4000-a45e-45f3ae49ebe7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (Kaggle Comp)",
   "language": "python",
   "name": "kaggle-comp"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

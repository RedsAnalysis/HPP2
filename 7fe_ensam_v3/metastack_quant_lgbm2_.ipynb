{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d33a4648-2943-40a8-8af7-ae50570d8d81",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Libraries imported successfully.\n",
      "Raw data loaded successfully.\n",
      "Setup complete.\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# BLOCK 1: SETUP, IMPORTS, AND DATA LOADING\n",
    "# =============================================================================\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "import time\n",
    "import os\n",
    "# --- Library Imports ---\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import gc\n",
    "from sklearn.model_selection import StratifiedKFold, train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import xgboost as xgb\n",
    "import catboost as cb\n",
    "import optuna\n",
    "from scipy.optimize import minimize\n",
    "print(\"Libraries imported successfully.\")\n",
    "# --- Helper Function for Winkler Score ---\n",
    "def winkler_score(y_true, lower, upper, alpha=0.1, return_coverage=False):\n",
    "    width = upper - lower\n",
    "    penalty_lower = np.where(y_true < lower, (2 / alpha) * (lower - y_true), 0)\n",
    "    penalty_upper = np.where(y_true > upper, (2 / alpha) * (y_true - upper), 0)\n",
    "    score = width + penalty_lower + penalty_upper\n",
    "    if return_coverage:\n",
    "        coverage = np.mean((y_true >= lower) & (y_true <= upper))\n",
    "        return np.mean(score), coverage\n",
    "    return np.mean(score)\n",
    "# --- Global Constants ---\n",
    "N_SPLITS = 5\n",
    "RANDOM_STATE = 42\n",
    "DATA_PATH = './'\n",
    "N_OPTUNA_TRIALS = 30 # A strong number for a comprehensive search\n",
    "COMPETITION_ALPHA = 0.1\n",
    "\n",
    "# --- Load Raw Data ---\n",
    "try:\n",
    "    # We drop the low-variance columns they identified right away\n",
    "    drop_cols=['id', 'golf', 'view_rainier', 'view_skyline', 'view_lakesamm','view_otherwater', 'view_other']\n",
    "    df_train = pd.read_csv(DATA_PATH + 'dataset.csv').drop(columns=drop_cols)\n",
    "    df_test = pd.read_csv(DATA_PATH + 'test.csv').drop(columns=drop_cols)\n",
    "    print(\"Raw data loaded successfully.\")\n",
    "except FileNotFoundError:\n",
    "    print(\"ERROR: Could not find 'dataset.csv' or 'test.csv'.\")\n",
    "    exit()\n",
    "# --- Prepare Target Variable ---\n",
    "y_true = df_train['sale_price'].copy()\n",
    "grade_for_stratify = df_train['grade'].copy()\n",
    "# The mean-error model works best when predicting the raw price directly\n",
    "# So, we will NOT log-transform the target this time.\n",
    "# df_train.drop('sale_price', axis=1, inplace=True) # We keep sale_price for FE\n",
    "print(\"Setup complete.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3a94b99e-758e-4790-9a36-e704094fb922",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Starting Block 2.5: Executing Feature Engineering Pipeline ---\n",
      "--- Starting Comprehensive Feature Engineering ---\n",
      "Step 1: Creating brute-force numerical interaction features...\n",
      "Step 2: Creating date features...\n",
      "Step 3: Creating TF-IDF features for text columns...\n",
      "Step 4: Creating group-by aggregation features...\n",
      "Step 5: Creating ratio features...\n",
      "Step 6: Creating geospatial clustering features...\n",
      "Step 7: Finalizing feature set...\n",
      "\n",
      "Comprehensive FE complete. Total features: 233\n",
      "Feature engineering complete. X shape: (200000, 233), X_test shape: (200000, 233)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Make sure to have these libraries installed\n",
    "# pip install pandas numpy scikit-learn\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.cluster import KMeans\n",
    "import gc\n",
    "\n",
    "# Define a random state for reproducibility\n",
    "RANDOM_STATE = 42\n",
    "\n",
    "def create_comprehensive_features(df_train, df_test):\n",
    "    \"\"\"\n",
    "    Combines original and new advanced feature engineering steps into a single pipeline.\n",
    "    \"\"\"\n",
    "    print(\"--- Starting Comprehensive Feature Engineering ---\")\n",
    "\n",
    "    # Store original indices and target variable\n",
    "    train_ids = df_train.index\n",
    "    test_ids = df_test.index\n",
    "    y_train = df_train['sale_price'].copy() # Keep the target separate\n",
    "\n",
    "    # Combine for consistent processing\n",
    "    df_train_temp = df_train.drop(columns=['sale_price'])\n",
    "    all_data = pd.concat([df_train_temp, df_test], axis=0, ignore_index=True)\n",
    "\n",
    "    # --- Original Feature Engineering ---\n",
    "\n",
    "    # A) Brute-Force Numerical Interactions\n",
    "    print(\"Step 1: Creating brute-force numerical interaction features...\")\n",
    "    NUMS = ['area', 'land_val', 'imp_val', 'sqft_lot', 'sqft', 'sqft_1', 'grade', 'year_built']\n",
    "    # Ensure all columns exist and are numeric, fill missing with 0 for safety\n",
    "    for col in NUMS:\n",
    "        if col not in all_data.columns:\n",
    "            all_data[col] = 0\n",
    "        else:\n",
    "            all_data[col] = pd.to_numeric(all_data[col], errors='coerce').fillna(0)\n",
    "            \n",
    "    for i in range(len(NUMS)):\n",
    "        for j in range(i + 1, len(NUMS)):\n",
    "            all_data[f'{NUMS[i]}_x_{NUMS[j]}'] = all_data[NUMS[i]] * all_data[NUMS[j]]\n",
    "\n",
    "    # B) Date Features\n",
    "    print(\"Step 2: Creating date features...\")\n",
    "    all_data['sale_date'] = pd.to_datetime(all_data['sale_date'])\n",
    "    all_data['sale_year'] = all_data['sale_date'].dt.year\n",
    "    all_data['sale_month'] = all_data['sale_date'].dt.month\n",
    "    all_data['sale_dayofyear'] = all_data['sale_date'].dt.dayofyear\n",
    "    all_data['age_at_sale'] = all_data['sale_year'] - all_data['year_built']\n",
    "\n",
    "    # C) TF-IDF Text Features\n",
    "    print(\"Step 3: Creating TF-IDF features for text columns...\")\n",
    "    text_cols = ['subdivision', 'zoning', 'city', 'sale_warning', 'join_status', 'submarket']\n",
    "    all_data[text_cols] = all_data[text_cols].fillna('missing').astype(str)\n",
    "    \n",
    "    for col in text_cols:\n",
    "        tfidf = TfidfVectorizer(analyzer='char', ngram_range=(3, 5), max_features=128, binary=True)\n",
    "        svd = TruncatedSVD(n_components=8, random_state=RANDOM_STATE)\n",
    "        \n",
    "        tfidf_matrix = tfidf.fit_transform(all_data[col])\n",
    "        tfidf_svd = svd.fit_transform(tfidf_matrix)\n",
    "        \n",
    "        tfidf_df = pd.DataFrame(tfidf_svd, columns=[f'{col}_tfidf_svd_{i}' for i in range(8)])\n",
    "        all_data = pd.concat([all_data, tfidf_df], axis=1)\n",
    "\n",
    "    # D) Log transform some interaction features\n",
    "    for c in ['land_val_x_imp_val', 'land_val_x_sqft', 'imp_val_x_sqft']:\n",
    "        if c in all_data.columns:\n",
    "            all_data[c] = np.log1p(all_data[c].fillna(0))\n",
    "\n",
    "    # --- New Feature Engineering Ideas ---\n",
    "\n",
    "    # F) Group-By Aggregation Features\n",
    "    print(\"Step 4: Creating group-by aggregation features...\")\n",
    "    group_cols = ['submarket', 'city', 'zoning']\n",
    "    num_cols_for_agg = ['grade', 'sqft', 'imp_val', 'land_val', 'age_at_sale']\n",
    "\n",
    "    for group_col in group_cols:\n",
    "        for num_col in num_cols_for_agg:\n",
    "            agg_stats = all_data.groupby(group_col)[num_col].agg(['mean', 'std', 'max', 'min']).reset_index()\n",
    "            agg_stats.columns = [group_col] + [f'{group_col}_{num_col}_{stat}' for stat in ['mean', 'std', 'max', 'min']]\n",
    "            all_data = pd.merge(all_data, agg_stats, on=group_col, how='left')\n",
    "            all_data[f'{num_col}_minus_{group_col}_mean'] = all_data[num_col] - all_data[f'{group_col}_{num_col}_mean']\n",
    "\n",
    "    # G) Ratio Features\n",
    "    print(\"Step 5: Creating ratio features...\")\n",
    "    # Add a small epsilon to prevent division by zero\n",
    "    epsilon = 1e-6 \n",
    "    all_data['total_val'] = all_data['imp_val'] + all_data['land_val']\n",
    "    all_data['imp_val_to_land_val_ratio'] = all_data['imp_val'] / (all_data['land_val'] + epsilon)\n",
    "    all_data['land_val_ratio'] = all_data['land_val'] / (all_data['total_val'] + epsilon)\n",
    "    all_data['sqft_to_lot_ratio'] = all_data['sqft'] / (all_data['sqft_lot'] + epsilon)\n",
    "    all_data['was_renovated'] = (all_data['year_reno'] > 0).astype(int)\n",
    "    all_data['reno_age_at_sale'] = np.where(all_data['was_renovated'] == 1, all_data['sale_year'] - all_data['year_reno'], -1)\n",
    "\n",
    "    # H) Geospatial Clustering Features\n",
    "    print(\"Step 6: Creating geospatial clustering features...\")\n",
    "    coords = all_data[['latitude', 'longitude']].copy()\n",
    "    coords.fillna(coords.median(), inplace=True) # Simple imputation\n",
    "\n",
    "    # KMeans is sensitive to feature scaling, but for lat/lon it's often okay without it.\n",
    "    kmeans = KMeans(n_clusters=20, random_state=RANDOM_STATE, n_init=10) \n",
    "    all_data['location_cluster'] = kmeans.fit_predict(coords)\n",
    "    \n",
    "    # Calculate distance to each cluster center\n",
    "    cluster_centers = kmeans.cluster_centers_\n",
    "    for i in range(len(cluster_centers)):\n",
    "        center = cluster_centers[i]\n",
    "        all_data[f'dist_to_cluster_{i}'] = np.sqrt((coords['latitude'] - center[0])**2 + (coords['longitude'] - center[1])**2)\n",
    "\n",
    "    # --- Final Cleanup ---\n",
    "    print(\"Step 7: Finalizing feature set...\")\n",
    "    cols_to_drop = ['sale_date', 'subdivision', 'zoning', 'city', 'sale_warning', 'join_status', 'submarket']\n",
    "    all_data = all_data.drop(columns=cols_to_drop)\n",
    "\n",
    "    # One-hot encode the new cluster feature\n",
    "    all_data = pd.get_dummies(all_data, columns=['location_cluster'], prefix='loc_cluster')\n",
    "    \n",
    "    # Final check for any remaining object columns to be safe (besides index)\n",
    "    object_cols = all_data.select_dtypes(include='object').columns\n",
    "    if len(object_cols) > 0:\n",
    "        print(f\"Warning: Found unexpected object columns: {object_cols}. Dropping them.\")\n",
    "        all_data = all_data.drop(columns=object_cols)\n",
    "        \n",
    "    all_data.fillna(0, inplace=True)\n",
    "\n",
    "    # Separate back into train and test sets\n",
    "    train_len = len(train_ids)\n",
    "    X = all_data.iloc[:train_len].copy()\n",
    "    X_test = all_data.iloc[train_len:].copy()\n",
    "    \n",
    "    # Restore original indices\n",
    "    X.index = train_ids\n",
    "    X_test.index = test_ids\n",
    "    \n",
    "    # Align columns - crucial for model prediction\n",
    "    X_test = X_test[X.columns]\n",
    "    \n",
    "    print(f\"\\nComprehensive FE complete. Total features: {X.shape[1]}\")\n",
    "    gc.collect()\n",
    "    \n",
    "    return X, X_test, y_train\n",
    "# =============================================================================\n",
    "# BLOCK 2.5: EXECUTE FEATURE ENGINEERING\n",
    "# =============================================================================\n",
    "print(\"\\n--- Starting Block 2.5: Executing Feature Engineering Pipeline ---\")\n",
    "\n",
    "# This is the crucial step that was missing.\n",
    "# We call the function to create our training and testing dataframes.\n",
    "X, X_test, y_train = create_comprehensive_features(df_train, df_test)\n",
    "\n",
    "# Let's verify the output\n",
    "print(f\"Feature engineering complete. X shape: {X.shape}, X_test shape: {X_test.shape}\")\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1fc25c52-2f59-4ab9-9878-c98a2b97e3de",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Loading all base and meta-model predictions from saved .npy files... ---\n",
      "All necessary .npy prediction files loaded successfully.\n",
      "\n",
      "--- Recreating calibrated OOF and Test bounds to use as meta-features ---\n",
      "All interval bounds successfully recreated.\n",
      "\n",
      "--- Constructing the final meta-feature set for stacking ---\n",
      "Added 3 top raw features for context.\n",
      "\n",
      "Meta-feature dataframes are now ready for training.\n",
      "Shape of Meta-Train: (200000, 15)\n",
      "Shape of Meta-Test:  (200000, 15)\n",
      "\n",
      "Meta-Train Head:\n",
      "        lower_A       upper_A       lower_B       upper_B       lower_C  \\\n",
      "0  2.193217e+05  2.873461e+05  2.059288e+05  3.034720e+05  1.730037e+05   \n",
      "1  2.460684e+05  4.153292e+05  2.587635e+05  3.831388e+05  1.906595e+05   \n",
      "2  2.844782e+05  3.586439e+05  2.594633e+05  3.823383e+05  2.254399e+05   \n",
      "3  2.149198e+05  2.957100e+05  2.051202e+05  3.022437e+05  1.781910e+05   \n",
      "4  1.414717e+06  2.041337e+06  1.343647e+06  1.983026e+06  1.113419e+06   \n",
      "\n",
      "        upper_C        width_A        width_B       width_C      center_A  \\\n",
      "0  3.388776e+05   68024.381231   97543.138134  1.658739e+05  2.533339e+05   \n",
      "1  4.866505e+05  169260.854840  124375.329794  2.959909e+05  3.306988e+05   \n",
      "2  4.238892e+05   74165.725202  122875.072450  1.984493e+05  3.215611e+05   \n",
      "3  3.493456e+05   80790.168719   97123.569156  1.711546e+05  2.553149e+05   \n",
      "4  2.336956e+06  626620.136108  639378.985150  1.223537e+06  1.728027e+06   \n",
      "\n",
      "       center_B      center_C  grade  sqft  age_at_sale  \n",
      "0  2.547004e+05  2.559407e+05      7  1560           39  \n",
      "1  3.209511e+05  3.386550e+05      7  2040           37  \n",
      "2  3.209008e+05  3.246646e+05      7  1640           20  \n",
      "3  2.536820e+05  2.637683e+05      8  2610            1  \n",
      "4  1.663337e+06  1.725188e+06     12  4040           20  \n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# BLOCK 13: CREATE META-FEATURES FOR STACKING ENSEMBLE\n",
    "# =============================================================================\n",
    "# This block prepares the data for our final, most powerful approach: stacking.\n",
    "# We will treat the outputs of our three best pipelines as features and train a\n",
    "# new \"meta-model\" to learn the optimal, non-linear way to combine them.\n",
    "\n",
    "# --- Step 1: Load All Base and Meta-Model Predictions ---\n",
    "# This extends your existing loading block to include the quantile model outputs.\n",
    "\n",
    "# Define paths to your saved prediction files\n",
    "PREDS_SAVE_PATH = './mean_models_v1/'\n",
    "NN_PREDS_PATH = './NN_model_predictions/'\n",
    "ERR_PATH = './error_models/'\n",
    "META_XGB_PATH = './meta_quantile_models/' # Corrected Path\n",
    "META_LGBM_PATH = './meta_lgbm_quantile_models/' # Corrected Path\n",
    "\n",
    "print(\"--- Loading all base and meta-model predictions from saved .npy files... ---\")\n",
    "try:\n",
    "    # --- LEVEL 0: Base Model Predictions ---\n",
    "    # Load Mean Model OOF & Test Predictions\n",
    "    oof_xgb_preds = np.load(os.path.join(PREDS_SAVE_PATH, 'oof_xgb_preds.npy'))\n",
    "    oof_cb_preds = np.load(os.path.join(PREDS_SAVE_PATH, 'oof_cb_preds.npy'))\n",
    "    oof_lgbm_preds = np.load(os.path.join(PREDS_SAVE_PATH, 'oof_lgbm_preds.npy'))\n",
    "    oof_nn_preds = np.load(os.path.join(NN_PREDS_PATH, 'oof_nn_preds.npy'))\n",
    "    test_xgb_preds = np.load(os.path.join(PREDS_SAVE_PATH, 'test_xgb_preds.npy'))\n",
    "    test_cb_preds = np.load(os.path.join(PREDS_SAVE_PATH, 'test_cb_preds.npy'))\n",
    "    test_lgbm_preds = np.load(os.path.join(PREDS_SAVE_PATH, 'test_lgbm_preds.npy'))\n",
    "    test_nn_preds = np.load(os.path.join(NN_PREDS_PATH, 'test_nn_preds.npy'))\n",
    "    \n",
    "    # Load Error Model OOF & Test Predictions\n",
    "    oof_error_preds_xgb = np.load(os.path.join(ERR_PATH, 'oof_error_preds_xgb.npy'))\n",
    "    oof_error_preds_cb = np.load(os.path.join(ERR_PATH, 'oof_error_preds_cb.npy'))\n",
    "    test_error_preds_xgb = np.load(os.path.join(ERR_PATH, 'test_error_preds_xgb.npy'))\n",
    "    test_error_preds_cb = np.load(os.path.join(ERR_PATH, 'test_error_preds_cb.npy'))\n",
    "\n",
    "    # --- LEVEL 1: Quantile Model Predictions ---\n",
    "    # Load XGB Quantile OOF & Test Predictions\n",
    "    oof_lower_xgb = np.load(os.path.join(META_XGB_PATH, 'oof_lower_preds.npy'))\n",
    "    oof_upper_xgb = np.load(os.path.join(META_XGB_PATH, 'oof_upper_preds.npy'))\n",
    "    test_lower_xgb = np.load(os.path.join(META_XGB_PATH, 'test_lower_preds.npy'))\n",
    "    test_upper_xgb = np.load(os.path.join(META_XGB_PATH, 'test_upper_preds.npy'))\n",
    "\n",
    "    # Load LGBM Quantile OOF & Test Predictions\n",
    "    oof_lower_lgbm = np.load(os.path.join(META_LGBM_PATH, 'oof_lower_preds_lgbm.npy'))\n",
    "    oof_upper_lgbm = np.load(os.path.join(META_LGBM_PATH, 'oof_upper_preds_lgbm.npy'))\n",
    "    test_lower_lgbm = np.load(os.path.join(META_LGBM_PATH, 'test_lower_preds_lgbm.npy'))\n",
    "    test_upper_lgbm = np.load(os.path.join(META_LGBM_PATH, 'test_upper_preds_lgbm.npy'))\n",
    "\n",
    "    print(\"All necessary .npy prediction files loaded successfully.\")\n",
    "\n",
    "except FileNotFoundError as e:\n",
    "    print(f\"\\nERROR: Could not find a required prediction file. {e}\")\n",
    "    print(\"Please ensure all previous training notebooks have been run successfully.\")\n",
    "\n",
    "# --- Step 2: Recreate Calibrated Bounds for all Pipelines ---\n",
    "# We need to recreate the exact, calibrated bounds from each pipeline to use as features.\n",
    "print(\"\\n--- Recreating calibrated OOF and Test bounds to use as meta-features ---\")\n",
    "\n",
    "# --- Pipeline A: Mean+Error Model Bounds ---\n",
    "oof_ensemble_mean = (oof_xgb_preds + oof_cb_preds + oof_lgbm_preds + oof_nn_preds) / 4\n",
    "test_ensemble_mean = (test_xgb_preds + test_cb_preds + test_lgbm_preds + test_nn_preds) / 4\n",
    "a_err, b_err = 1.9799, 2.1755 # From previous optimization\n",
    "oof_error_final = np.clip((oof_error_preds_xgb * 0.60 + oof_error_preds_cb * 0.40), 0, None)\n",
    "test_error_final = np.clip((test_error_preds_xgb * 0.60 + test_error_preds_cb * 0.40), 0, None)\n",
    "\n",
    "oof_lower_A = oof_ensemble_mean - oof_error_final * a_err\n",
    "oof_upper_A = oof_ensemble_mean + oof_error_final * b_err\n",
    "test_lower_A = test_ensemble_mean - test_error_final * a_err\n",
    "test_upper_A = test_ensemble_mean + test_error_final * b_err\n",
    "\n",
    "# --- Pipeline B: XGB Quantile Model Bounds ---\n",
    "a_xgb, b_xgb = 0.8118, 1.1960 # From previous optimization\n",
    "oof_lower_B, oof_upper_B = np.minimum(oof_lower_xgb, oof_upper_xgb) * a_xgb, np.maximum(oof_lower_xgb, oof_upper_xgb) * b_xgb\n",
    "test_lower_B, test_upper_B = np.minimum(test_lower_xgb, test_upper_xgb) * a_xgb, np.maximum(test_lower_xgb, test_upper_xgb) * b_xgb\n",
    "\n",
    "# --- Pipeline C: LGBM Quantile Model Bounds ---\n",
    "a_lgbm, b_lgbm = 0.8118, 1.1960 # Placeholder; ideally, recalibrate as before, but reusing is fine for a start\n",
    "oof_lower_C, oof_upper_C = np.minimum(oof_lower_lgbm, oof_upper_lgbm) * a_lgbm, np.maximum(oof_lower_lgbm, oof_upper_lgbm) * b_lgbm\n",
    "test_lower_C, test_upper_C = np.minimum(test_lower_lgbm, test_upper_lgbm) * a_lgbm, np.maximum(test_lower_lgbm, test_upper_lgbm) * b_lgbm\n",
    "\n",
    "print(\"All interval bounds successfully recreated.\")\n",
    "\n",
    "# --- Step 3: Construct the Meta-Feature DataFrame ---\n",
    "print(\"\\n--- Constructing the final meta-feature set for stacking ---\")\n",
    "\n",
    "meta_features_train = pd.DataFrame({\n",
    "    'lower_A': oof_lower_A, 'upper_A': oof_upper_A,\n",
    "    'lower_B': oof_lower_B, 'upper_B': oof_upper_B,\n",
    "    'lower_C': oof_lower_C, 'upper_C': oof_upper_C,\n",
    "    \n",
    "    # Engineered features from the bounds are crucial\n",
    "    'width_A': oof_upper_A - oof_lower_A,\n",
    "    'width_B': oof_upper_B - oof_lower_B,\n",
    "    'width_C': oof_upper_C - oof_lower_C,\n",
    "    \n",
    "    'center_A': (oof_upper_A + oof_lower_A) / 2,\n",
    "    'center_B': (oof_upper_B + oof_lower_B) / 2,\n",
    "    'center_C': (oof_upper_C + oof_lower_C) / 2,\n",
    "})\n",
    "\n",
    "meta_features_test = pd.DataFrame({\n",
    "    'lower_A': test_lower_A, 'upper_A': test_upper_A,\n",
    "    'lower_B': test_lower_B, 'upper_B': test_upper_B,\n",
    "    'lower_C': test_lower_C, 'upper_C': test_upper_C,\n",
    "\n",
    "    'width_A': test_upper_A - test_lower_A,\n",
    "    'width_B': test_upper_B - test_lower_B,\n",
    "    'width_C': test_upper_C - test_upper_C,\n",
    "    \n",
    "    'center_A': (test_upper_A + test_lower_A) / 2,\n",
    "    'center_B': (test_upper_B + test_lower_B) / 2,\n",
    "    'center_C': (test_upper_C + test_lower_C) / 2,\n",
    "})\n",
    "\n",
    "# --- (Optional but Recommended) Add Top Raw Features ---\n",
    "# Add the absolute most important raw features to give the meta-model more context.\n",
    "# We need to load the original data to get these.\n",
    "df_train_raw = pd.read_csv('./dataset.csv')\n",
    "df_test_raw = pd.read_csv('./test.csv')\n",
    "\n",
    "top_raw_feats = ['grade', 'sqft', 'age_at_sale'] # Example, choose your best\n",
    "for feat in top_raw_feats:\n",
    "    # Need to re-calculate age_at_sale if not present\n",
    "    if feat == 'age_at_sale':\n",
    "        df_train_raw['sale_year'] = pd.to_datetime(df_train_raw['sale_date']).dt.year\n",
    "        df_test_raw['sale_year'] = pd.to_datetime(df_test_raw['sale_date']).dt.year\n",
    "        meta_features_train[feat] = df_train_raw['sale_year'] - df_train_raw['year_built']\n",
    "        meta_features_test[feat] = df_test_raw['sale_year'] - df_test_raw['year_built']\n",
    "    else:\n",
    "        meta_features_train[feat] = df_train_raw[feat]\n",
    "        meta_features_test[feat] = df_test_raw[feat]\n",
    "\n",
    "print(f\"Added {len(top_raw_feats)} top raw features for context.\")\n",
    "\n",
    "# --- Step 4: Define Meta-Model Targets ---\n",
    "# The target for our meta-models is simply the true sale price.\n",
    "# We will train two separate models, but they will both predict the same target.\n",
    "# The custom objective function will teach them to produce the lower/upper bound.\n",
    "y_meta_target = y_true.copy()\n",
    "\n",
    "print(\"\\nMeta-feature dataframes are now ready for training.\")\n",
    "print(f\"Shape of Meta-Train: {meta_features_train.shape}\")\n",
    "print(f\"Shape of Meta-Test:  {meta_features_test.shape}\")\n",
    "print(\"\\nMeta-Train Head:\")\n",
    "print(meta_features_train.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e8c81f2b-cce5-4d7b-b3a4-f09551d44b2d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-07-25 20:44:07,280] A new study created in memory with name: no-name-f525ae55-e0cc-4b67-b428-e42ac0a94452\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Preparing data for Meta-Model Optuna tuning ---\n",
      "\n",
      "--- Tuning the Lower-Bound Meta-Model (alpha=0.05)... ---\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e6a1fe0959f84b3c94423c88de0f78d3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/75 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-07-25 20:44:09,929] Trial 0 finished with value: 6822.0987678806605 and parameters: {'learning_rate': 0.019748355560134247, 'n_estimators': 1084, 'num_leaves': 11, 'lambda_l1': 1.9939761842585475, 'lambda_l2': 1.133456493069595, 'feature_fraction': 0.7747687293036789, 'bagging_fraction': 0.7234944211580132, 'bagging_freq': 7}. Best is trial 0 with value: 6822.0987678806605.\n",
      "[I 2025-07-25 20:44:13,862] Trial 1 finished with value: 6831.771709183329 and parameters: {'learning_rate': 0.014026176705003982, 'n_estimators': 1373, 'num_leaves': 46, 'lambda_l1': 2.4400960072604896, 'lambda_l2': 49.694387142689315, 'feature_fraction': 0.831440158015546, 'bagging_fraction': 0.8186904684798766, 'bagging_freq': 3}. Best is trial 0 with value: 6822.0987678806605.\n",
      "[I 2025-07-25 20:44:15,324] Trial 2 finished with value: 6836.056740597542 and parameters: {'learning_rate': 0.04624142917532263, 'n_estimators': 591, 'num_leaves': 34, 'lambda_l1': 1.9017668716759553, 'lambda_l2': 35.44524255565193, 'feature_fraction': 0.7887591860961503, 'bagging_fraction': 0.9119946848025392, 'bagging_freq': 3}. Best is trial 0 with value: 6822.0987678806605.\n",
      "[I 2025-07-25 20:44:16,945] Trial 3 finished with value: 6838.892197020679 and parameters: {'learning_rate': 0.03505631488840672, 'n_estimators': 1891, 'num_leaves': 34, 'lambda_l1': 1.0560976736936516, 'lambda_l2': 14.229587037341947, 'feature_fraction': 0.9196970910005798, 'bagging_fraction': 0.7024301124090099, 'bagging_freq': 5}. Best is trial 0 with value: 6822.0987678806605.\n",
      "[I 2025-07-25 20:44:19,166] Trial 4 finished with value: 6855.8831330489365 and parameters: {'learning_rate': 0.034827601527979625, 'n_estimators': 1405, 'num_leaves': 46, 'lambda_l1': 10.3337223390845, 'lambda_l2': 94.86768112076153, 'feature_fraction': 0.9419323568408186, 'bagging_fraction': 0.7295007694975176, 'bagging_freq': 3}. Best is trial 0 with value: 6822.0987678806605.\n",
      "[I 2025-07-25 20:44:24,416] Trial 5 finished with value: 6845.273981220507 and parameters: {'learning_rate': 0.01213929315327313, 'n_estimators': 1930, 'num_leaves': 45, 'lambda_l1': 22.09846379534358, 'lambda_l2': 3.0816463113840507, 'feature_fraction': 0.8946384785357472, 'bagging_fraction': 0.8038521294762051, 'bagging_freq': 5}. Best is trial 0 with value: 6822.0987678806605.\n",
      "[I 2025-07-25 20:44:26,249] Trial 6 finished with value: 6817.344247898851 and parameters: {'learning_rate': 0.019749991023233403, 'n_estimators': 2479, 'num_leaves': 26, 'lambda_l1': 1.542290602419607, 'lambda_l2': 0.6973702928906569, 'feature_fraction': 0.8891347063860783, 'bagging_fraction': 0.7088302064851906, 'bagging_freq': 3}. Best is trial 6 with value: 6817.344247898851.\n",
      "[I 2025-07-25 20:44:27,497] Trial 7 finished with value: 6844.932971709287 and parameters: {'learning_rate': 0.039157284457643764, 'n_estimators': 927, 'num_leaves': 48, 'lambda_l1': 0.48736005894315715, 'lambda_l2': 5.4575805232303995, 'feature_fraction': 0.9572130797971681, 'bagging_fraction': 0.8461927386819177, 'bagging_freq': 1}. Best is trial 6 with value: 6817.344247898851.\n",
      "[I 2025-07-25 20:44:28,961] Trial 8 finished with value: 6824.755535997931 and parameters: {'learning_rate': 0.033706822537426004, 'n_estimators': 1240, 'num_leaves': 47, 'lambda_l1': 0.2852219541957224, 'lambda_l2': 0.5241601711570801, 'feature_fraction': 0.7852319797489324, 'bagging_fraction': 0.7431788550634546, 'bagging_freq': 5}. Best is trial 6 with value: 6817.344247898851.\n",
      "[I 2025-07-25 20:44:35,042] Trial 9 finished with value: 6843.081894311746 and parameters: {'learning_rate': 0.030085664252615055, 'n_estimators': 1410, 'num_leaves': 38, 'lambda_l1': 6.457120556094542, 'lambda_l2': 6.2027942513996726, 'feature_fraction': 0.9124263074449056, 'bagging_fraction': 0.976124334528548, 'bagging_freq': 6}. Best is trial 6 with value: 6817.344247898851.\n",
      "[I 2025-07-25 20:44:37,149] Trial 10 finished with value: 6822.657443164006 and parameters: {'learning_rate': 0.020036018218583168, 'n_estimators': 2471, 'num_leaves': 21, 'lambda_l1': 0.11864653798461212, 'lambda_l2': 0.1582951042003387, 'feature_fraction': 0.8556276553989135, 'bagging_fraction': 0.7846217981884537, 'bagging_freq': 1}. Best is trial 6 with value: 6817.344247898851.\n",
      "[I 2025-07-25 20:44:38,997] Trial 11 finished with value: 6935.181119900612 and parameters: {'learning_rate': 0.018868889885522114, 'n_estimators': 2153, 'num_leaves': 11, 'lambda_l1': 67.61346792693524, 'lambda_l2': 1.0601412456043178, 'feature_fraction': 0.7110000839223152, 'bagging_fraction': 0.7531415911547479, 'bagging_freq': 7}. Best is trial 6 with value: 6817.344247898851.\n",
      "[I 2025-07-25 20:44:40,435] Trial 12 finished with value: 6823.685476106158 and parameters: {'learning_rate': 0.025642868600992387, 'n_estimators': 949, 'num_leaves': 22, 'lambda_l1': 0.5143538542716344, 'lambda_l2': 0.5880125678810357, 'feature_fraction': 0.7297650087412304, 'bagging_fraction': 0.7046506444677205, 'bagging_freq': 7}. Best is trial 6 with value: 6817.344247898851.\n",
      "[I 2025-07-25 20:44:43,420] Trial 13 finished with value: 6820.971829328445 and parameters: {'learning_rate': 0.01585098290117359, 'n_estimators': 1707, 'num_leaves': 11, 'lambda_l1': 4.12202328134553, 'lambda_l2': 0.1140692590551168, 'feature_fraction': 0.7728417083080804, 'bagging_fraction': 0.9077557404598701, 'bagging_freq': 2}. Best is trial 6 with value: 6817.344247898851.\n",
      "[I 2025-07-25 20:44:46,162] Trial 14 finished with value: 6834.284550977325 and parameters: {'learning_rate': 0.015216109399212023, 'n_estimators': 2453, 'num_leaves': 22, 'lambda_l1': 8.042426088528341, 'lambda_l2': 0.11837417991688984, 'feature_fraction': 0.8754138612423619, 'bagging_fraction': 0.901108306723355, 'bagging_freq': 2}. Best is trial 6 with value: 6817.344247898851.\n",
      "[I 2025-07-25 20:44:49,780] Trial 15 finished with value: 6856.16731237094 and parameters: {'learning_rate': 0.01024546219054841, 'n_estimators': 1742, 'num_leaves': 17, 'lambda_l1': 42.19557907601632, 'lambda_l2': 0.21287454283824964, 'feature_fraction': 0.8236776014719255, 'bagging_fraction': 0.914220194978263, 'bagging_freq': 2}. Best is trial 6 with value: 6817.344247898851.\n",
      "[I 2025-07-25 20:44:52,588] Trial 16 finished with value: 6826.713899875784 and parameters: {'learning_rate': 0.016096660974667526, 'n_estimators': 2177, 'num_leaves': 27, 'lambda_l1': 4.446423154338636, 'lambda_l2': 0.33961609720822844, 'feature_fraction': 0.9825792306036644, 'bagging_fraction': 0.9806459535169375, 'bagging_freq': 4}. Best is trial 6 with value: 6817.344247898851.\n",
      "[I 2025-07-25 20:44:54,056] Trial 17 finished with value: 6834.235469172662 and parameters: {'learning_rate': 0.024305269740785222, 'n_estimators': 1648, 'num_leaves': 15, 'lambda_l1': 16.157581892361566, 'lambda_l2': 1.6539546762992428, 'feature_fraction': 0.7541476914645295, 'bagging_fraction': 0.8772038268485346, 'bagging_freq': 2}. Best is trial 6 with value: 6817.344247898851.\n",
      "[I 2025-07-25 20:44:55,902] Trial 18 finished with value: 6852.69304083551 and parameters: {'learning_rate': 0.017219244737845744, 'n_estimators': 597, 'num_leaves': 30, 'lambda_l1': 1.082556567259492, 'lambda_l2': 0.28181924515817686, 'feature_fraction': 0.8386356756801094, 'bagging_fraction': 0.9315736990401097, 'bagging_freq': 4}. Best is trial 6 with value: 6817.344247898851.\n",
      "[I 2025-07-25 20:44:59,552] Trial 19 finished with value: 6822.560436374812 and parameters: {'learning_rate': 0.012202458571260491, 'n_estimators': 2164, 'num_leaves': 27, 'lambda_l1': 4.164564933944829, 'lambda_l2': 0.10529580307259447, 'feature_fraction': 0.8041454476974268, 'bagging_fraction': 0.9513916662084158, 'bagging_freq': 2}. Best is trial 6 with value: 6817.344247898851.\n",
      "[I 2025-07-25 20:45:01,341] Trial 20 finished with value: 6820.636748897313 and parameters: {'learning_rate': 0.021687501611781724, 'n_estimators': 1663, 'num_leaves': 17, 'lambda_l1': 1.060004890234526, 'lambda_l2': 0.551685991702746, 'feature_fraction': 0.8689352536498451, 'bagging_fraction': 0.8645661217225576, 'bagging_freq': 3}. Best is trial 6 with value: 6817.344247898851.\n",
      "[I 2025-07-25 20:45:03,358] Trial 21 finished with value: 6821.384132604003 and parameters: {'learning_rate': 0.02317695958879481, 'n_estimators': 1608, 'num_leaves': 16, 'lambda_l1': 1.0139059799607253, 'lambda_l2': 0.5215845290476917, 'feature_fraction': 0.8721747153706825, 'bagging_fraction': 0.8675791617374536, 'bagging_freq': 3}. Best is trial 6 with value: 6817.344247898851.\n",
      "[I 2025-07-25 20:45:04,652] Trial 22 finished with value: 6812.49960517772 and parameters: {'learning_rate': 0.028679521404211834, 'n_estimators': 1906, 'num_leaves': 10, 'lambda_l1': 0.33889627517299786, 'lambda_l2': 1.109183055065178, 'feature_fraction': 0.882146881986468, 'bagging_fraction': 0.8362369089503959, 'bagging_freq': 4}. Best is trial 22 with value: 6812.49960517772.\n",
      "[I 2025-07-25 20:45:06,433] Trial 23 finished with value: 6819.518759479427 and parameters: {'learning_rate': 0.027791887029368873, 'n_estimators': 1959, 'num_leaves': 18, 'lambda_l1': 0.17545397960974954, 'lambda_l2': 1.4844628054073488, 'feature_fraction': 0.9035304100676248, 'bagging_fraction': 0.8411306930309288, 'bagging_freq': 4}. Best is trial 22 with value: 6812.49960517772.\n",
      "[I 2025-07-25 20:45:07,836] Trial 24 finished with value: 6819.4533908533995 and parameters: {'learning_rate': 0.02790429682133423, 'n_estimators': 2306, 'num_leaves': 20, 'lambda_l1': 0.14311943200856475, 'lambda_l2': 2.325494640285399, 'feature_fraction': 0.8997914997298954, 'bagging_fraction': 0.7794270811175968, 'bagging_freq': 4}. Best is trial 22 with value: 6812.49960517772.\n",
      "[I 2025-07-25 20:45:09,217] Trial 25 finished with value: 6835.996487466425 and parameters: {'learning_rate': 0.029487975059372416, 'n_estimators': 2320, 'num_leaves': 25, 'lambda_l1': 0.3318456973649448, 'lambda_l2': 3.0526724863367396, 'feature_fraction': 0.9316608692162109, 'bagging_fraction': 0.7701230708242777, 'bagging_freq': 4}. Best is trial 22 with value: 6812.49960517772.\n",
      "[I 2025-07-25 20:45:10,132] Trial 26 finished with value: 6838.078744233287 and parameters: {'learning_rate': 0.04379078485254881, 'n_estimators': 2320, 'num_leaves': 31, 'lambda_l1': 0.10942189808570287, 'lambda_l2': 6.98762381346009, 'feature_fraction': 0.9995106641350313, 'bagging_fraction': 0.817817486055428, 'bagging_freq': 5}. Best is trial 22 with value: 6812.49960517772.\n",
      "[I 2025-07-25 20:45:11,312] Trial 27 finished with value: 6824.155482728663 and parameters: {'learning_rate': 0.025871983171502837, 'n_estimators': 2028, 'num_leaves': 14, 'lambda_l1': 0.19253375280021734, 'lambda_l2': 1.9539513044744248, 'feature_fraction': 0.8909560016372694, 'bagging_fraction': 0.7852374544665175, 'bagging_freq': 4}. Best is trial 22 with value: 6812.49960517772.\n",
      "[I 2025-07-25 20:45:12,629] Trial 28 finished with value: 6829.108441457794 and parameters: {'learning_rate': 0.03186544918227075, 'n_estimators': 2317, 'num_leaves': 24, 'lambda_l1': 0.5835750488766092, 'lambda_l2': 0.8714733236267649, 'feature_fraction': 0.9568953351952398, 'bagging_fraction': 0.7621681656678623, 'bagging_freq': 6}. Best is trial 22 with value: 6812.49960517772.\n",
      "[I 2025-07-25 20:45:14,509] Trial 29 finished with value: 6830.814957618633 and parameters: {'learning_rate': 0.020552554330806178, 'n_estimators': 2498, 'num_leaves': 20, 'lambda_l1': 0.27427953544038697, 'lambda_l2': 2.041468527969201, 'feature_fraction': 0.8565987441625758, 'bagging_fraction': 0.7277913551221773, 'bagging_freq': 6}. Best is trial 22 with value: 6812.49960517772.\n",
      "[I 2025-07-25 20:45:15,741] Trial 30 finished with value: 6818.809100676024 and parameters: {'learning_rate': 0.026775180706795913, 'n_estimators': 1829, 'num_leaves': 10, 'lambda_l1': 1.8623984756555128, 'lambda_l2': 10.78829513360491, 'feature_fraction': 0.813189754642441, 'bagging_fraction': 0.7939739259252981, 'bagging_freq': 4}. Best is trial 22 with value: 6812.49960517772.\n",
      "[I 2025-07-25 20:45:16,961] Trial 31 finished with value: 6819.454188969091 and parameters: {'learning_rate': 0.026741309294614636, 'n_estimators': 1805, 'num_leaves': 10, 'lambda_l1': 1.4321088763393577, 'lambda_l2': 13.743553487418776, 'feature_fraction': 0.8887220585193094, 'bagging_fraction': 0.8004461950401701, 'bagging_freq': 4}. Best is trial 22 with value: 6812.49960517772.\n",
      "[I 2025-07-25 20:45:18,823] Trial 32 finished with value: 6819.6868770470055 and parameters: {'learning_rate': 0.023172015410835915, 'n_estimators': 2157, 'num_leaves': 13, 'lambda_l1': 2.5674801269203527, 'lambda_l2': 15.56758188584616, 'feature_fraction': 0.8159459795218469, 'bagging_fraction': 0.8221885898118749, 'bagging_freq': 3}. Best is trial 22 with value: 6812.49960517772.\n",
      "[I 2025-07-25 20:45:19,824] Trial 33 finished with value: 6819.442368795472 and parameters: {'learning_rate': 0.03860945018545533, 'n_estimators': 2061, 'num_leaves': 19, 'lambda_l1': 1.7971979492581152, 'lambda_l2': 3.800985649880693, 'feature_fraction': 0.842834158106912, 'bagging_fraction': 0.8310621586570338, 'bagging_freq': 4}. Best is trial 22 with value: 6812.49960517772.\n",
      "[I 2025-07-25 20:45:20,700] Trial 34 finished with value: 6820.483564980725 and parameters: {'learning_rate': 0.040672636381188045, 'n_estimators': 1530, 'num_leaves': 13, 'lambda_l1': 1.7928506441088004, 'lambda_l2': 10.22447693423434, 'feature_fraction': 0.8401435300670886, 'bagging_fraction': 0.836966621289964, 'bagging_freq': 5}. Best is trial 22 with value: 6812.49960517772.\n",
      "[I 2025-07-25 20:45:21,767] Trial 35 finished with value: 6820.233820201658 and parameters: {'learning_rate': 0.03844918148125575, 'n_estimators': 2039, 'num_leaves': 10, 'lambda_l1': 0.7331183129468369, 'lambda_l2': 29.180327642152136, 'feature_fraction': 0.8156863327745815, 'bagging_fraction': 0.8891015451769846, 'bagging_freq': 3}. Best is trial 22 with value: 6812.49960517772.\n",
      "[I 2025-07-25 20:45:22,887] Trial 36 finished with value: 6837.453529271026 and parameters: {'learning_rate': 0.04834454721600819, 'n_estimators': 1841, 'num_leaves': 36, 'lambda_l1': 2.6853063308272085, 'lambda_l2': 4.57306034960007, 'feature_fraction': 0.8441180815792593, 'bagging_fraction': 0.8092415265563073, 'bagging_freq': 5}. Best is trial 22 with value: 6812.49960517772.\n",
      "[I 2025-07-25 20:45:25,370] Trial 37 finished with value: 6834.420700472677 and parameters: {'learning_rate': 0.018142622044137493, 'n_estimators': 2023, 'num_leaves': 42, 'lambda_l1': 1.5499551068915323, 'lambda_l2': 25.608949532658926, 'feature_fraction': 0.8645030138496005, 'bagging_fraction': 0.8303684699728267, 'bagging_freq': 3}. Best is trial 22 with value: 6812.49960517772.\n",
      "[I 2025-07-25 20:45:26,271] Trial 38 finished with value: 6822.619818920368 and parameters: {'learning_rate': 0.03694746446523167, 'n_estimators': 1915, 'num_leaves': 19, 'lambda_l1': 0.7869690219287362, 'lambda_l2': 3.7172116558711625, 'feature_fraction': 0.802015702744434, 'bagging_fraction': 0.8550419259480184, 'bagging_freq': 5}. Best is trial 22 with value: 6812.49960517772.\n",
      "[I 2025-07-25 20:45:27,456] Trial 39 finished with value: 6833.807222476713 and parameters: {'learning_rate': 0.03392180609412099, 'n_estimators': 1516, 'num_leaves': 30, 'lambda_l1': 0.4298011603507965, 'lambda_l2': 0.7448770039431925, 'feature_fraction': 0.9176459646734635, 'bagging_fraction': 0.7420205514992047, 'bagging_freq': 3}. Best is trial 22 with value: 6812.49960517772.\n",
      "[I 2025-07-25 20:45:28,557] Trial 40 finished with value: 6821.586736845073 and parameters: {'learning_rate': 0.04289303056831794, 'n_estimators': 1207, 'num_leaves': 13, 'lambda_l1': 2.2611052935353637, 'lambda_l2': 62.18342348091088, 'feature_fraction': 0.7684111957720763, 'bagging_fraction': 0.7111096530651999, 'bagging_freq': 4}. Best is trial 22 with value: 6812.49960517772.\n",
      "[I 2025-07-25 20:45:29,760] Trial 41 finished with value: 6825.548266483337 and parameters: {'learning_rate': 0.029258256322387304, 'n_estimators': 2284, 'num_leaves': 24, 'lambda_l1': 0.20226011285840942, 'lambda_l2': 2.4473345550146006, 'feature_fraction': 0.890584420199323, 'bagging_fraction': 0.7875409882588563, 'bagging_freq': 4}. Best is trial 22 with value: 6812.49960517772.\n",
      "[I 2025-07-25 20:45:30,989] Trial 42 finished with value: 6832.9645104403135 and parameters: {'learning_rate': 0.03170163748620926, 'n_estimators': 2256, 'num_leaves': 27, 'lambda_l1': 0.1487176770921807, 'lambda_l2': 1.1322614250669374, 'feature_fraction': 0.9333814637663388, 'bagging_fraction': 0.7997988319338898, 'bagging_freq': 5}. Best is trial 22 with value: 6812.49960517772.\n",
      "[I 2025-07-25 20:45:32,815] Trial 43 finished with value: 6822.696814097394 and parameters: {'learning_rate': 0.02464299809499148, 'n_estimators': 2401, 'num_leaves': 19, 'lambda_l1': 0.3956166676866233, 'lambda_l2': 8.572027867917404, 'feature_fraction': 0.9058758093339474, 'bagging_fraction': 0.7741776243201632, 'bagging_freq': 4}. Best is trial 22 with value: 6812.49960517772.\n",
      "[I 2025-07-25 20:45:35,366] Trial 44 finished with value: 6816.7409323900465 and parameters: {'learning_rate': 0.021172276548702785, 'n_estimators': 2079, 'num_leaves': 15, 'lambda_l1': 0.26350001578792676, 'lambda_l2': 4.171085290237271, 'feature_fraction': 0.8788348698116842, 'bagging_fraction': 0.7460281776570472, 'bagging_freq': 3}. Best is trial 22 with value: 6812.49960517772.\n",
      "[I 2025-07-25 20:45:37,002] Trial 45 finished with value: 6825.692123949617 and parameters: {'learning_rate': 0.021507894237596838, 'n_estimators': 2088, 'num_leaves': 12, 'lambda_l1': 5.775220451249666, 'lambda_l2': 20.435703715379418, 'feature_fraction': 0.8821392560550199, 'bagging_fraction': 0.7438669580866103, 'bagging_freq': 3}. Best is trial 22 with value: 6812.49960517772.\n",
      "[I 2025-07-25 20:45:39,121] Trial 46 finished with value: 6820.3289038402345 and parameters: {'learning_rate': 0.01866440759131823, 'n_estimators': 1808, 'num_leaves': 15, 'lambda_l1': 0.6873750747936762, 'lambda_l2': 4.047408018673495, 'feature_fraction': 0.851977926961766, 'bagging_fraction': 0.733246697417028, 'bagging_freq': 3}. Best is trial 22 with value: 6812.49960517772.\n",
      "[I 2025-07-25 20:45:40,918] Trial 47 finished with value: 6820.241621397312 and parameters: {'learning_rate': 0.02018616746081943, 'n_estimators': 1962, 'num_leaves': 15, 'lambda_l1': 0.3222246128095175, 'lambda_l2': 1.3253545051185696, 'feature_fraction': 0.7975359910651013, 'bagging_fraction': 0.7616645864365214, 'bagging_freq': 2}. Best is trial 22 with value: 6812.49960517772.\n",
      "[I 2025-07-25 20:45:43,840] Trial 48 finished with value: 6827.434055728857 and parameters: {'learning_rate': 0.014110737288642582, 'n_estimators': 1764, 'num_leaves': 50, 'lambda_l1': 3.5269682715673167, 'lambda_l2': 5.6133548377914435, 'feature_fraction': 0.8322936812018397, 'bagging_fraction': 0.7030691078177946, 'bagging_freq': 1}. Best is trial 22 with value: 6812.49960517772.\n",
      "[I 2025-07-25 20:45:44,807] Trial 49 finished with value: 6824.278541682314 and parameters: {'learning_rate': 0.03567051317040561, 'n_estimators': 1891, 'num_leaves': 10, 'lambda_l1': 0.2566736689788997, 'lambda_l2': 0.37309520155319126, 'feature_fraction': 0.8578735676462255, 'bagging_fraction': 0.7236129161256275, 'bagging_freq': 3}. Best is trial 22 with value: 6812.49960517772.\n",
      "[I 2025-07-25 20:45:46,349] Trial 50 finished with value: 6835.994703643829 and parameters: {'learning_rate': 0.02303480308506723, 'n_estimators': 1370, 'num_leaves': 12, 'lambda_l1': 11.014911472181591, 'lambda_l2': 10.52628828465497, 'feature_fraction': 0.8257240525425878, 'bagging_fraction': 0.7151846066194341, 'bagging_freq': 4}. Best is trial 22 with value: 6812.49960517772.\n",
      "[I 2025-07-25 20:45:47,935] Trial 51 finished with value: 6820.637708213852 and parameters: {'learning_rate': 0.02695234349846162, 'n_estimators': 2426, 'num_leaves': 22, 'lambda_l1': 0.123843993265537, 'lambda_l2': 2.7326942577537765, 'feature_fraction': 0.8779700569084604, 'bagging_fraction': 0.7483542581581166, 'bagging_freq': 4}. Best is trial 22 with value: 6812.49960517772.\n",
      "[I 2025-07-25 20:45:49,348] Trial 52 finished with value: 6826.609985079573 and parameters: {'learning_rate': 0.03161501224625634, 'n_estimators': 2233, 'num_leaves': 18, 'lambda_l1': 1.3732515321667123, 'lambda_l2': 0.8948160104102427, 'feature_fraction': 0.900608247850302, 'bagging_fraction': 0.7950714753864472, 'bagging_freq': 3}. Best is trial 22 with value: 6812.49960517772.\n",
      "[I 2025-07-25 20:45:50,666] Trial 53 finished with value: 6821.220466828753 and parameters: {'learning_rate': 0.028685100984638723, 'n_estimators': 2086, 'num_leaves': 21, 'lambda_l1': 0.2040121790163199, 'lambda_l2': 2.1543898942329056, 'feature_fraction': 0.9186513194797292, 'bagging_fraction': 0.8130592172171744, 'bagging_freq': 4}. Best is trial 22 with value: 6812.49960517772.\n",
      "[I 2025-07-25 20:45:52,398] Trial 54 finished with value: 6835.026602993393 and parameters: {'learning_rate': 0.02488364784976006, 'n_estimators': 2385, 'num_leaves': 16, 'lambda_l1': 0.1478185814664838, 'lambda_l2': 5.023230632424336, 'feature_fraction': 0.9534967381705644, 'bagging_fraction': 0.7783634995116209, 'bagging_freq': 5}. Best is trial 22 with value: 6812.49960517772.\n",
      "[I 2025-07-25 20:45:53,866] Trial 55 finished with value: 6816.60979224888 and parameters: {'learning_rate': 0.02120541123304111, 'n_estimators': 2211, 'num_leaves': 12, 'lambda_l1': 2.0544776705453556, 'lambda_l2': 6.918953498695076, 'feature_fraction': 0.8662218503675381, 'bagging_fraction': 0.7669057929810876, 'bagging_freq': 4}. Best is trial 22 with value: 6812.49960517772.\n",
      "[I 2025-07-25 20:45:55,847] Trial 56 finished with value: 6819.725349729958 and parameters: {'learning_rate': 0.017318477580556938, 'n_estimators': 2134, 'num_leaves': 12, 'lambda_l1': 1.9088024617998927, 'lambda_l2': 8.363024271138466, 'feature_fraction': 0.863163221603516, 'bagging_fraction': 0.8503504897692726, 'bagging_freq': 2}. Best is trial 22 with value: 6812.49960517772.\n",
      "[I 2025-07-25 20:45:57,620] Trial 57 finished with value: 6823.075465159374 and parameters: {'learning_rate': 0.021267991445813574, 'n_estimators': 1966, 'num_leaves': 14, 'lambda_l1': 3.1248164208775586, 'lambda_l2': 6.628041019985333, 'feature_fraction': 0.8480396263512326, 'bagging_fraction': 0.8312036360774866, 'bagging_freq': 4}. Best is trial 22 with value: 6812.49960517772.\n",
      "[I 2025-07-25 20:45:59,230] Trial 58 finished with value: 6822.277237224626 and parameters: {'learning_rate': 0.018955393069569153, 'n_estimators': 2237, 'num_leaves': 11, 'lambda_l1': 5.716982282198321, 'lambda_l2': 13.638224913838613, 'feature_fraction': 0.7845133077487856, 'bagging_fraction': 0.7587509465284037, 'bagging_freq': 3}. Best is trial 22 with value: 6812.49960517772.\n",
      "[I 2025-07-25 20:46:00,390] Trial 59 finished with value: 6881.791415566781 and parameters: {'learning_rate': 0.01958364542740657, 'n_estimators': 515, 'num_leaves': 16, 'lambda_l1': 1.0294022664305864, 'lambda_l2': 4.0569185071514635, 'feature_fraction': 0.8819862810142696, 'bagging_fraction': 0.7331847432532637, 'bagging_freq': 5}. Best is trial 22 with value: 6812.49960517772.\n",
      "[I 2025-07-25 20:46:02,533] Trial 60 finished with value: 6826.630549041276 and parameters: {'learning_rate': 0.022397281735732334, 'n_estimators': 2088, 'num_leaves': 14, 'lambda_l1': 0.8444820979167298, 'lambda_l2': 44.28599421038016, 'feature_fraction': 0.8347359529588534, 'bagging_fraction': 0.7661293124811306, 'bagging_freq': 4}. Best is trial 22 with value: 6812.49960517772.\n",
      "[I 2025-07-25 20:46:04,060] Trial 61 finished with value: 6819.139709171697 and parameters: {'learning_rate': 0.024356176786537526, 'n_estimators': 2363, 'num_leaves': 17, 'lambda_l1': 1.4693476256178157, 'lambda_l2': 3.399506507364646, 'feature_fraction': 0.9087337586012888, 'bagging_fraction': 0.7893157285427289, 'bagging_freq': 4}. Best is trial 22 with value: 6812.49960517772.\n",
      "[I 2025-07-25 20:46:05,967] Trial 62 finished with value: 6827.913399455649 and parameters: {'learning_rate': 0.02373025572297041, 'n_estimators': 2354, 'num_leaves': 17, 'lambda_l1': 1.4171989728928287, 'lambda_l2': 3.727997821992016, 'feature_fraction': 0.9338212772180617, 'bagging_fraction': 0.9968932255724875, 'bagging_freq': 4}. Best is trial 22 with value: 6812.49960517772.\n",
      "[I 2025-07-25 20:46:07,390] Trial 63 finished with value: 6817.877041597132 and parameters: {'learning_rate': 0.025744260078067117, 'n_estimators': 2197, 'num_leaves': 11, 'lambda_l1': 2.1430854566394104, 'lambda_l2': 1.4557991922487152, 'feature_fraction': 0.9113911339109921, 'bagging_fraction': 0.8609214265735962, 'bagging_freq': 4}. Best is trial 22 with value: 6812.49960517772.\n",
      "[I 2025-07-25 20:46:09,278] Trial 64 finished with value: 6823.915867965407 and parameters: {'learning_rate': 0.025804902785694535, 'n_estimators': 2210, 'num_leaves': 10, 'lambda_l1': 3.2852829395761, 'lambda_l2': 1.5612687793983708, 'feature_fraction': 0.9098042620570992, 'bagging_fraction': 0.7933280139183686, 'bagging_freq': 3}. Best is trial 22 with value: 6812.49960517772.\n",
      "[I 2025-07-25 20:46:10,779] Trial 65 finished with value: 6830.214483486307 and parameters: {'learning_rate': 0.02221167112194334, 'n_estimators': 2478, 'num_leaves': 11, 'lambda_l1': 0.5535268306268721, 'lambda_l2': 0.47903183751994044, 'feature_fraction': 0.9255337993097894, 'bagging_fraction': 0.8795370068973587, 'bagging_freq': 6}. Best is trial 22 with value: 6812.49960517772.\n",
      "[I 2025-07-25 20:46:12,731] Trial 66 finished with value: 6822.459886727041 and parameters: {'learning_rate': 0.020659286245733738, 'n_estimators': 2178, 'num_leaves': 12, 'lambda_l1': 2.3041547069719757, 'lambda_l2': 0.6642081013810939, 'feature_fraction': 0.94364061212014, 'bagging_fraction': 0.8628850339195212, 'bagging_freq': 4}. Best is trial 22 with value: 6812.49960517772.\n",
      "[I 2025-07-25 20:46:14,929] Trial 67 finished with value: 6824.962129490742 and parameters: {'learning_rate': 0.017312999931760963, 'n_estimators': 2379, 'num_leaves': 14, 'lambda_l1': 4.971233488299536, 'lambda_l2': 1.7680145084607672, 'feature_fraction': 0.8703639454375159, 'bagging_fraction': 0.7526382518168884, 'bagging_freq': 3}. Best is trial 22 with value: 6812.49960517772.\n",
      "[I 2025-07-25 20:46:16,308] Trial 68 finished with value: 6825.692436156794 and parameters: {'learning_rate': 0.030404072622824383, 'n_estimators': 1989, 'num_leaves': 11, 'lambda_l1': 1.1335956525314757, 'lambda_l2': 1.281751388499091, 'feature_fraction': 0.8911021173527724, 'bagging_fraction': 0.7164006935493288, 'bagging_freq': 4}. Best is trial 22 with value: 6812.49960517772.\n",
      "[I 2025-07-25 20:46:17,691] Trial 69 finished with value: 6831.285637877557 and parameters: {'learning_rate': 0.02712709534923338, 'n_estimators': 1865, 'num_leaves': 32, 'lambda_l1': 7.724542496178768, 'lambda_l2': 0.9205346367764182, 'feature_fraction': 0.8975873803648108, 'bagging_fraction': 0.8074177658812645, 'bagging_freq': 5}. Best is trial 22 with value: 6812.49960517772.\n",
      "[I 2025-07-25 20:46:19,601] Trial 70 finished with value: 6828.868968328925 and parameters: {'learning_rate': 0.02527110723323784, 'n_estimators': 2440, 'num_leaves': 43, 'lambda_l1': 1.2377412976807012, 'lambda_l2': 2.7804708841054295, 'feature_fraction': 0.9676355194276407, 'bagging_fraction': 0.8220506150457444, 'bagging_freq': 4}. Best is trial 22 with value: 6812.49960517772.\n",
      "[I 2025-07-25 20:46:21,309] Trial 71 finished with value: 6824.165450789684 and parameters: {'learning_rate': 0.02319653854435124, 'n_estimators': 2124, 'num_leaves': 13, 'lambda_l1': 1.7627604733261362, 'lambda_l2': 3.212605704470396, 'feature_fraction': 0.8748514363981381, 'bagging_fraction': 0.8560809221174194, 'bagging_freq': 4}. Best is trial 22 with value: 6812.49960517772.\n",
      "[I 2025-07-25 20:46:23,637] Trial 72 finished with value: 6822.34337041833 and parameters: {'learning_rate': 0.015919065292533875, 'n_estimators': 2027, 'num_leaves': 18, 'lambda_l1': 2.0954837879162764, 'lambda_l2': 6.087135564865777, 'feature_fraction': 0.8142031170892067, 'bagging_fraction': 0.8360691048468533, 'bagging_freq': 4}. Best is trial 22 with value: 6812.49960517772.\n",
      "[I 2025-07-25 20:46:25,731] Trial 73 finished with value: 6971.49251426948 and parameters: {'learning_rate': 0.01949906513597005, 'n_estimators': 762, 'num_leaves': 16, 'lambda_l1': 94.5160562912144, 'lambda_l2': 8.196247287021839, 'feature_fraction': 0.9090557202409557, 'bagging_fraction': 0.8726766508291588, 'bagging_freq': 4}. Best is trial 22 with value: 6812.49960517772.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-07-25 20:46:27,299] A new study created in memory with name: no-name-19432802-220d-4354-9a5b-f39cf529c500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-07-25 20:46:27,297] Trial 74 finished with value: 6818.208249155021 and parameters: {'learning_rate': 0.02429942155271104, 'n_estimators': 2303, 'num_leaves': 10, 'lambda_l1': 2.8964499187619137, 'lambda_l2': 4.946434789740576, 'feature_fraction': 0.8843867126201211, 'bagging_fraction': 0.827302587339491, 'bagging_freq': 3}. Best is trial 22 with value: 6812.49960517772.\n",
      "\n",
      "--- Tuning the Upper-Bound Meta-Model (alpha=0.95)... ---\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a3e16122f45b4fc494a1e059ef3c3924",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/75 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-07-25 20:46:28,377] Trial 0 finished with value: 7894.718321042673 and parameters: {'learning_rate': 0.04122783223518715, 'n_estimators': 2015, 'num_leaves': 28, 'lambda_l1': 0.18283506089915916, 'lambda_l2': 8.070268121784082, 'feature_fraction': 0.9687553598711565, 'bagging_fraction': 0.9774523728746314, 'bagging_freq': 5}. Best is trial 0 with value: 7894.718321042673.\n",
      "[I 2025-07-25 20:46:31,177] Trial 1 finished with value: 7869.922452755748 and parameters: {'learning_rate': 0.020001463640311117, 'n_estimators': 2236, 'num_leaves': 33, 'lambda_l1': 74.52640494101348, 'lambda_l2': 4.081051325034335, 'feature_fraction': 0.8503323192878747, 'bagging_fraction': 0.8774411301660135, 'bagging_freq': 1}. Best is trial 1 with value: 7869.922452755748.\n",
      "[I 2025-07-25 20:46:32,987] Trial 2 finished with value: 7922.625723815521 and parameters: {'learning_rate': 0.03267519225797239, 'n_estimators': 1799, 'num_leaves': 35, 'lambda_l1': 2.417881569814411, 'lambda_l2': 0.1965119523852925, 'feature_fraction': 0.7998681355155923, 'bagging_fraction': 0.9207097398223839, 'bagging_freq': 1}. Best is trial 1 with value: 7869.922452755748.\n",
      "[I 2025-07-25 20:46:35,455] Trial 3 finished with value: 7934.855628417144 and parameters: {'learning_rate': 0.02070980246764801, 'n_estimators': 2395, 'num_leaves': 49, 'lambda_l1': 0.3346535227339354, 'lambda_l2': 48.65289540594853, 'feature_fraction': 0.7638117857744978, 'bagging_fraction': 0.9014682217179737, 'bagging_freq': 6}. Best is trial 1 with value: 7869.922452755748.\n",
      "[I 2025-07-25 20:46:38,434] Trial 4 finished with value: 7911.659464549094 and parameters: {'learning_rate': 0.01746696016383003, 'n_estimators': 2085, 'num_leaves': 49, 'lambda_l1': 2.0443059537913197, 'lambda_l2': 82.14204333414584, 'feature_fraction': 0.7849353305088856, 'bagging_fraction': 0.9198968710015107, 'bagging_freq': 3}. Best is trial 1 with value: 7869.922452755748.\n",
      "[I 2025-07-25 20:46:40,135] Trial 5 finished with value: 7866.386015361625 and parameters: {'learning_rate': 0.020653996617190154, 'n_estimators': 2051, 'num_leaves': 18, 'lambda_l1': 1.886341455349515, 'lambda_l2': 99.94414007136655, 'feature_fraction': 0.7933247631879992, 'bagging_fraction': 0.723644597901601, 'bagging_freq': 6}. Best is trial 5 with value: 7866.386015361625.\n",
      "[I 2025-07-25 20:46:41,173] Trial 6 finished with value: 7887.850892202798 and parameters: {'learning_rate': 0.04302890167018838, 'n_estimators': 1970, 'num_leaves': 32, 'lambda_l1': 0.8517510818940012, 'lambda_l2': 0.7870113306149396, 'feature_fraction': 0.7418839166729639, 'bagging_fraction': 0.8975133664015466, 'bagging_freq': 2}. Best is trial 5 with value: 7866.386015361625.\n",
      "[I 2025-07-25 20:46:44,933] Trial 7 finished with value: 7889.623160136212 and parameters: {'learning_rate': 0.012445390230491486, 'n_estimators': 1201, 'num_leaves': 28, 'lambda_l1': 0.5638588890236323, 'lambda_l2': 9.447073147354292, 'feature_fraction': 0.875257251493252, 'bagging_fraction': 0.9177394805242428, 'bagging_freq': 2}. Best is trial 5 with value: 7866.386015361625.\n",
      "[I 2025-07-25 20:46:45,880] Trial 8 finished with value: 7879.002503819491 and parameters: {'learning_rate': 0.04344498828707846, 'n_estimators': 773, 'num_leaves': 50, 'lambda_l1': 89.8756285805883, 'lambda_l2': 0.19323651098890812, 'feature_fraction': 0.7009157407770732, 'bagging_fraction': 0.9020659033548726, 'bagging_freq': 2}. Best is trial 5 with value: 7866.386015361625.\n",
      "[I 2025-07-25 20:46:46,987] Trial 9 finished with value: 7862.540233273288 and parameters: {'learning_rate': 0.03126711186892128, 'n_estimators': 1674, 'num_leaves': 20, 'lambda_l1': 0.16877649045013904, 'lambda_l2': 0.20344626684580808, 'feature_fraction': 0.992665626254047, 'bagging_fraction': 0.9026760304251529, 'bagging_freq': 7}. Best is trial 9 with value: 7862.540233273288.\n",
      "[I 2025-07-25 20:46:48,456] Trial 10 finished with value: 7872.78729596906 and parameters: {'learning_rate': 0.02953616903134106, 'n_estimators': 1444, 'num_leaves': 12, 'lambda_l1': 12.9736540804557, 'lambda_l2': 1.0219678639342682, 'feature_fraction': 0.9990795517792612, 'bagging_fraction': 0.8046796428598754, 'bagging_freq': 7}. Best is trial 9 with value: 7862.540233273288.\n",
      "[I 2025-07-25 20:46:49,874] Trial 11 finished with value: 7865.610843158205 and parameters: {'learning_rate': 0.027685954410276326, 'n_estimators': 1583, 'num_leaves': 16, 'lambda_l1': 0.10231298262624547, 'lambda_l2': 27.138492852789827, 'feature_fraction': 0.9053136486100319, 'bagging_fraction': 0.7012498649653578, 'bagging_freq': 7}. Best is trial 9 with value: 7862.540233273288.\n",
      "[I 2025-07-25 20:46:51,058] Trial 12 finished with value: 7892.445713253218 and parameters: {'learning_rate': 0.0287620428357716, 'n_estimators': 1547, 'num_leaves': 19, 'lambda_l1': 0.1363703445721337, 'lambda_l2': 17.24196013318211, 'feature_fraction': 0.915676333116629, 'bagging_fraction': 0.7028755852000946, 'bagging_freq': 7}. Best is trial 9 with value: 7862.540233273288.\n",
      "[I 2025-07-25 20:46:52,641] Trial 13 finished with value: 7844.941802238107 and parameters: {'learning_rate': 0.027397851964296484, 'n_estimators': 1156, 'num_leaves': 10, 'lambda_l1': 0.13449114062164158, 'lambda_l2': 1.0522196055898028, 'feature_fraction': 0.9549028380355625, 'bagging_fraction': 0.8087330162432866, 'bagging_freq': 5}. Best is trial 13 with value: 7844.941802238107.\n",
      "[I 2025-07-25 20:46:53,769] Trial 14 finished with value: 7851.079073704546 and parameters: {'learning_rate': 0.035340134963126366, 'n_estimators': 975, 'num_leaves': 10, 'lambda_l1': 5.9708734502621255, 'lambda_l2': 0.8001680386913799, 'feature_fraction': 0.9555888547319727, 'bagging_fraction': 0.8124147942542801, 'bagging_freq': 4}. Best is trial 13 with value: 7844.941802238107.\n",
      "[I 2025-07-25 20:46:55,985] Trial 15 finished with value: 7902.623593405003 and parameters: {'learning_rate': 0.015100834555463996, 'n_estimators': 783, 'num_leaves': 24, 'lambda_l1': 8.433304913678736, 'lambda_l2': 0.9679357661625271, 'feature_fraction': 0.9432884209625493, 'bagging_fraction': 0.8032443058283107, 'bagging_freq': 4}. Best is trial 13 with value: 7844.941802238107.\n",
      "[I 2025-07-25 20:46:57,454] Trial 16 finished with value: 7859.464482433468 and parameters: {'learning_rate': 0.024209721704394487, 'n_estimators': 1136, 'num_leaves': 10, 'lambda_l1': 10.74486004583578, 'lambda_l2': 2.2284804470410298, 'feature_fraction': 0.9384684363267267, 'bagging_fraction': 0.79460894596431, 'bagging_freq': 4}. Best is trial 13 with value: 7844.941802238107.\n",
      "[I 2025-07-25 20:46:59,196] Trial 17 finished with value: 7893.9301056034 and parameters: {'learning_rate': 0.037139855555176064, 'n_estimators': 501, 'num_leaves': 40, 'lambda_l1': 26.254755931027383, 'lambda_l2': 0.5943070717191674, 'feature_fraction': 0.895095061995601, 'bagging_fraction': 0.8422861399623008, 'bagging_freq': 5}. Best is trial 13 with value: 7844.941802238107.\n",
      "[I 2025-07-25 20:47:00,021] Trial 18 finished with value: 7860.4409157937735 and parameters: {'learning_rate': 0.04808853904661362, 'n_estimators': 1201, 'num_leaves': 14, 'lambda_l1': 4.068497452969222, 'lambda_l2': 2.2846990265783003, 'feature_fraction': 0.8384305125394093, 'bagging_fraction': 0.7549689535873164, 'bagging_freq': 5}. Best is trial 13 with value: 7844.941802238107.\n",
      "[I 2025-07-25 20:47:03,279] Trial 19 finished with value: 7880.668412730151 and parameters: {'learning_rate': 0.025096008055396524, 'n_estimators': 921, 'num_leaves': 23, 'lambda_l1': 30.67261099460022, 'lambda_l2': 0.4294348811519176, 'feature_fraction': 0.9505831789629682, 'bagging_fraction': 0.8427139830022177, 'bagging_freq': 3}. Best is trial 13 with value: 7844.941802238107.\n",
      "[I 2025-07-25 20:47:04,357] Trial 20 finished with value: 7854.72473169578 and parameters: {'learning_rate': 0.03690401976444775, 'n_estimators': 1362, 'num_leaves': 11, 'lambda_l1': 4.7487390934596485, 'lambda_l2': 0.40514622090621777, 'feature_fraction': 0.9705202493072033, 'bagging_fraction': 0.7634604581293531, 'bagging_freq': 4}. Best is trial 13 with value: 7844.941802238107.\n",
      "[I 2025-07-25 20:47:05,420] Trial 21 finished with value: 7849.509234666686 and parameters: {'learning_rate': 0.03320090472226808, 'n_estimators': 1373, 'num_leaves': 10, 'lambda_l1': 1.2522835426132728, 'lambda_l2': 0.36396778769993043, 'feature_fraction': 0.9714185061113126, 'bagging_fraction': 0.7651604982957906, 'bagging_freq': 4}. Best is trial 13 with value: 7844.941802238107.\n",
      "[I 2025-07-25 20:47:06,643] Trial 22 finished with value: 7861.154661680138 and parameters: {'learning_rate': 0.03543534932034487, 'n_estimators': 950, 'num_leaves': 15, 'lambda_l1': 0.7922734058299692, 'lambda_l2': 1.2467857819551564, 'feature_fraction': 0.9262426490303595, 'bagging_fraction': 0.7693896095566992, 'bagging_freq': 3}. Best is trial 13 with value: 7844.941802238107.\n",
      "[I 2025-07-25 20:47:08,050] Trial 23 finished with value: 7851.494358024165 and parameters: {'learning_rate': 0.024805192424523767, 'n_estimators': 1344, 'num_leaves': 11, 'lambda_l1': 1.1466629401215487, 'lambda_l2': 0.3952169367126473, 'feature_fraction': 0.9686068503957072, 'bagging_fraction': 0.8243600577599184, 'bagging_freq': 5}. Best is trial 13 with value: 7844.941802238107.\n",
      "[I 2025-07-25 20:47:09,292] Trial 24 finished with value: 7881.264120426286 and parameters: {'learning_rate': 0.04996446603113165, 'n_estimators': 1025, 'num_leaves': 22, 'lambda_l1': 0.27691119593910263, 'lambda_l2': 0.1402666205401512, 'feature_fraction': 0.8765775869700043, 'bagging_fraction': 0.740520541546916, 'bagging_freq': 6}. Best is trial 13 with value: 7844.941802238107.\n",
      "[I 2025-07-25 20:47:10,290] Trial 25 finished with value: 7859.862287347266 and parameters: {'learning_rate': 0.03370800354081677, 'n_estimators': 571, 'num_leaves': 15, 'lambda_l1': 0.41048926541805925, 'lambda_l2': 4.302539715386428, 'feature_fraction': 0.9813333245145215, 'bagging_fraction': 0.7839685986691931, 'bagging_freq': 4}. Best is trial 13 with value: 7844.941802238107.\n",
      "[I 2025-07-25 20:47:12,046] Trial 26 finished with value: 8163.602487642402 and parameters: {'learning_rate': 0.010390120237474234, 'n_estimators': 773, 'num_leaves': 10, 'lambda_l1': 5.274198249150751, 'lambda_l2': 1.5902932665660796, 'feature_fraction': 0.9526498463432174, 'bagging_fraction': 0.8615641312848418, 'bagging_freq': 3}. Best is trial 13 with value: 7844.941802238107.\n",
      "[I 2025-07-25 20:47:13,265] Trial 27 finished with value: 7936.159558187304 and parameters: {'learning_rate': 0.03886008819004876, 'n_estimators': 1267, 'num_leaves': 42, 'lambda_l1': 1.4530415423998373, 'lambda_l2': 0.2924384666154773, 'feature_fraction': 0.9254400565641393, 'bagging_fraction': 0.8214961891062907, 'bagging_freq': 4}. Best is trial 13 with value: 7844.941802238107.\n",
      "[I 2025-07-25 20:47:14,890] Trial 28 finished with value: 7873.713510342843 and parameters: {'learning_rate': 0.027056118788756783, 'n_estimators': 1062, 'num_leaves': 13, 'lambda_l1': 21.40543072154886, 'lambda_l2': 0.1111979846669449, 'feature_fraction': 0.8852323313156735, 'bagging_fraction': 0.7370545729119238, 'bagging_freq': 5}. Best is trial 13 with value: 7844.941802238107.\n",
      "[I 2025-07-25 20:47:16,095] Trial 29 finished with value: 7884.25452319003 and parameters: {'learning_rate': 0.042066474643593525, 'n_estimators': 1726, 'num_leaves': 25, 'lambda_l1': 7.008741412145651, 'lambda_l2': 0.7121754709920421, 'feature_fraction': 0.9722418454929802, 'bagging_fraction': 0.9692948934309193, 'bagging_freq': 6}. Best is trial 13 with value: 7844.941802238107.\n",
      "[I 2025-07-25 20:47:17,475] Trial 30 finished with value: 7850.721653120062 and parameters: {'learning_rate': 0.02281299696897118, 'n_estimators': 1434, 'num_leaves': 17, 'lambda_l1': 0.2222478379225906, 'lambda_l2': 8.102185452300716, 'feature_fraction': 0.8452783420194467, 'bagging_fraction': 0.7787356994051661, 'bagging_freq': 4}. Best is trial 13 with value: 7844.941802238107.\n",
      "[I 2025-07-25 20:47:19,227] Trial 31 finished with value: 7858.161012572876 and parameters: {'learning_rate': 0.022789693724929058, 'n_estimators': 1449, 'num_leaves': 18, 'lambda_l1': 0.2136344931283219, 'lambda_l2': 8.343914607395199, 'feature_fraction': 0.8336986435688114, 'bagging_fraction': 0.7804590405856583, 'bagging_freq': 4}. Best is trial 13 with value: 7844.941802238107.\n",
      "[I 2025-07-25 20:47:21,093] Trial 32 finished with value: 7846.452182580229 and parameters: {'learning_rate': 0.018129361646585046, 'n_estimators': 1331, 'num_leaves': 13, 'lambda_l1': 0.5803581013342121, 'lambda_l2': 1.9738074842972242, 'feature_fraction': 0.849772845273161, 'bagging_fraction': 0.8167573479925002, 'bagging_freq': 5}. Best is trial 13 with value: 7844.941802238107.\n",
      "[I 2025-07-25 20:47:23,260] Trial 33 finished with value: 7851.991031158333 and parameters: {'learning_rate': 0.016699673324184, 'n_estimators': 1896, 'num_leaves': 14, 'lambda_l1': 0.39499009966482473, 'lambda_l2': 5.278832549057167, 'feature_fraction': 0.8211789011110713, 'bagging_fraction': 0.8270242201925329, 'bagging_freq': 5}. Best is trial 13 with value: 7844.941802238107.\n",
      "[I 2025-07-25 20:47:25,161] Trial 34 finished with value: 7861.321678495849 and parameters: {'learning_rate': 0.0189946841040151, 'n_estimators': 1310, 'num_leaves': 16, 'lambda_l1': 0.5809919331032185, 'lambda_l2': 2.258307663672395, 'feature_fraction': 0.8555129411795568, 'bagging_fraction': 0.8670341435204281, 'bagging_freq': 6}. Best is trial 13 with value: 7844.941802238107.\n",
      "[I 2025-07-25 20:47:27,620] Trial 35 finished with value: 7874.968650949201 and parameters: {'learning_rate': 0.01550287560077931, 'n_estimators': 1620, 'num_leaves': 21, 'lambda_l1': 0.2814452020660235, 'lambda_l2': 6.302279340336197, 'feature_fraction': 0.8584904757078677, 'bagging_fraction': 0.7854847813254084, 'bagging_freq': 3}. Best is trial 13 with value: 7844.941802238107.\n",
      "[I 2025-07-25 20:47:29,398] Trial 36 finished with value: 7880.470721552982 and parameters: {'learning_rate': 0.021779191339962468, 'n_estimators': 1449, 'num_leaves': 28, 'lambda_l1': 0.10706171425018261, 'lambda_l2': 3.092735459550783, 'feature_fraction': 0.8222049075859483, 'bagging_fraction': 0.7571576774476221, 'bagging_freq': 5}. Best is trial 13 with value: 7844.941802238107.\n",
      "[I 2025-07-25 20:47:30,961] Trial 37 finished with value: 7859.693924659258 and parameters: {'learning_rate': 0.019057542704133276, 'n_estimators': 1798, 'num_leaves': 18, 'lambda_l1': 0.20395766368167084, 'lambda_l2': 13.284594004707342, 'feature_fraction': 0.7638511321108736, 'bagging_fraction': 0.7251213211338408, 'bagging_freq': 5}. Best is trial 13 with value: 7844.941802238107.\n",
      "[I 2025-07-25 20:47:33,800] Trial 38 finished with value: 7847.697645491877 and parameters: {'learning_rate': 0.013571988583278002, 'n_estimators': 2343, 'num_leaves': 13, 'lambda_l1': 0.5580706613293877, 'lambda_l2': 1.4989218110449802, 'feature_fraction': 0.8120507967098908, 'bagging_fraction': 0.9630311026052074, 'bagging_freq': 5}. Best is trial 13 with value: 7844.941802238107.\n",
      "[I 2025-07-25 20:47:36,889] Trial 39 finished with value: 7856.1533003528075 and parameters: {'learning_rate': 0.014009804273475184, 'n_estimators': 2267, 'num_leaves': 13, 'lambda_l1': 2.9329607242980167, 'lambda_l2': 1.4972260142053126, 'feature_fraction': 0.8074435381476929, 'bagging_fraction': 0.9916342694298321, 'bagging_freq': 6}. Best is trial 13 with value: 7844.941802238107.\n",
      "[I 2025-07-25 20:47:41,009] Trial 40 finished with value: 7894.30583854912 and parameters: {'learning_rate': 0.011956741837230106, 'n_estimators': 2450, 'num_leaves': 36, 'lambda_l1': 1.0326176841957215, 'lambda_l2': 0.5436759656416471, 'feature_fraction': 0.7765134600434958, 'bagging_fraction': 0.952062025413451, 'bagging_freq': 6}. Best is trial 13 with value: 7844.941802238107.\n",
      "[I 2025-07-25 20:47:43,871] Trial 41 finished with value: 7864.631448683768 and parameters: {'learning_rate': 0.012610665341693325, 'n_estimators': 1148, 'num_leaves': 17, 'lambda_l1': 0.5677564179132083, 'lambda_l2': 3.401689888200277, 'feature_fraction': 0.8637342305720485, 'bagging_fraction': 0.8869295430801275, 'bagging_freq': 5}. Best is trial 13 with value: 7844.941802238107.\n",
      "[I 2025-07-25 20:47:45,025] Trial 42 finished with value: 7853.004864481441 and parameters: {'learning_rate': 0.03177160279023977, 'n_estimators': 2210, 'num_leaves': 12, 'lambda_l1': 1.6062285674303387, 'lambda_l2': 1.7801402760307996, 'feature_fraction': 0.8396488586640679, 'bagging_fraction': 0.9460447584153029, 'bagging_freq': 5}. Best is trial 13 with value: 7844.941802238107.\n",
      "[I 2025-07-25 20:47:46,787] Trial 43 finished with value: 7846.279616220912 and parameters: {'learning_rate': 0.017644057232908752, 'n_estimators': 1528, 'num_leaves': 13, 'lambda_l1': 0.6999492129981891, 'lambda_l2': 0.2594331785483803, 'feature_fraction': 0.8054905732302885, 'bagging_fraction': 0.7761189170720452, 'bagging_freq': 4}. Best is trial 13 with value: 7844.941802238107.\n",
      "[I 2025-07-25 20:47:48,995] Trial 44 finished with value: 7843.848734918456 and parameters: {'learning_rate': 0.017439191858558505, 'n_estimators': 2142, 'num_leaves': 13, 'lambda_l1': 0.8507164965084676, 'lambda_l2': 0.23296295423613425, 'feature_fraction': 0.8045075764242033, 'bagging_fraction': 0.7976648159544895, 'bagging_freq': 1}. Best is trial 44 with value: 7843.848734918456.\n",
      "[I 2025-07-25 20:47:51,170] Trial 45 finished with value: 7875.239379224068 and parameters: {'learning_rate': 0.017821014046208367, 'n_estimators': 2321, 'num_leaves': 26, 'lambda_l1': 0.7488249481816329, 'lambda_l2': 0.2252996371625252, 'feature_fraction': 0.7404256076470046, 'bagging_fraction': 0.8354105007973107, 'bagging_freq': 1}. Best is trial 44 with value: 7843.848734918456.\n",
      "[I 2025-07-25 20:47:54,218] Trial 46 finished with value: 7868.747857066798 and parameters: {'learning_rate': 0.014101745313522865, 'n_estimators': 2160, 'num_leaves': 20, 'lambda_l1': 2.3152686731745913, 'lambda_l2': 0.15588823367458404, 'feature_fraction': 0.8026363598465591, 'bagging_fraction': 0.8086941502655199, 'bagging_freq': 1}. Best is trial 44 with value: 7843.848734918456.\n",
      "[I 2025-07-25 20:47:56,556] Trial 47 finished with value: 7847.09493355923 and parameters: {'learning_rate': 0.016454304929032697, 'n_estimators': 1967, 'num_leaves': 13, 'lambda_l1': 0.4561366609724982, 'lambda_l2': 0.2525231979314143, 'feature_fraction': 0.7863853753923193, 'bagging_fraction': 0.8559023182864188, 'bagging_freq': 2}. Best is trial 44 with value: 7843.848734918456.\n",
      "[I 2025-07-25 20:47:58,880] Trial 48 finished with value: 7846.295523847684 and parameters: {'learning_rate': 0.016809952824315435, 'n_estimators': 1933, 'num_leaves': 15, 'lambda_l1': 0.36019393946222006, 'lambda_l2': 0.24206039027403836, 'feature_fraction': 0.7457439899798612, 'bagging_fraction': 0.8573851984012447, 'bagging_freq': 2}. Best is trial 44 with value: 7843.848734918456.\n",
      "[I 2025-07-25 20:48:01,003] Trial 49 finished with value: 7842.0534573387295 and parameters: {'learning_rate': 0.019558401236286698, 'n_estimators': 2095, 'num_leaves': 15, 'lambda_l1': 0.15571495159823545, 'lambda_l2': 0.10994317203466118, 'feature_fraction': 0.7364131537809377, 'bagging_fraction': 0.8740878978292693, 'bagging_freq': 1}. Best is trial 49 with value: 7842.0534573387295.\n",
      "[I 2025-07-25 20:48:03,764] Trial 50 finished with value: 7848.493602192062 and parameters: {'learning_rate': 0.019692643343567393, 'n_estimators': 2093, 'num_leaves': 16, 'lambda_l1': 0.14383467613364814, 'lambda_l2': 0.10152695370202061, 'feature_fraction': 0.7195971021404015, 'bagging_fraction': 0.8722878820201797, 'bagging_freq': 1}. Best is trial 49 with value: 7842.0534573387295.\n",
      "[I 2025-07-25 20:48:06,194] Trial 51 finished with value: 7845.229594159472 and parameters: {'learning_rate': 0.017675304977232346, 'n_estimators': 1844, 'num_leaves': 15, 'lambda_l1': 0.28679157700695623, 'lambda_l2': 0.1659548395706531, 'feature_fraction': 0.7434288884866292, 'bagging_fraction': 0.8822012256864413, 'bagging_freq': 2}. Best is trial 49 with value: 7842.0534573387295.\n",
      "[I 2025-07-25 20:48:07,886] Trial 52 finished with value: 7850.38570021738 and parameters: {'learning_rate': 0.020778521390551825, 'n_estimators': 1878, 'num_leaves': 20, 'lambda_l1': 0.14868042113152233, 'lambda_l2': 0.16153707133463577, 'feature_fraction': 0.7389460153694908, 'bagging_fraction': 0.8790766687231466, 'bagging_freq': 2}. Best is trial 49 with value: 7842.0534573387295.\n",
      "[I 2025-07-25 20:48:10,321] Trial 53 finished with value: 7845.9395711128045 and parameters: {'learning_rate': 0.015949421362153927, 'n_estimators': 2000, 'num_leaves': 15, 'lambda_l1': 0.2805344309822921, 'lambda_l2': 0.2620962949024039, 'feature_fraction': 0.7225319320539642, 'bagging_fraction': 0.909896881095749, 'bagging_freq': 1}. Best is trial 49 with value: 7842.0534573387295.\n",
      "[I 2025-07-25 20:48:12,978] Trial 54 finished with value: 7846.997002852493 and parameters: {'learning_rate': 0.015716120233269973, 'n_estimators': 2036, 'num_leaves': 18, 'lambda_l1': 0.268827016084287, 'lambda_l2': 0.3012933199205018, 'feature_fraction': 0.701306650503228, 'bagging_fraction': 0.9280489496372973, 'bagging_freq': 1}. Best is trial 49 with value: 7842.0534573387295.\n",
      "[I 2025-07-25 20:48:14,645] Trial 55 finished with value: 7840.336129115629 and parameters: {'learning_rate': 0.020571237612325412, 'n_estimators': 1795, 'num_leaves': 11, 'lambda_l1': 0.13447612881164278, 'lambda_l2': 0.14433384360343252, 'feature_fraction': 0.7235107649030099, 'bagging_fraction': 0.9133297339704415, 'bagging_freq': 1}. Best is trial 55 with value: 7840.336129115629.\n",
      "[I 2025-07-25 20:48:16,640] Trial 56 finished with value: 7840.803253994906 and parameters: {'learning_rate': 0.021112366163523494, 'n_estimators': 2112, 'num_leaves': 11, 'lambda_l1': 0.1104120368502704, 'lambda_l2': 0.14048106007629163, 'feature_fraction': 0.7260743191760992, 'bagging_fraction': 0.9089490800644918, 'bagging_freq': 1}. Best is trial 55 with value: 7840.336129115629.\n",
      "[I 2025-07-25 20:48:18,524] Trial 57 finished with value: 7844.250250000322 and parameters: {'learning_rate': 0.021248983824071355, 'n_estimators': 2131, 'num_leaves': 11, 'lambda_l1': 0.10799084312851893, 'lambda_l2': 0.1278798753198068, 'feature_fraction': 0.7537376794634219, 'bagging_fraction': 0.9317651097018209, 'bagging_freq': 1}. Best is trial 55 with value: 7840.336129115629.\n",
      "[I 2025-07-25 20:48:20,072] Trial 58 finished with value: 7841.7683456469995 and parameters: {'learning_rate': 0.02667343579983141, 'n_estimators': 2196, 'num_leaves': 11, 'lambda_l1': 0.10576376088669705, 'lambda_l2': 0.10431368782276695, 'feature_fraction': 0.7574979789534984, 'bagging_fraction': 0.8955127841333116, 'bagging_freq': 1}. Best is trial 55 with value: 7840.336129115629.\n",
      "[I 2025-07-25 20:48:22,055] Trial 59 finished with value: 7840.817724507494 and parameters: {'learning_rate': 0.021465112169373534, 'n_estimators': 2121, 'num_leaves': 11, 'lambda_l1': 0.10180970473841192, 'lambda_l2': 0.12328970885316845, 'feature_fraction': 0.7572388374850716, 'bagging_fraction': 0.9360614159311231, 'bagging_freq': 1}. Best is trial 55 with value: 7840.336129115629.\n",
      "[I 2025-07-25 20:48:24,086] Trial 60 finished with value: 7839.934330695066 and parameters: {'learning_rate': 0.023848269854353392, 'n_estimators': 2490, 'num_leaves': 11, 'lambda_l1': 0.17430810597067134, 'lambda_l2': 0.10103141832098816, 'feature_fraction': 0.7250257525561209, 'bagging_fraction': 0.8979102048347968, 'bagging_freq': 1}. Best is trial 60 with value: 7839.934330695066.\n",
      "[I 2025-07-25 20:48:25,607] Trial 61 finished with value: 7836.665471891065 and parameters: {'learning_rate': 0.023725793296471377, 'n_estimators': 2485, 'num_leaves': 11, 'lambda_l1': 0.166231997417782, 'lambda_l2': 0.1858887091966384, 'feature_fraction': 0.7259068442840229, 'bagging_fraction': 0.9003048674000318, 'bagging_freq': 1}. Best is trial 61 with value: 7836.665471891065.\n",
      "[I 2025-07-25 20:48:27,325] Trial 62 finished with value: 7839.992756886048 and parameters: {'learning_rate': 0.026645712657600667, 'n_estimators': 2492, 'num_leaves': 11, 'lambda_l1': 0.12515932069438615, 'lambda_l2': 0.18594288929255479, 'feature_fraction': 0.7247166634741306, 'bagging_fraction': 0.8938512269704801, 'bagging_freq': 1}. Best is trial 61 with value: 7836.665471891065.\n",
      "[I 2025-07-25 20:48:28,638] Trial 63 finished with value: 7833.066987804033 and parameters: {'learning_rate': 0.025632930257330388, 'n_estimators': 2465, 'num_leaves': 11, 'lambda_l1': 0.1127291451064576, 'lambda_l2': 0.18495405240388993, 'feature_fraction': 0.7226883413567464, 'bagging_fraction': 0.8940845919788092, 'bagging_freq': 1}. Best is trial 63 with value: 7833.066987804033.\n",
      "[I 2025-07-25 20:48:30,246] Trial 64 finished with value: 7845.835655123024 and parameters: {'learning_rate': 0.029709706563759857, 'n_estimators': 2482, 'num_leaves': 10, 'lambda_l1': 0.17222675742875254, 'lambda_l2': 0.1887585706816767, 'feature_fraction': 0.7159609203922128, 'bagging_fraction': 0.9090976510284463, 'bagging_freq': 1}. Best is trial 63 with value: 7833.066987804033.\n",
      "[I 2025-07-25 20:48:32,059] Trial 65 finished with value: 7843.077591056068 and parameters: {'learning_rate': 0.02359170585669566, 'n_estimators': 2416, 'num_leaves': 12, 'lambda_l1': 0.1234537901982967, 'lambda_l2': 0.13257162436724132, 'feature_fraction': 0.7287357364000782, 'bagging_fraction': 0.8922385383674714, 'bagging_freq': 2}. Best is trial 63 with value: 7833.066987804033.\n",
      "[I 2025-07-25 20:48:33,726] Trial 66 finished with value: 7840.203399117291 and parameters: {'learning_rate': 0.025249517159505522, 'n_estimators': 2359, 'num_leaves': 11, 'lambda_l1': 0.19636366329696517, 'lambda_l2': 0.18191966882776142, 'feature_fraction': 0.7036815056280656, 'bagging_fraction': 0.9216696380194308, 'bagging_freq': 1}. Best is trial 63 with value: 7833.066987804033.\n",
      "[I 2025-07-25 20:48:35,916] Trial 67 finished with value: 7901.007221027485 and parameters: {'learning_rate': 0.025727162569358242, 'n_estimators': 2500, 'num_leaves': 44, 'lambda_l1': 0.18544153555363, 'lambda_l2': 0.34922101523459337, 'feature_fraction': 0.7082219106095092, 'bagging_fraction': 0.9136571223281681, 'bagging_freq': 2}. Best is trial 63 with value: 7833.066987804033.\n",
      "[I 2025-07-25 20:48:37,200] Trial 68 finished with value: 7844.673631084614 and parameters: {'learning_rate': 0.02894386193979558, 'n_estimators': 2379, 'num_leaves': 10, 'lambda_l1': 0.2183538058228467, 'lambda_l2': 0.5043542211930269, 'feature_fraction': 0.7306201797467421, 'bagging_fraction': 0.9032692983253199, 'bagging_freq': 1}. Best is trial 63 with value: 7833.066987804033.\n",
      "[I 2025-07-25 20:48:38,753] Trial 69 finished with value: 7841.260890156091 and parameters: {'learning_rate': 0.0240795383887814, 'n_estimators': 2283, 'num_leaves': 12, 'lambda_l1': 0.13641069207522044, 'lambda_l2': 0.1881684142012174, 'feature_fraction': 0.7143775358317273, 'bagging_fraction': 0.9223722632493169, 'bagging_freq': 1}. Best is trial 63 with value: 7833.066987804033.\n",
      "[I 2025-07-25 20:48:40,770] Trial 70 finished with value: 7883.718874120917 and parameters: {'learning_rate': 0.02613296828474432, 'n_estimators': 2410, 'num_leaves': 35, 'lambda_l1': 0.18552327673053395, 'lambda_l2': 0.18315522717063717, 'feature_fraction': 0.7027744487803992, 'bagging_fraction': 0.9397893793811243, 'bagging_freq': 2}. Best is trial 63 with value: 7833.066987804033.\n",
      "[I 2025-07-25 20:48:42,422] Trial 71 finished with value: 7841.547340680799 and parameters: {'learning_rate': 0.0229300771277844, 'n_estimators': 2335, 'num_leaves': 11, 'lambda_l1': 0.12436238880131859, 'lambda_l2': 0.13199272787367697, 'feature_fraction': 0.7259368100697825, 'bagging_fraction': 0.9366399347927381, 'bagging_freq': 1}. Best is trial 63 with value: 7833.066987804033.\n",
      "[I 2025-07-25 20:48:44,040] Trial 72 finished with value: 7842.5884910878185 and parameters: {'learning_rate': 0.02501398973452808, 'n_estimators': 2247, 'num_leaves': 14, 'lambda_l1': 0.10009465445274453, 'lambda_l2': 0.14100257813250439, 'feature_fraction': 0.7701094904604431, 'bagging_fraction': 0.921267943725149, 'bagging_freq': 1}. Best is trial 63 with value: 7833.066987804033.\n",
      "[I 2025-07-25 20:48:45,540] Trial 73 finished with value: 7842.084122059062 and parameters: {'learning_rate': 0.022282392433797805, 'n_estimators': 2375, 'num_leaves': 11, 'lambda_l1': 0.16960192929909507, 'lambda_l2': 0.3101009399208353, 'feature_fraction': 0.7109303825856668, 'bagging_fraction': 0.9022200883764268, 'bagging_freq': 1}. Best is trial 63 with value: 7833.066987804033.\n",
      "[I 2025-07-25 20:48:47,500] Trial 74 finished with value: 7838.505059435494 and parameters: {'learning_rate': 0.020223053606948028, 'n_estimators': 2431, 'num_leaves': 10, 'lambda_l1': 0.222409191927723, 'lambda_l2': 0.2056652484159923, 'feature_fraction': 0.7326106030817728, 'bagging_fraction': 0.8903091486418637, 'bagging_freq': 1}. Best is trial 63 with value: 7833.066987804033.\n",
      "\n",
      "Meta-Model Tuning Complete.\n",
      "Best Lower Meta-Params: {'learning_rate': 0.028679521404211834, 'n_estimators': 1906, 'num_leaves': 10, 'lambda_l1': 0.33889627517299786, 'lambda_l2': 1.109183055065178, 'feature_fraction': 0.882146881986468, 'bagging_fraction': 0.8362369089503959, 'bagging_freq': 4}\n",
      "Best Upper Meta-Params: {'learning_rate': 0.025632930257330388, 'n_estimators': 2465, 'num_leaves': 11, 'lambda_l1': 0.1127291451064576, 'lambda_l2': 0.18495405240388993, 'feature_fraction': 0.7226883413567464, 'bagging_fraction': 0.8940845919788092, 'bagging_freq': 1}\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# BLOCK 14: TUNE META-MODELS WITH OPTUNA (Standard Quantile Objective)\n",
    "# =============================================================================\n",
    "# We will tune two separate LightGBM models on the meta-features. Instead of a\n",
    "# custom objective, we will use the stable, built-in 'quantile' objective.\n",
    "# - One model will be tuned to predict the 5th percentile (our new lower bound).\n",
    "# - One model will be tuned to predict the 95th percentile (our new upper bound).\n",
    "\n",
    "import lightgbm as lgb\n",
    "import optuna\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "\n",
    "print(\"\\n--- Preparing data for Meta-Model Optuna tuning ---\")\n",
    "# meta_features_train and y_meta_target should be available from the previous block\n",
    "X_train_opt, X_val_opt, y_train_opt, y_val_opt = train_test_split(\n",
    "    meta_features_train, y_meta_target, test_size=0.20, random_state=RANDOM_STATE\n",
    ")\n",
    "\n",
    "def objective_meta_quantile(trial, alpha_value):\n",
    "    \"\"\"Unified Optuna objective for the meta quantile models.\"\"\"\n",
    "    params = {\n",
    "        'objective': 'quantile',\n",
    "        'alpha': alpha_value,\n",
    "        'metric': 'quantile', # Pinball Loss is the correct metric for this objective\n",
    "        'random_state': RANDOM_STATE, 'n_jobs': -1, 'verbosity': -1, 'boosting_type': 'gbdt',\n",
    "        'learning_rate': trial.suggest_float('learning_rate', 0.01, 0.05, log=True),\n",
    "        'n_estimators': trial.suggest_int('n_estimators', 500, 2500),\n",
    "        'num_leaves': trial.suggest_int('num_leaves', 10, 50),\n",
    "        'lambda_l1': trial.suggest_float('lambda_l1', 0.1, 100.0, log=True),\n",
    "        'lambda_l2': trial.suggest_float('lambda_l2', 0.1, 100.0, log=True),\n",
    "        'feature_fraction': trial.suggest_float('feature_fraction', 0.7, 1.0),\n",
    "        'bagging_fraction': trial.suggest_float('bagging_fraction', 0.7, 1.0),\n",
    "        'bagging_freq': trial.suggest_int('bagging_freq', 1, 7),\n",
    "    }\n",
    "    model = lgb.LGBMRegressor(**params)\n",
    "    model.fit(X_train_opt, y_train_opt,\n",
    "              eval_set=[(X_val_opt, y_val_opt)],\n",
    "              callbacks=[lgb.early_stopping(50, verbose=False)])\n",
    "    \n",
    "    preds = model.predict(X_val_opt)\n",
    "    pinball_loss = np.mean(np.where(y_val_opt >= preds, \n",
    "                                    (y_val_opt - preds) * alpha_value, \n",
    "                                    (preds - y_val_opt) * (1 - alpha_value)))\n",
    "    return pinball_loss\n",
    "\n",
    "# --- Tune Lower-Bound Meta-Model (alpha=0.05) ---\n",
    "print(\"\\n--- Tuning the Lower-Bound Meta-Model (alpha=0.05)... ---\")\n",
    "study_meta_lower = optuna.create_study(direction='minimize')\n",
    "study_meta_lower.optimize(lambda trial: objective_meta_quantile(trial, 0.05), n_trials=75, show_progress_bar=True)\n",
    "best_params_meta_lower = study_meta_lower.best_params\n",
    "\n",
    "# --- Tune Upper-Bound Meta-Model (alpha=0.95) ---\n",
    "print(\"\\n--- Tuning the Upper-Bound Meta-Model (alpha=0.95)... ---\")\n",
    "study_meta_upper = optuna.create_study(direction='minimize')\n",
    "study_meta_upper.optimize(lambda trial: objective_meta_quantile(trial, 0.95), n_trials=75, show_progress_bar=True)\n",
    "best_params_meta_upper = study_meta_upper.best_params\n",
    "\n",
    "print(\"\\nMeta-Model Tuning Complete.\")\n",
    "print(f\"Best Lower Meta-Params: {best_params_meta_lower}\")\n",
    "print(f\"Best Upper Meta-Params: {best_params_meta_upper}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "038310b2-0d19-4196-a824-3fbd15d30b9f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Starting K-Fold training for final meta-models ---\n",
      "Meta-Model K-Fold 1/5...\n",
      "Meta-Model K-Fold 2/5...\n",
      "Meta-Model K-Fold 3/5...\n",
      "Meta-Model K-Fold 4/5...\n",
      "Meta-Model K-Fold 5/5...\n",
      "\n",
      "--- Saving final meta-model predictions... ---\n",
      "Final meta-model predictions saved successfully.\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# BLOCK 15: K-FOLD TRAINING FOR META-MODELS & SAVE PREDICTIONS\n",
    "# =============================================================================\n",
    "import lightgbm as lgb\n",
    "import numpy as np\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "import os\n",
    "import gc\n",
    "\n",
    "# Define a path to save the final meta-model predictions\n",
    "META_FINAL_PATH = './meta_final_models/'\n",
    "os.makedirs(META_FINAL_PATH, exist_ok=True)\n",
    "\n",
    "# Initialize prediction arrays\n",
    "oof_meta_lower = np.zeros(len(meta_features_train))\n",
    "test_meta_lower = np.zeros(len(meta_features_test))\n",
    "oof_meta_upper = np.zeros(len(meta_features_train))\n",
    "test_meta_upper = np.zeros(len(meta_features_test))\n",
    "\n",
    "skf = StratifiedKFold(n_splits=N_SPLITS, shuffle=True, random_state=RANDOM_STATE)\n",
    "\n",
    "print(\"\\n--- Starting K-Fold training for final meta-models ---\")\n",
    "for fold, (train_idx, val_idx) in enumerate(skf.split(meta_features_train, grade_for_stratify)):\n",
    "    print(f\"Meta-Model K-Fold {fold+1}/{N_SPLITS}...\")\n",
    "    X_train, X_val = meta_features_train.iloc[train_idx], meta_features_train.iloc[val_idx]\n",
    "    y_train, y_val = y_meta_target[train_idx], y_meta_target[val_idx]\n",
    "    \n",
    "    # --- Train and predict lower-bound meta-model ---\n",
    "    model_lower = lgb.LGBMRegressor(objective='quantile', alpha=0.05, **best_params_meta_lower)\n",
    "    model_lower.fit(X_train, y_train, eval_set=[(X_val, y_val)], callbacks=[lgb.early_stopping(50, verbose=False)])\n",
    "    oof_meta_lower[val_idx] = model_lower.predict(X_val)\n",
    "    test_meta_lower += model_lower.predict(meta_features_test) / N_SPLITS\n",
    "\n",
    "    # --- Train and predict upper-bound meta-model ---\n",
    "    model_upper = lgb.LGBMRegressor(objective='quantile', alpha=0.95, **best_params_meta_upper)\n",
    "    model_upper.fit(X_train, y_train, eval_set=[(X_val, y_val)], callbacks=[lgb.early_stopping(50, verbose=False)])\n",
    "    oof_meta_upper[val_idx] = model_upper.predict(X_val)\n",
    "    test_meta_upper += model_upper.predict(meta_features_test) / N_SPLITS\n",
    "    gc.collect()\n",
    "\n",
    "# --- Save the final prediction artifacts ---\n",
    "print(\"\\n--- Saving final meta-model predictions... ---\")\n",
    "np.save(os.path.join(META_FINAL_PATH, 'oof_meta_lower.npy'), oof_meta_lower)\n",
    "np.save(os.path.join(META_FINAL_PATH, 'test_meta_lower.npy'), test_meta_lower)\n",
    "np.save(os.path.join(META_FINAL_PATH, 'oof_meta_upper.npy'), oof_meta_upper)\n",
    "np.save(os.path.join(META_FINAL_PATH, 'test_meta_upper.npy'), test_meta_upper)\n",
    "print(\"Final meta-model predictions saved successfully.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "59e26397-9fc6-4e8c-82aa-7cbefe906e4f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Loading final predictions for calibration ---\n",
      "\n",
      "--- Performing final calibration on stacked predictions ---\n",
      "\n",
      "============================================================\n",
      "FINAL STACKED META-MODEL RESULTS\n",
      "============================================================\n",
      "Previous Best Blended Score: $291,785.50\n",
      "Final STACKED OOF Winkler Score: $293,184.57\n",
      "Optimal Calibration Multipliers: a=0.9988, b=1.0015\n",
      "------------------------------------------------------------\n",
      "\n",
      "'submission_FINAL_STACKED_293184.csv' created successfully! This is your definitive submission.\n",
      "\n",
      "Final Submission Head:\n",
      "       id       pi_lower      pi_upper\n",
      "0  200000  806353.792926  1.008121e+06\n",
      "1  200001  586240.829410  8.020544e+05\n",
      "2  200002  452492.466350  6.478229e+05\n",
      "3  200003  293412.479786  4.170843e+05\n",
      "4  200004  391745.367117  8.192224e+05\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# BLOCK 16: FINAL CALIBRATION AND SUBMISSION\n",
    "# =============================================================================\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy.optimize import minimize\n",
    "import os\n",
    "\n",
    "# Helper function\n",
    "def winkler_score(y_true, lower, upper, alpha=0.1):\n",
    "    width = upper - lower\n",
    "    penalty_lower = np.where(y_true < lower, (2 / alpha) * (lower - y_true), 0)\n",
    "    penalty_upper = np.where(y_true > upper, (2 / alpha) * (y_true - upper), 0)\n",
    "    return np.mean(width + penalty_lower + penalty_upper)\n",
    "\n",
    "# --- Step 1: Load final predictions and true labels ---\n",
    "print(\"\\n--- Loading final predictions for calibration ---\")\n",
    "META_FINAL_PATH = './meta_final_models/'\n",
    "y_true = pd.read_csv('./dataset.csv')['sale_price'].values\n",
    "\n",
    "oof_lower = np.load(os.path.join(META_FINAL_PATH, 'oof_meta_lower.npy'))\n",
    "test_lower = np.load(os.path.join(META_FINAL_PATH, 'test_meta_lower.npy'))\n",
    "oof_upper = np.load(os.path.join(META_FINAL_PATH, 'oof_meta_upper.npy'))\n",
    "test_upper = np.load(os.path.join(META_FINAL_PATH, 'test_meta_upper.npy'))\n",
    "\n",
    "# --- Step 2: Final Calibration ---\n",
    "print(\"\\n--- Performing final calibration on stacked predictions ---\")\n",
    "def get_calibrated_winkler(multipliers, y_true_oof, lower_oof, upper_oof):\n",
    "    a, b = multipliers\n",
    "    lower_raw = np.minimum(lower_oof, upper_oof)\n",
    "    upper_raw = np.maximum(lower_oof, upper_oof)\n",
    "    return winkler_score(y_true_oof, lower_raw * a, upper_raw * b)\n",
    "\n",
    "res_calib = minimize(\n",
    "    fun=get_calibrated_winkler, x0=[1.0, 1.0], \n",
    "    args=(y_true, oof_lower, oof_upper),\n",
    "    method='L-BFGS-B', bounds=[(0.8, 1.2), (0.8, 1.2)]\n",
    ")\n",
    "best_a, best_b = res_calib.x\n",
    "final_score = res_calib.fun\n",
    "\n",
    "# --- Step 3: Display Results and Create Submission ---\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"FINAL STACKED META-MODEL RESULTS\")\n",
    "print(\"=\"*60)\n",
    "print(f\"Previous Best Blended Score: $291,785.50\")\n",
    "print(f\"Final STACKED OOF Winkler Score: ${final_score:,.2f}\")\n",
    "print(f\"Optimal Calibration Multipliers: a={best_a:.4f}, b={best_b:.4f}\")\n",
    "print(\"-\" * 60)\n",
    "\n",
    "# Apply calibration and enforce non-crossing\n",
    "final_test_lower = np.minimum(test_lower, test_upper) * best_a\n",
    "final_test_upper = np.maximum(test_lower, test_upper) * best_b\n",
    "final_test_upper = np.maximum(final_test_lower + 1, final_test_upper)\n",
    "\n",
    "# Load IDs for submission\n",
    "df_test_raw = pd.read_csv('./test.csv')\n",
    "submission_df_final = pd.DataFrame({\n",
    "    'id': df_test_raw['id'],\n",
    "    'pi_lower': final_test_lower,\n",
    "    'pi_upper': final_test_upper\n",
    "})\n",
    "\n",
    "submission_filename = f'submission_FINAL_STACKED_{int(final_score)}.csv'\n",
    "submission_df_final.to_csv(submission_filename, index=False)\n",
    "\n",
    "print(f\"\\n'{submission_filename}' created successfully! This is your definitive submission.\")\n",
    "print(\"\\nFinal Submission Head:\")\n",
    "print(submission_df_final.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e6a1b2ed-e304-4a00-ab91-fa2d3bb84334",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Definitive 2-submission blend created successfully.\n",
      "       id       pi_lower      pi_upper\n",
      "0  200000  810199.059449  1.017671e+06\n",
      "1  200001  588965.221580  8.062583e+05\n",
      "2  200002  454997.291877  6.532279e+05\n",
      "3  200003  293889.746395  4.211828e+05\n",
      "4  200004  388572.426359  8.320829e+05\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Load your two best submission files\n",
    "df_blend = pd.read_csv('submission_FINAL_3M_BLEND_291785.csv')\n",
    "df_stacked = pd.read_csv('submission_FINAL_STACKED_293087.csv')\n",
    "\n",
    "# Create the final ensemble with a simple 50/50 average\n",
    "final_submission = pd.DataFrame()\n",
    "final_submission['id'] = df_blend['id']\n",
    "final_submission['pi_lower'] = 0.5 * df_blend['pi_lower'] + 0.5 * df_stacked['pi_lower']\n",
    "final_submission['pi_upper'] = 0.5 * df_blend['pi_upper'] + 0.5 * df_stacked['pi_upper']\n",
    "\n",
    "# Final sanity check\n",
    "final_submission['pi_upper'] = np.maximum(final_submission['pi_lower'] + 1, final_submission['pi_upper'])\n",
    "\n",
    "# Save the definitive submission\n",
    "final_submission.to_csv('submission_DEFINITIVE_BLEND_291k.csv', index=False)\n",
    "\n",
    "print(\"Definitive 2-submission blend created successfully.\")\n",
    "print(final_submission.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "335fddf3-c89a-46ff-8cb3-bb30507f8cf2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (Kaggle Comp)",
   "language": "python",
   "name": "kaggle-comp"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

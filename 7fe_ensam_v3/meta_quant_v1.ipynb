{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d33a4648-2943-40a8-8af7-ae50570d8d81",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Libraries imported successfully.\n",
      "Raw data loaded successfully.\n",
      "Setup complete.\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# BLOCK 1: SETUP, IMPORTS, AND DATA LOADING\n",
    "# =============================================================================\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "import time\n",
    "import os\n",
    "# --- Library Imports ---\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import gc\n",
    "from sklearn.model_selection import StratifiedKFold, train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import xgboost as xgb\n",
    "import catboost as cb\n",
    "import optuna\n",
    "from scipy.optimize import minimize\n",
    "print(\"Libraries imported successfully.\")\n",
    "# --- Helper Function for Winkler Score ---\n",
    "def winkler_score(y_true, lower, upper, alpha=0.1, return_coverage=False):\n",
    "    width = upper - lower\n",
    "    penalty_lower = np.where(y_true < lower, (2 / alpha) * (lower - y_true), 0)\n",
    "    penalty_upper = np.where(y_true > upper, (2 / alpha) * (y_true - upper), 0)\n",
    "    score = width + penalty_lower + penalty_upper\n",
    "    if return_coverage:\n",
    "        coverage = np.mean((y_true >= lower) & (y_true <= upper))\n",
    "        return np.mean(score), coverage\n",
    "    return np.mean(score)\n",
    "# --- Global Constants ---\n",
    "N_SPLITS = 5\n",
    "RANDOM_STATE = 42\n",
    "DATA_PATH = './'\n",
    "N_OPTUNA_TRIALS = 30 # A strong number for a comprehensive search\n",
    "COMPETITION_ALPHA = 0.1\n",
    "\n",
    "# --- Load Raw Data ---\n",
    "try:\n",
    "    # We drop the low-variance columns they identified right away\n",
    "    drop_cols=['id', 'golf', 'view_rainier', 'view_skyline', 'view_lakesamm','view_otherwater', 'view_other']\n",
    "    df_train = pd.read_csv(DATA_PATH + 'dataset.csv').drop(columns=drop_cols)\n",
    "    df_test = pd.read_csv(DATA_PATH + 'test.csv').drop(columns=drop_cols)\n",
    "    print(\"Raw data loaded successfully.\")\n",
    "except FileNotFoundError:\n",
    "    print(\"ERROR: Could not find 'dataset.csv' or 'test.csv'.\")\n",
    "    exit()\n",
    "# --- Prepare Target Variable ---\n",
    "y_true = df_train['sale_price'].copy()\n",
    "grade_for_stratify = df_train['grade'].copy()\n",
    "# The mean-error model works best when predicting the raw price directly\n",
    "# So, we will NOT log-transform the target this time.\n",
    "# df_train.drop('sale_price', axis=1, inplace=True) # We keep sale_price for FE\n",
    "print(\"Setup complete.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3a94b99e-758e-4790-9a36-e704094fb922",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Starting Block 2.5: Executing Feature Engineering Pipeline ---\n",
      "--- Starting Comprehensive Feature Engineering ---\n",
      "Step 1: Creating brute-force numerical interaction features...\n",
      "Step 2: Creating date features...\n",
      "Step 3: Creating TF-IDF features for text columns...\n",
      "Step 4: Creating group-by aggregation features...\n",
      "Step 5: Creating ratio features...\n",
      "Step 6: Creating geospatial clustering features...\n",
      "Step 7: Finalizing feature set...\n",
      "\n",
      "Comprehensive FE complete. Total features: 233\n",
      "Feature engineering complete. X shape: (200000, 233), X_test shape: (200000, 233)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Make sure to have these libraries installed\n",
    "# pip install pandas numpy scikit-learn\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.cluster import KMeans\n",
    "import gc\n",
    "\n",
    "# Define a random state for reproducibility\n",
    "RANDOM_STATE = 42\n",
    "\n",
    "def create_comprehensive_features(df_train, df_test):\n",
    "    \"\"\"\n",
    "    Combines original and new advanced feature engineering steps into a single pipeline.\n",
    "    \"\"\"\n",
    "    print(\"--- Starting Comprehensive Feature Engineering ---\")\n",
    "\n",
    "    # Store original indices and target variable\n",
    "    train_ids = df_train.index\n",
    "    test_ids = df_test.index\n",
    "    y_train = df_train['sale_price'].copy() # Keep the target separate\n",
    "\n",
    "    # Combine for consistent processing\n",
    "    df_train_temp = df_train.drop(columns=['sale_price'])\n",
    "    all_data = pd.concat([df_train_temp, df_test], axis=0, ignore_index=True)\n",
    "\n",
    "    # --- Original Feature Engineering ---\n",
    "\n",
    "    # A) Brute-Force Numerical Interactions\n",
    "    print(\"Step 1: Creating brute-force numerical interaction features...\")\n",
    "    NUMS = ['area', 'land_val', 'imp_val', 'sqft_lot', 'sqft', 'sqft_1', 'grade', 'year_built']\n",
    "    # Ensure all columns exist and are numeric, fill missing with 0 for safety\n",
    "    for col in NUMS:\n",
    "        if col not in all_data.columns:\n",
    "            all_data[col] = 0\n",
    "        else:\n",
    "            all_data[col] = pd.to_numeric(all_data[col], errors='coerce').fillna(0)\n",
    "            \n",
    "    for i in range(len(NUMS)):\n",
    "        for j in range(i + 1, len(NUMS)):\n",
    "            all_data[f'{NUMS[i]}_x_{NUMS[j]}'] = all_data[NUMS[i]] * all_data[NUMS[j]]\n",
    "\n",
    "    # B) Date Features\n",
    "    print(\"Step 2: Creating date features...\")\n",
    "    all_data['sale_date'] = pd.to_datetime(all_data['sale_date'])\n",
    "    all_data['sale_year'] = all_data['sale_date'].dt.year\n",
    "    all_data['sale_month'] = all_data['sale_date'].dt.month\n",
    "    all_data['sale_dayofyear'] = all_data['sale_date'].dt.dayofyear\n",
    "    all_data['age_at_sale'] = all_data['sale_year'] - all_data['year_built']\n",
    "\n",
    "    # C) TF-IDF Text Features\n",
    "    print(\"Step 3: Creating TF-IDF features for text columns...\")\n",
    "    text_cols = ['subdivision', 'zoning', 'city', 'sale_warning', 'join_status', 'submarket']\n",
    "    all_data[text_cols] = all_data[text_cols].fillna('missing').astype(str)\n",
    "    \n",
    "    for col in text_cols:\n",
    "        tfidf = TfidfVectorizer(analyzer='char', ngram_range=(3, 5), max_features=128, binary=True)\n",
    "        svd = TruncatedSVD(n_components=8, random_state=RANDOM_STATE)\n",
    "        \n",
    "        tfidf_matrix = tfidf.fit_transform(all_data[col])\n",
    "        tfidf_svd = svd.fit_transform(tfidf_matrix)\n",
    "        \n",
    "        tfidf_df = pd.DataFrame(tfidf_svd, columns=[f'{col}_tfidf_svd_{i}' for i in range(8)])\n",
    "        all_data = pd.concat([all_data, tfidf_df], axis=1)\n",
    "\n",
    "    # D) Log transform some interaction features\n",
    "    for c in ['land_val_x_imp_val', 'land_val_x_sqft', 'imp_val_x_sqft']:\n",
    "        if c in all_data.columns:\n",
    "            all_data[c] = np.log1p(all_data[c].fillna(0))\n",
    "\n",
    "    # --- New Feature Engineering Ideas ---\n",
    "\n",
    "    # F) Group-By Aggregation Features\n",
    "    print(\"Step 4: Creating group-by aggregation features...\")\n",
    "    group_cols = ['submarket', 'city', 'zoning']\n",
    "    num_cols_for_agg = ['grade', 'sqft', 'imp_val', 'land_val', 'age_at_sale']\n",
    "\n",
    "    for group_col in group_cols:\n",
    "        for num_col in num_cols_for_agg:\n",
    "            agg_stats = all_data.groupby(group_col)[num_col].agg(['mean', 'std', 'max', 'min']).reset_index()\n",
    "            agg_stats.columns = [group_col] + [f'{group_col}_{num_col}_{stat}' for stat in ['mean', 'std', 'max', 'min']]\n",
    "            all_data = pd.merge(all_data, agg_stats, on=group_col, how='left')\n",
    "            all_data[f'{num_col}_minus_{group_col}_mean'] = all_data[num_col] - all_data[f'{group_col}_{num_col}_mean']\n",
    "\n",
    "    # G) Ratio Features\n",
    "    print(\"Step 5: Creating ratio features...\")\n",
    "    # Add a small epsilon to prevent division by zero\n",
    "    epsilon = 1e-6 \n",
    "    all_data['total_val'] = all_data['imp_val'] + all_data['land_val']\n",
    "    all_data['imp_val_to_land_val_ratio'] = all_data['imp_val'] / (all_data['land_val'] + epsilon)\n",
    "    all_data['land_val_ratio'] = all_data['land_val'] / (all_data['total_val'] + epsilon)\n",
    "    all_data['sqft_to_lot_ratio'] = all_data['sqft'] / (all_data['sqft_lot'] + epsilon)\n",
    "    all_data['was_renovated'] = (all_data['year_reno'] > 0).astype(int)\n",
    "    all_data['reno_age_at_sale'] = np.where(all_data['was_renovated'] == 1, all_data['sale_year'] - all_data['year_reno'], -1)\n",
    "\n",
    "    # H) Geospatial Clustering Features\n",
    "    print(\"Step 6: Creating geospatial clustering features...\")\n",
    "    coords = all_data[['latitude', 'longitude']].copy()\n",
    "    coords.fillna(coords.median(), inplace=True) # Simple imputation\n",
    "\n",
    "    # KMeans is sensitive to feature scaling, but for lat/lon it's often okay without it.\n",
    "    kmeans = KMeans(n_clusters=20, random_state=RANDOM_STATE, n_init=10) \n",
    "    all_data['location_cluster'] = kmeans.fit_predict(coords)\n",
    "    \n",
    "    # Calculate distance to each cluster center\n",
    "    cluster_centers = kmeans.cluster_centers_\n",
    "    for i in range(len(cluster_centers)):\n",
    "        center = cluster_centers[i]\n",
    "        all_data[f'dist_to_cluster_{i}'] = np.sqrt((coords['latitude'] - center[0])**2 + (coords['longitude'] - center[1])**2)\n",
    "\n",
    "    # --- Final Cleanup ---\n",
    "    print(\"Step 7: Finalizing feature set...\")\n",
    "    cols_to_drop = ['sale_date', 'subdivision', 'zoning', 'city', 'sale_warning', 'join_status', 'submarket']\n",
    "    all_data = all_data.drop(columns=cols_to_drop)\n",
    "\n",
    "    # One-hot encode the new cluster feature\n",
    "    all_data = pd.get_dummies(all_data, columns=['location_cluster'], prefix='loc_cluster')\n",
    "    \n",
    "    # Final check for any remaining object columns to be safe (besides index)\n",
    "    object_cols = all_data.select_dtypes(include='object').columns\n",
    "    if len(object_cols) > 0:\n",
    "        print(f\"Warning: Found unexpected object columns: {object_cols}. Dropping them.\")\n",
    "        all_data = all_data.drop(columns=object_cols)\n",
    "        \n",
    "    all_data.fillna(0, inplace=True)\n",
    "\n",
    "    # Separate back into train and test sets\n",
    "    train_len = len(train_ids)\n",
    "    X = all_data.iloc[:train_len].copy()\n",
    "    X_test = all_data.iloc[train_len:].copy()\n",
    "    \n",
    "    # Restore original indices\n",
    "    X.index = train_ids\n",
    "    X_test.index = test_ids\n",
    "    \n",
    "    # Align columns - crucial for model prediction\n",
    "    X_test = X_test[X.columns]\n",
    "    \n",
    "    print(f\"\\nComprehensive FE complete. Total features: {X.shape[1]}\")\n",
    "    gc.collect()\n",
    "    \n",
    "    return X, X_test, y_train\n",
    "# =============================================================================\n",
    "# BLOCK 2.5: EXECUTE FEATURE ENGINEERING\n",
    "# =============================================================================\n",
    "print(\"\\n--- Starting Block 2.5: Executing Feature Engineering Pipeline ---\")\n",
    "\n",
    "# This is the crucial step that was missing.\n",
    "# We call the function to create our training and testing dataframes.\n",
    "X, X_test, y_train = create_comprehensive_features(df_train, df_test)\n",
    "\n",
    "# Let's verify the output\n",
    "print(f\"Feature engineering complete. X shape: {X.shape}, X_test shape: {X_test.shape}\")\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1fc25c52-2f59-4ab9-9878-c98a2b97e3de",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Loading all base model predictions from saved .npy files... ---\n",
      "All MEAN AND ERROR models predictions loaded successfully.\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# BLOCK 3: LOAD ALL PRE-TRAINED MODEL PREDICTIONS\n",
    "# =============================================================================\n",
    "\n",
    "# Define paths to your saved prediction files\n",
    "PREDS_SAVE_PATH = './mean_models_v1/' # For XGB and CatBoost preds\n",
    "NN_PREDS_PATH = './NN_model_predictions/' # For NN preds\n",
    "ERR_PATH = './error_models/' # For error preds\n",
    "\n",
    "print(\"--- Loading all base model predictions from saved .npy files... ---\")\n",
    "try:\n",
    "    # Load Mean Model OOF (Out-of-Fold) Predictions\n",
    "    oof_xgb_preds = np.load(os.path.join(PREDS_SAVE_PATH, 'oof_xgb_preds.npy'))\n",
    "    oof_cb_preds = np.load(os.path.join(PREDS_SAVE_PATH, 'oof_cb_preds.npy'))\n",
    "    oof_lgbm_preds = np.load(os.path.join(PREDS_SAVE_PATH, 'oof_lgbm_preds.npy'))\n",
    "    oof_nn_preds = np.load(os.path.join(NN_PREDS_PATH, 'oof_nn_preds.npy'))\n",
    "\n",
    "    oof_error_preds_cb = np.load(os.path.join(ERR_PATH, 'oof_error_preds_cb.npy'))\n",
    "    oof_error_preds_lgbm = np.load(os.path.join(ERR_PATH, 'oof_error_preds_lgbm.npy'))\n",
    "    oof_error_preds_xgb = np.load(os.path.join(ERR_PATH, 'oof_error_preds_xgb.npy'))\n",
    "    \n",
    "    # Load Mean Model Test Predictions\n",
    "    test_xgb_preds = np.load(os.path.join(PREDS_SAVE_PATH, 'test_xgb_preds.npy'))\n",
    "    test_cb_preds = np.load(os.path.join(PREDS_SAVE_PATH, 'test_cb_preds.npy'))\n",
    "    test_lgbm_preds = np.load(os.path.join(PREDS_SAVE_PATH, 'test_lgbm_preds.npy'))\n",
    "    test_nn_preds = np.load(os.path.join(NN_PREDS_PATH, 'test_nn_preds.npy'))\n",
    "\n",
    "    test_error_preds_cb = np.load(os.path.join(ERR_PATH, 'test_error_preds_cb.npy'))\n",
    "    test_error_preds_lgbm = np.load(os.path.join(ERR_PATH, 'test_error_preds_lgbm.npy'))\n",
    "    test_error_preds_xgb = np.load(os.path.join(ERR_PATH, 'test_error_preds_xgb.npy'))\n",
    "\n",
    "     \n",
    "    \n",
    "    print(\"All MEAN AND ERROR models predictions loaded successfully.\")\n",
    "    \n",
    "except FileNotFoundError as e:\n",
    "    print(f\"\\nERROR: Could not find a required prediction file. {e}\")\n",
    "    print(\"Please ensure you have run all training notebooks and saved their predictions first.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e8c81f2b-cce5-4d7b-b3a4-f09551d44b2d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Training a simple model to determine feature importance... ---\n",
      "Feature importance ranking created.\n",
      "Top 5 features:\n",
      "                   Feature    Importance\n",
      "0  submarket_land_val_mean  5.867908e+13\n",
      "1                total_val  4.830099e+13\n",
      "2                    grade  2.531556e+13\n",
      "3                join_year  2.396559e+13\n",
      "4     submarket_grade_mean  1.996464e+13\n",
      "\n",
      "--- Building an ELITE feature set to reduce noise and stack predictions ---\n",
      "Selected the top 50 raw features.\n",
      "\n",
      "Elite feature set for quantile models created.\n",
      "Final Shape: (200000, 57)\n",
      "Total features include: 50 raw + 4 mean preds + 3 error preds = 57\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "781"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# BLOCK 5.5: BUILD ELITE FEATURE SET FOR QUANTILE MODELS\n",
    "# =============================================================================\n",
    "# This block creates the definitive feature set that will be used to train the\n",
    "# new quantile models. It's a \"stacked\" feature set, combining:\n",
    "# 1. The most important raw features (to reduce noise).\n",
    "# 2. The predictions from the Stage 1 Mean Ensemble (to know the central tendency).\n",
    "# 3. The predictions from the Stage 2 Error Ensemble (to understand the uncertainty).\n",
    "# This gives the quantile models maximum context.\n",
    "\n",
    "# --- Step 1: Generate Feature Importance Ranking ---\n",
    "# We'll train a quick XGBoost model on the full raw feature set ('X') to get a\n",
    "# robust ranking of feature importance based on 'gain'.\n",
    "print(\"\\n--- Training a simple model to determine feature importance... ---\")\n",
    "\n",
    "# For performance, we can use a subset of data for importance calculation if needed\n",
    "# X_sample, _, y_sample, _ = train_test_split(X, y_true, test_size=0.5, random_state=RANDOM_STATE)\n",
    "# dtrain_importance = xgb.DMatrix(X_sample, label=y_sample)\n",
    "dtrain_importance = xgb.DMatrix(X, label=y_true) # Using full data for best accuracy\n",
    "\n",
    "params = {\n",
    "    'objective': 'reg:squarederror',\n",
    "    'eval_metric': 'rmse',\n",
    "    'seed': RANDOM_STATE,\n",
    "    'n_jobs': -1\n",
    "}\n",
    "bst_for_importance = xgb.train(\n",
    "    params,\n",
    "    dtrain_importance,\n",
    "    num_boost_round=500, # A reasonable number of rounds\n",
    "    verbose_eval=False\n",
    ")\n",
    "\n",
    "importance_scores = bst_for_importance.get_score(importance_type='gain')\n",
    "feature_importance = pd.DataFrame({\n",
    "    'Feature': importance_scores.keys(),\n",
    "    'Importance': importance_scores.values()\n",
    "}).sort_values(by='Importance', ascending=False).reset_index(drop=True)\n",
    "\n",
    "print(\"Feature importance ranking created.\")\n",
    "print(\"Top 5 features:\")\n",
    "print(feature_importance.head())\n",
    "\n",
    "\n",
    "# --- Step 2: Build the ELITE Feature Set ---\n",
    "# Now, we construct the final feature set for our quantile models.\n",
    "print(\"\\n--- Building an ELITE feature set to reduce noise and stack predictions ---\")\n",
    "\n",
    "# Define how many of the top raw features to include\n",
    "N_TOP_FEATURES = 50\n",
    "elite_raw_features = feature_importance['Feature'].head(N_TOP_FEATURES).tolist()\n",
    "print(f\"Selected the top {N_TOP_FEATURES} raw features.\")\n",
    "\n",
    "# --- Create the training set for the quantile models (X_for_quantile) ---\n",
    "# Start with the elite raw features\n",
    "X_for_quantile = X[elite_raw_features].copy()\n",
    "\n",
    "# Add the Stage 1 MEAN model predictions (OOF)\n",
    "X_for_quantile['oof_mean_xgb'] = oof_xgb_preds\n",
    "X_for_quantile['oof_mean_cb'] = oof_cb_preds\n",
    "X_for_quantile['oof_mean_lgbm'] = oof_lgbm_preds\n",
    "X_for_quantile['oof_mean_nn'] = oof_nn_preds\n",
    "\n",
    "# Add the Stage 2 ERROR model predictions (OOF) - THIS IS THE NEW ADDITION\n",
    "X_for_quantile['oof_error_xgb'] = oof_error_preds_xgb\n",
    "X_for_quantile['oof_error_cb'] = oof_error_preds_cb\n",
    "X_for_quantile['oof_error_lgbm'] = oof_error_preds_lgbm\n",
    "\n",
    "# --- Create the test set for the quantile models (X_test_for_quantile) ---\n",
    "# It is CRUCIAL that the test set has the exact same features in the same order.\n",
    "X_test_for_quantile = X_test[elite_raw_features].copy()\n",
    "\n",
    "# Add the Stage 1 MEAN model predictions (test)\n",
    "X_test_for_quantile['oof_mean_xgb'] = test_xgb_preds\n",
    "X_test_for_quantile['oof_mean_cb'] = test_cb_preds\n",
    "X_test_for_quantile['oof_mean_lgbm'] = test_lgbm_preds\n",
    "X_test_for_quantile['oof_mean_nn'] = test_nn_preds\n",
    "\n",
    "# Add the Stage 2 ERROR model predictions (test)\n",
    "X_test_for_quantile['oof_error_xgb'] = test_error_preds_xgb\n",
    "X_test_for_quantile['oof_error_cb'] = test_error_preds_cb\n",
    "X_test_for_quantile['oof_error_lgbm'] = test_error_preds_lgbm\n",
    "\n",
    "\n",
    "# Final check and report\n",
    "# Ensure columns are aligned\n",
    "X_test_for_quantile = X_test_for_quantile[X_for_quantile.columns]\n",
    "\n",
    "print(f\"\\nElite feature set for quantile models created.\")\n",
    "print(f\"Final Shape: {X_for_quantile.shape}\")\n",
    "print(f\"Total features include: {N_TOP_FEATURES} raw + 4 mean preds + 3 error preds = {X_for_quantile.shape[1]}\")\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "038310b2-0d19-4196-a824-3fbd15d30b9f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-07-25 18:54:00,688] A new study created in memory with name: no-name-5041c12b-3691-4643-9384-faba761566df\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Preparing data for Optuna tuning ---\n",
      "Data split for tuning: 160000 train, 40000 validation samples.\n",
      "\n",
      "--- Tuning the XGBoost Lower-Bound Model (alpha=0.05)... ---\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "493ab4cb805f48caa7b133767407c185",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/50 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-07-25 18:54:06,948] Trial 0 finished with value: 6853.092056820176 and parameters: {'eta': 0.06774242455930102, 'max_depth': 5, 'subsample': 0.9297907401975716, 'colsample_bytree': 0.723331642560421, 'lambda': 0.015026612570275745, 'alpha': 17.44138634257978, 'min_child_weight': 3}. Best is trial 0 with value: 6853.092056820176.\n",
      "[I 2025-07-25 18:54:16,767] Trial 1 finished with value: 6876.910690848608 and parameters: {'eta': 0.032023353865626505, 'max_depth': 8, 'subsample': 0.9221149321683481, 'colsample_bytree': 0.6807156313368463, 'lambda': 2.1625499967817516, 'alpha': 0.08197922179369121, 'min_child_weight': 15}. Best is trial 0 with value: 6853.092056820176.\n",
      "[I 2025-07-25 18:54:22,697] Trial 2 finished with value: 6895.8986685040745 and parameters: {'eta': 0.03702676635881892, 'max_depth': 8, 'subsample': 0.7522119784199633, 'colsample_bytree': 0.948023536009083, 'lambda': 31.22100518074038, 'alpha': 4.184244987671205, 'min_child_weight': 2}. Best is trial 0 with value: 6853.092056820176.\n",
      "[I 2025-07-25 18:54:39,431] Trial 3 finished with value: 6820.4623786033335 and parameters: {'eta': 0.011959757045163303, 'max_depth': 6, 'subsample': 0.7099696193169207, 'colsample_bytree': 0.7013395718675207, 'lambda': 0.03317074034547487, 'alpha': 0.012235653393010248, 'min_child_weight': 14}. Best is trial 3 with value: 6820.4623786033335.\n",
      "[I 2025-07-25 18:54:50,160] Trial 4 finished with value: 6861.319794603159 and parameters: {'eta': 0.015952645896677196, 'max_depth': 7, 'subsample': 0.6050493662485875, 'colsample_bytree': 0.8355371694182234, 'lambda': 0.6257815465185146, 'alpha': 0.9864558831293988, 'min_child_weight': 8}. Best is trial 3 with value: 6820.4623786033335.\n",
      "[I 2025-07-25 18:54:58,982] Trial 5 finished with value: 6826.430521487466 and parameters: {'eta': 0.019941443133370353, 'max_depth': 6, 'subsample': 0.6142471513124251, 'colsample_bytree': 0.8761191443779256, 'lambda': 0.43299651496969904, 'alpha': 0.027569863615324694, 'min_child_weight': 10}. Best is trial 3 with value: 6820.4623786033335.\n",
      "[I 2025-07-25 18:55:04,227] Trial 6 finished with value: 6882.427692272756 and parameters: {'eta': 0.09750350885743833, 'max_depth': 3, 'subsample': 0.6849989595732329, 'colsample_bytree': 0.8948547857215915, 'lambda': 0.9254536951717143, 'alpha': 26.774197750758507, 'min_child_weight': 12}. Best is trial 3 with value: 6820.4623786033335.\n",
      "[I 2025-07-25 18:55:10,538] Trial 7 finished with value: 6846.8350623532415 and parameters: {'eta': 0.03699418514633305, 'max_depth': 7, 'subsample': 0.8881149546379683, 'colsample_bytree': 0.7600364762365822, 'lambda': 0.5378968855991144, 'alpha': 0.04458804723783751, 'min_child_weight': 18}. Best is trial 3 with value: 6820.4623786033335.\n",
      "[I 2025-07-25 18:55:20,689] Trial 8 finished with value: 6858.058735207731 and parameters: {'eta': 0.024887949487051416, 'max_depth': 6, 'subsample': 0.7257127581704845, 'colsample_bytree': 0.7543648999869975, 'lambda': 27.58353159697494, 'alpha': 1.4073680647507065, 'min_child_weight': 9}. Best is trial 3 with value: 6820.4623786033335.\n",
      "[I 2025-07-25 18:55:28,895] Trial 9 finished with value: 6844.17278090315 and parameters: {'eta': 0.026996284880921317, 'max_depth': 6, 'subsample': 0.8982143774123759, 'colsample_bytree': 0.7429647568139363, 'lambda': 0.010003980045626646, 'alpha': 6.282069921789395, 'min_child_weight': 8}. Best is trial 3 with value: 6820.4623786033335.\n",
      "[I 2025-07-25 18:56:02,553] Trial 10 finished with value: 6811.9157258360865 and parameters: {'eta': 0.010494061297144669, 'max_depth': 4, 'subsample': 0.8360397893703576, 'colsample_bytree': 0.6125541865239064, 'lambda': 0.07633522778345912, 'alpha': 0.17653352767729222, 'min_child_weight': 15}. Best is trial 10 with value: 6811.9157258360865.\n",
      "[I 2025-07-25 18:56:37,656] Trial 11 finished with value: 6817.436245345816 and parameters: {'eta': 0.010151478868806576, 'max_depth': 4, 'subsample': 0.8294204680963467, 'colsample_bytree': 0.6009719396109885, 'lambda': 0.08755188771791432, 'alpha': 0.010248630326211766, 'min_child_weight': 19}. Best is trial 10 with value: 6811.9157258360865.\n",
      "[I 2025-07-25 18:57:05,582] Trial 12 finished with value: 6822.330774979656 and parameters: {'eta': 0.0100353519554514, 'max_depth': 4, 'subsample': 0.8281518676835329, 'colsample_bytree': 0.6029502616427413, 'lambda': 0.05329381200826747, 'alpha': 0.18790947753091028, 'min_child_weight': 20}. Best is trial 10 with value: 6811.9157258360865.\n",
      "[I 2025-07-25 18:57:38,077] Trial 13 finished with value: 6832.444611978969 and parameters: {'eta': 0.01419164576380057, 'max_depth': 3, 'subsample': 0.8137654595590269, 'colsample_bytree': 0.6023769164859066, 'lambda': 0.12218559702819319, 'alpha': 0.2831532260074472, 'min_child_weight': 17}. Best is trial 10 with value: 6811.9157258360865.\n",
      "[I 2025-07-25 18:58:07,936] Trial 14 finished with value: 6994.392732549417 and parameters: {'eta': 0.010071180502723414, 'max_depth': 4, 'subsample': 0.8357575043402192, 'colsample_bytree': 0.6630522095404637, 'lambda': 0.11929276057788592, 'alpha': 92.55635992890949, 'min_child_weight': 20}. Best is trial 10 with value: 6811.9157258360865.\n",
      "[I 2025-07-25 18:58:26,138] Trial 15 finished with value: 6827.065447435021 and parameters: {'eta': 0.01773346855814491, 'max_depth': 4, 'subsample': 0.7881103170313908, 'colsample_bytree': 0.6465174835035388, 'lambda': 6.49932630900255, 'alpha': 0.011576421678553538, 'min_child_weight': 16}. Best is trial 10 with value: 6811.9157258360865.\n",
      "[I 2025-07-25 18:58:37,423] Trial 16 finished with value: 6842.070500898464 and parameters: {'eta': 0.04870543106141441, 'max_depth': 5, 'subsample': 0.8652273701243156, 'colsample_bytree': 0.637276736198451, 'lambda': 0.18896369825471385, 'alpha': 0.2619092342757562, 'min_child_weight': 13}. Best is trial 10 with value: 6811.9157258360865.\n",
      "[I 2025-07-25 18:59:08,808] Trial 17 finished with value: 6832.696887286217 and parameters: {'eta': 0.01395340934326235, 'max_depth': 3, 'subsample': 0.7637199556170517, 'colsample_bytree': 0.8101580604077221, 'lambda': 0.04453857073497791, 'alpha': 0.096539505992094, 'min_child_weight': 18}. Best is trial 10 with value: 6811.9157258360865.\n",
      "[I 2025-07-25 18:59:24,923] Trial 18 finished with value: 6823.194249778461 and parameters: {'eta': 0.019196668010310405, 'max_depth': 4, 'subsample': 0.8540481263826668, 'colsample_bytree': 0.621010260674519, 'lambda': 2.611937151882503, 'alpha': 0.7695976996034374, 'min_child_weight': 20}. Best is trial 10 with value: 6811.9157258360865.\n",
      "[I 2025-07-25 18:59:35,413] Trial 19 finished with value: 6823.707719866457 and parameters: {'eta': 0.022444835508312718, 'max_depth': 5, 'subsample': 0.7934141854744058, 'colsample_bytree': 0.6817543216200236, 'lambda': 0.22048250427743865, 'alpha': 0.029769378693639194, 'min_child_weight': 6}. Best is trial 10 with value: 6811.9157258360865.\n",
      "[I 2025-07-25 18:59:55,232] Trial 20 finished with value: 6853.785173545548 and parameters: {'eta': 0.012295996926687053, 'max_depth': 4, 'subsample': 0.9492015494177445, 'colsample_bytree': 0.7904544800510469, 'lambda': 96.23633922564143, 'alpha': 0.06653632298807159, 'min_child_weight': 11}. Best is trial 10 with value: 6811.9157258360865.\n",
      "[I 2025-07-25 19:00:13,175] Trial 21 finished with value: 6811.579848924852 and parameters: {'eta': 0.011855060997122453, 'max_depth': 5, 'subsample': 0.6748352585107193, 'colsample_bytree': 0.7103457354669841, 'lambda': 0.032458366393503454, 'alpha': 0.017519485264763976, 'min_child_weight': 14}. Best is trial 21 with value: 6811.579848924852.\n",
      "[I 2025-07-25 19:00:30,182] Trial 22 finished with value: 6809.607492758082 and parameters: {'eta': 0.012021467020549837, 'max_depth': 5, 'subsample': 0.673001592878496, 'colsample_bytree': 0.7108273918486016, 'lambda': 0.01908328421729402, 'alpha': 0.011844172375422374, 'min_child_weight': 15}. Best is trial 22 with value: 6809.607492758082.\n",
      "[I 2025-07-25 19:00:46,424] Trial 23 finished with value: 6817.467505620703 and parameters: {'eta': 0.012678087155326725, 'max_depth': 5, 'subsample': 0.6681884197198619, 'colsample_bytree': 0.7167126439632162, 'lambda': 0.02218118909060006, 'alpha': 0.020176900634915076, 'min_child_weight': 15}. Best is trial 22 with value: 6809.607492758082.\n",
      "[I 2025-07-25 19:01:05,246] Trial 24 finished with value: 6811.701611927569 and parameters: {'eta': 0.01580736632346565, 'max_depth': 5, 'subsample': 0.6536081095782359, 'colsample_bytree': 0.6844055306468371, 'lambda': 0.02076403850008567, 'alpha': 0.13070865139436447, 'min_child_weight': 13}. Best is trial 22 with value: 6809.607492758082.\n",
      "[I 2025-07-25 19:01:22,176] Trial 25 finished with value: 6819.18396824592 and parameters: {'eta': 0.014621208594719192, 'max_depth': 5, 'subsample': 0.6455475967523825, 'colsample_bytree': 0.6873273278709383, 'lambda': 0.023489806022086942, 'alpha': 0.04298483498301927, 'min_child_weight': 13}. Best is trial 22 with value: 6809.607492758082.\n",
      "[I 2025-07-25 19:01:32,201] Trial 26 finished with value: 6841.589871579677 and parameters: {'eta': 0.01604876389623114, 'max_depth': 7, 'subsample': 0.6411396943111735, 'colsample_bytree': 0.7312997811548368, 'lambda': 0.010023167869806558, 'alpha': 0.42215932533642114, 'min_child_weight': 11}. Best is trial 22 with value: 6809.607492758082.\n",
      "[I 2025-07-25 19:01:46,681] Trial 27 finished with value: 6818.610781771343 and parameters: {'eta': 0.01767764957196844, 'max_depth': 5, 'subsample': 0.6957463364238806, 'colsample_bytree': 0.7844282478688398, 'lambda': 0.02944555467627154, 'alpha': 0.08770754972398838, 'min_child_weight': 13}. Best is trial 22 with value: 6809.607492758082.\n",
      "[I 2025-07-25 19:01:54,638] Trial 28 finished with value: 6827.425900596474 and parameters: {'eta': 0.021245662505702128, 'max_depth': 6, 'subsample': 0.7404997964133604, 'colsample_bytree': 0.7072738097396704, 'lambda': 0.23273813447682024, 'alpha': 0.019016840317299746, 'min_child_weight': 16}. Best is trial 22 with value: 6809.607492758082.\n",
      "[I 2025-07-25 19:01:57,975] Trial 29 finished with value: 6824.418722583149 and parameters: {'eta': 0.0616976594553586, 'max_depth': 5, 'subsample': 0.6497987848894583, 'colsample_bytree': 0.6592725962135818, 'lambda': 0.016378925089601384, 'alpha': 0.0449370633272064, 'min_child_weight': 17}. Best is trial 22 with value: 6809.607492758082.\n",
      "[I 2025-07-25 19:02:14,733] Trial 30 finished with value: 6827.727840187131 and parameters: {'eta': 0.013428784730190343, 'max_depth': 5, 'subsample': 0.6706572127912569, 'colsample_bytree': 0.7308468927129745, 'lambda': 0.042722252746194234, 'alpha': 0.14093502416033354, 'min_child_weight': 5}. Best is trial 22 with value: 6809.607492758082.\n",
      "[I 2025-07-25 19:02:39,832] Trial 31 finished with value: 6821.085321793622 and parameters: {'eta': 0.011517606194168412, 'max_depth': 4, 'subsample': 0.7107390904162031, 'colsample_bytree': 0.6390046134312769, 'lambda': 0.06593033516589726, 'alpha': 0.6285661669784064, 'min_child_weight': 14}. Best is trial 22 with value: 6809.607492758082.\n",
      "[I 2025-07-25 19:02:57,658] Trial 32 finished with value: 6812.327789242926 and parameters: {'eta': 0.011120154979951943, 'max_depth': 5, 'subsample': 0.6256234369577466, 'colsample_bytree': 0.6673575449324928, 'lambda': 0.017441218907348854, 'alpha': 0.1479036221450893, 'min_child_weight': 15}. Best is trial 22 with value: 6809.607492758082.\n",
      "[I 2025-07-25 19:03:18,551] Trial 33 finished with value: 6819.713347635176 and parameters: {'eta': 0.015523220273979756, 'max_depth': 4, 'subsample': 0.6696826659110855, 'colsample_bytree': 0.6961875257906065, 'lambda': 0.07404204658466405, 'alpha': 2.417037843395152, 'min_child_weight': 12}. Best is trial 22 with value: 6809.607492758082.\n",
      "[I 2025-07-25 19:03:33,728] Trial 34 finished with value: 6815.064738718046 and parameters: {'eta': 0.01249211881732535, 'max_depth': 6, 'subsample': 0.7381022121450509, 'colsample_bytree': 0.6279975913649181, 'lambda': 0.029231666476123314, 'alpha': 0.022664133080018245, 'min_child_weight': 14}. Best is trial 22 with value: 6809.607492758082.\n",
      "[I 2025-07-25 19:03:56,258] Trial 35 finished with value: 6802.089641840101 and parameters: {'eta': 0.011179706525759989, 'max_depth': 5, 'subsample': 0.7020252586633959, 'colsample_bytree': 0.7187717643779563, 'lambda': 0.010738093929645162, 'alpha': 0.061669941714552284, 'min_child_weight': 16}. Best is trial 35 with value: 6802.089641840101.\n",
      "[I 2025-07-25 19:04:10,541] Trial 36 finished with value: 6802.855382537827 and parameters: {'eta': 0.01694504188443175, 'max_depth': 5, 'subsample': 0.7053298369161868, 'colsample_bytree': 0.7201280867891331, 'lambda': 0.011819766931398964, 'alpha': 0.016202156601989315, 'min_child_weight': 16}. Best is trial 35 with value: 6802.089641840101.\n",
      "[I 2025-07-25 19:04:18,419] Trial 37 finished with value: 6819.451780597818 and parameters: {'eta': 0.028661395497079488, 'max_depth': 6, 'subsample': 0.7027111466076701, 'colsample_bytree': 0.7689797055431091, 'lambda': 0.010071634368839778, 'alpha': 0.016209107669924054, 'min_child_weight': 17}. Best is trial 35 with value: 6802.089641840101.\n",
      "[I 2025-07-25 19:04:28,539] Trial 38 finished with value: 6857.690591047966 and parameters: {'eta': 0.0179187084728637, 'max_depth': 8, 'subsample': 0.6873556806588915, 'colsample_bytree': 0.7171356694703142, 'lambda': 0.013885847976156149, 'alpha': 0.041208246628927844, 'min_child_weight': 16}. Best is trial 35 with value: 6802.089641840101.\n",
      "[I 2025-07-25 19:04:45,740] Trial 39 finished with value: 6826.682521524653 and parameters: {'eta': 0.011680674361602954, 'max_depth': 7, 'subsample': 0.714247845477529, 'colsample_bytree': 0.8069909263662137, 'lambda': 0.036484434139046784, 'alpha': 0.010608395987291797, 'min_child_weight': 18}. Best is trial 35 with value: 6802.089641840101.\n",
      "[I 2025-07-25 19:04:52,771] Trial 40 finished with value: 6844.422614538902 and parameters: {'eta': 0.0403537969913637, 'max_depth': 6, 'subsample': 0.6267438743823832, 'colsample_bytree': 0.7490375822141762, 'lambda': 0.016045664880297263, 'alpha': 0.05995082719209148, 'min_child_weight': 1}. Best is trial 35 with value: 6802.089641840101.\n",
      "[I 2025-07-25 19:05:07,421] Trial 41 finished with value: 6820.025708459353 and parameters: {'eta': 0.015996207623123888, 'max_depth': 5, 'subsample': 0.6024480489498929, 'colsample_bytree': 0.6974598839950054, 'lambda': 0.022537745323345782, 'alpha': 0.028558918555618616, 'min_child_weight': 12}. Best is trial 35 with value: 6802.089641840101.\n",
      "[I 2025-07-25 19:05:22,052] Trial 42 finished with value: 6819.575946184218 and parameters: {'eta': 0.013673361138147462, 'max_depth': 5, 'subsample': 0.6777979278823618, 'colsample_bytree': 0.7345388901372918, 'lambda': 0.014279065165850473, 'alpha': 0.026535537164628614, 'min_child_weight': 14}. Best is trial 35 with value: 6802.089641840101.\n",
      "[I 2025-07-25 19:05:31,402] Trial 43 finished with value: 6816.722522277313 and parameters: {'eta': 0.02314214610309349, 'max_depth': 5, 'subsample': 0.6538429948389258, 'colsample_bytree': 0.6751993293913503, 'lambda': 0.033981280227864744, 'alpha': 0.01577415284292422, 'min_child_weight': 10}. Best is trial 35 with value: 6802.089641840101.\n",
      "[I 2025-07-25 19:05:33,705] Trial 44 finished with value: 6866.396112880861 and parameters: {'eta': 0.0993647330458974, 'max_depth': 6, 'subsample': 0.7238151226533819, 'colsample_bytree': 0.7140113973199037, 'lambda': 0.050434441436283, 'alpha': 0.10446528368011493, 'min_child_weight': 15}. Best is trial 35 with value: 6802.089641840101.\n",
      "[I 2025-07-25 19:05:56,359] Trial 45 finished with value: 6816.639289276488 and parameters: {'eta': 0.011390533150766212, 'max_depth': 5, 'subsample': 0.7529191928772048, 'colsample_bytree': 0.7618109191228579, 'lambda': 0.010283614835387297, 'alpha': 0.06021663617910979, 'min_child_weight': 16}. Best is trial 35 with value: 6802.089641840101.\n",
      "[I 2025-07-25 19:06:05,558] Trial 46 finished with value: 6837.101171934582 and parameters: {'eta': 0.019640890797990366, 'max_depth': 6, 'subsample': 0.6272882573321418, 'colsample_bytree': 0.6887543743946892, 'lambda': 0.10603755703906875, 'alpha': 0.03511688646619394, 'min_child_weight': 13}. Best is trial 35 with value: 6802.089641840101.\n",
      "[I 2025-07-25 19:06:18,143] Trial 47 finished with value: 6802.479413783707 and parameters: {'eta': 0.015609520572741724, 'max_depth': 5, 'subsample': 0.6609977071330699, 'colsample_bytree': 0.8469838308850589, 'lambda': 0.3577111148410406, 'alpha': 0.014495101199330503, 'min_child_weight': 19}. Best is trial 35 with value: 6802.089641840101.\n",
      "[I 2025-07-25 19:06:45,350] Trial 48 finished with value: 6817.320455061816 and parameters: {'eta': 0.010852670523541523, 'max_depth': 4, 'subsample': 0.6964944689643634, 'colsample_bytree': 0.8786050808546922, 'lambda': 0.6283752155796121, 'alpha': 0.015284878560747962, 'min_child_weight': 19}. Best is trial 35 with value: 6802.089641840101.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-07-25 19:07:06,193] A new study created in memory with name: no-name-1dc32a53-e3f3-400f-9a79-026bd205e049\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-07-25 19:07:06,191] Trial 49 finished with value: 6815.43512410467 and parameters: {'eta': 0.01257153833124896, 'max_depth': 5, 'subsample': 0.7230896669246231, 'colsample_bytree': 0.9393850097154499, 'lambda': 4.316110182703192, 'alpha': 0.010588962831465362, 'min_child_weight': 19}. Best is trial 35 with value: 6802.089641840101.\n",
      "\n",
      "Lower-Bound Model Tuning Complete.\n",
      "Best Validation Score (Pinball Loss): 6,802.0896\n",
      "Best Parameters Found: {'eta': 0.011179706525759989, 'max_depth': 5, 'subsample': 0.7020252586633959, 'colsample_bytree': 0.7187717643779563, 'lambda': 0.010738093929645162, 'alpha': 0.061669941714552284, 'min_child_weight': 16, 'n_estimators': 1520}\n",
      "\n",
      "--- Tuning the XGBoost Upper-Bound Model (alpha=0.95)... ---\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "391fb7dbe33d4fa4b9a3c2d5e118177b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/50 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-07-25 19:07:11,555] Trial 0 finished with value: 8042.0927096222385 and parameters: {'eta': 0.039920613647788455, 'max_depth': 8, 'subsample': 0.7839958704975047, 'colsample_bytree': 0.6505382733097559, 'lambda': 0.13588375646265957, 'alpha': 0.05233827925678519, 'min_child_weight': 12}. Best is trial 0 with value: 8042.0927096222385.\n",
      "[I 2025-07-25 19:07:18,835] Trial 1 finished with value: 7915.0715451055985 and parameters: {'eta': 0.0364348602768857, 'max_depth': 7, 'subsample': 0.8606925696521479, 'colsample_bytree': 0.7756860117110942, 'lambda': 69.0869988765632, 'alpha': 0.016257085516729344, 'min_child_weight': 5}. Best is trial 1 with value: 7915.0715451055985.\n",
      "[I 2025-07-25 19:07:25,126] Trial 2 finished with value: 7845.656902966944 and parameters: {'eta': 0.053626963192890155, 'max_depth': 5, 'subsample': 0.8823081253252743, 'colsample_bytree': 0.6964156923408096, 'lambda': 76.64140923615967, 'alpha': 0.28194921119293476, 'min_child_weight': 13}. Best is trial 2 with value: 7845.656902966944.\n",
      "[I 2025-07-25 19:07:40,387] Trial 3 finished with value: 7833.913170235373 and parameters: {'eta': 0.01978628281049339, 'max_depth': 6, 'subsample': 0.8781763526427303, 'colsample_bytree': 0.6201894051805832, 'lambda': 1.99011078838021, 'alpha': 26.618803277898188, 'min_child_weight': 20}. Best is trial 3 with value: 7833.913170235373.\n",
      "[I 2025-07-25 19:07:47,176] Trial 4 finished with value: 8087.992523015794 and parameters: {'eta': 0.03898684676942352, 'max_depth': 8, 'subsample': 0.8691403620914848, 'colsample_bytree': 0.8375748767165592, 'lambda': 0.11669492265639712, 'alpha': 0.04694846407867349, 'min_child_weight': 9}. Best is trial 3 with value: 7833.913170235373.\n",
      "[I 2025-07-25 19:08:04,506] Trial 5 finished with value: 7862.234123543194 and parameters: {'eta': 0.015456133389449669, 'max_depth': 7, 'subsample': 0.6521891789810064, 'colsample_bytree': 0.8325534195059665, 'lambda': 0.39581286468347526, 'alpha': 29.634893312507206, 'min_child_weight': 17}. Best is trial 3 with value: 7833.913170235373.\n",
      "[I 2025-07-25 19:08:15,497] Trial 6 finished with value: 7843.152720088828 and parameters: {'eta': 0.025346846387322357, 'max_depth': 4, 'subsample': 0.9399216803426951, 'colsample_bytree': 0.7546364119520967, 'lambda': 0.08237560102245248, 'alpha': 0.0910657999455215, 'min_child_weight': 8}. Best is trial 3 with value: 7833.913170235373.\n",
      "[I 2025-07-25 19:08:19,827] Trial 7 finished with value: 8031.432067581503 and parameters: {'eta': 0.052273554953601434, 'max_depth': 8, 'subsample': 0.659523447176587, 'colsample_bytree': 0.7272948529035262, 'lambda': 0.2176121845411671, 'alpha': 0.020458651539022238, 'min_child_weight': 7}. Best is trial 3 with value: 7833.913170235373.\n",
      "[I 2025-07-25 19:08:46,094] Trial 8 finished with value: 7834.14391980775 and parameters: {'eta': 0.01206420772169899, 'max_depth': 6, 'subsample': 0.9452730791642407, 'colsample_bytree': 0.6150624412746046, 'lambda': 2.1792845492963706, 'alpha': 7.005269668733796, 'min_child_weight': 18}. Best is trial 3 with value: 7833.913170235373.\n",
      "[I 2025-07-25 19:08:54,328] Trial 9 finished with value: 7857.817182993613 and parameters: {'eta': 0.07461435230175979, 'max_depth': 3, 'subsample': 0.7108759965884854, 'colsample_bytree': 0.7966759761533417, 'lambda': 0.8269047617968631, 'alpha': 6.483075215834413, 'min_child_weight': 8}. Best is trial 3 with value: 7833.913170235373.\n",
      "[I 2025-07-25 19:09:07,135] Trial 10 finished with value: 7878.406521821962 and parameters: {'eta': 0.024019818016719475, 'max_depth': 5, 'subsample': 0.7887895507844997, 'colsample_bytree': 0.9088075312193411, 'lambda': 0.012904202725024854, 'alpha': 67.70888935434513, 'min_child_weight': 20}. Best is trial 3 with value: 7833.913170235373.\n",
      "[I 2025-07-25 19:09:29,699] Trial 11 finished with value: 7843.530967308771 and parameters: {'eta': 0.01234098616546762, 'max_depth': 6, 'subsample': 0.94906573759721, 'colsample_bytree': 0.6092593759660199, 'lambda': 4.913748413013363, 'alpha': 3.9746784044707733, 'min_child_weight': 20}. Best is trial 3 with value: 7833.913170235373.\n",
      "[I 2025-07-25 19:09:56,012] Trial 12 finished with value: 7824.02861051451 and parameters: {'eta': 0.010081675306256513, 'max_depth': 6, 'subsample': 0.9003039649754689, 'colsample_bytree': 0.6079746751506584, 'lambda': 6.563080525158694, 'alpha': 8.612538700529253, 'min_child_weight': 17}. Best is trial 12 with value: 7824.02861051451.\n",
      "[I 2025-07-25 19:10:12,410] Trial 13 finished with value: 7843.281834102574 and parameters: {'eta': 0.016924287195479917, 'max_depth': 6, 'subsample': 0.8288384854584843, 'colsample_bytree': 0.6755599358380232, 'lambda': 7.574908104956396, 'alpha': 1.6461792946798703, 'min_child_weight': 16}. Best is trial 12 with value: 7824.02861051451.\n",
      "[I 2025-07-25 19:10:36,147] Trial 14 finished with value: 7823.403340357309 and parameters: {'eta': 0.018948900811953513, 'max_depth': 4, 'subsample': 0.8914268586175867, 'colsample_bytree': 0.6418790773149997, 'lambda': 14.664542982011982, 'alpha': 18.13551147713146, 'min_child_weight': 14}. Best is trial 14 with value: 7823.403340357309.\n",
      "[I 2025-07-25 19:11:06,694] Trial 15 finished with value: 7823.151552964272 and parameters: {'eta': 0.010747640021782768, 'max_depth': 3, 'subsample': 0.7480672161218691, 'colsample_bytree': 0.672319938356443, 'lambda': 18.867334693913516, 'alpha': 0.6439457731439236, 'min_child_weight': 14}. Best is trial 15 with value: 7823.151552964272.\n",
      "[I 2025-07-25 19:11:27,402] Trial 16 finished with value: 7826.923938016582 and parameters: {'eta': 0.014452187780213843, 'max_depth': 3, 'subsample': 0.7352127729960043, 'colsample_bytree': 0.7085506254036543, 'lambda': 21.384265329113653, 'alpha': 0.5320110052963043, 'min_child_weight': 14}. Best is trial 15 with value: 7823.151552964272.\n",
      "[I 2025-07-25 19:11:38,707] Trial 17 finished with value: 7816.380207722764 and parameters: {'eta': 0.02205764179722564, 'max_depth': 4, 'subsample': 0.6111418672704618, 'colsample_bytree': 0.6622147422999819, 'lambda': 23.588888905933747, 'alpha': 1.8866474147587122, 'min_child_weight': 1}. Best is trial 17 with value: 7816.380207722764.\n",
      "[I 2025-07-25 19:11:47,846] Trial 18 finished with value: 7830.7744691145 and parameters: {'eta': 0.028807785144401025, 'max_depth': 4, 'subsample': 0.6980375340724823, 'colsample_bytree': 0.6668233871671067, 'lambda': 26.952948257862865, 'alpha': 0.27701100982108595, 'min_child_weight': 2}. Best is trial 17 with value: 7816.380207722764.\n",
      "[I 2025-07-25 19:11:59,535] Trial 19 finished with value: 7842.396141023511 and parameters: {'eta': 0.022446115365779434, 'max_depth': 3, 'subsample': 0.6196517840490487, 'colsample_bytree': 0.9444391217000373, 'lambda': 38.27866166078779, 'alpha': 1.8977118353788107, 'min_child_weight': 1}. Best is trial 17 with value: 7816.380207722764.\n",
      "[I 2025-07-25 19:12:02,476] Trial 20 finished with value: 7858.182516716061 and parameters: {'eta': 0.08470258890275853, 'max_depth': 4, 'subsample': 0.6147501810349507, 'colsample_bytree': 0.7339704986037888, 'lambda': 2.529170602655722, 'alpha': 0.9311451560082069, 'min_child_weight': 11}. Best is trial 17 with value: 7816.380207722764.\n",
      "[I 2025-07-25 19:12:19,480] Trial 21 finished with value: 7833.6823056159465 and parameters: {'eta': 0.017215278590859643, 'max_depth': 4, 'subsample': 0.8183992471951184, 'colsample_bytree': 0.64898402955494, 'lambda': 14.51014815614477, 'alpha': 20.15542636167604, 'min_child_weight': 15}. Best is trial 17 with value: 7816.380207722764.\n",
      "[I 2025-07-25 19:12:52,224] Trial 22 finished with value: 7875.189630728579 and parameters: {'eta': 0.019687436480683492, 'max_depth': 3, 'subsample': 0.7442749208454759, 'colsample_bytree': 0.6794303203949095, 'lambda': 16.594729400471135, 'alpha': 85.52088604249543, 'min_child_weight': 4}. Best is trial 17 with value: 7816.380207722764.\n",
      "[I 2025-07-25 19:13:14,462] Trial 23 finished with value: 7824.48736456227 and parameters: {'eta': 0.01031584843821409, 'max_depth': 5, 'subsample': 0.6704270372067558, 'colsample_bytree': 0.651655927203877, 'lambda': 86.13525731504426, 'alpha': 2.635034067741404, 'min_child_weight': 11}. Best is trial 17 with value: 7816.380207722764.\n",
      "[I 2025-07-25 19:13:32,725] Trial 24 finished with value: 7825.291491063562 and parameters: {'eta': 0.012837254081120835, 'max_depth': 4, 'subsample': 0.8243443773070772, 'colsample_bytree': 0.7165507162782411, 'lambda': 11.186059100929734, 'alpha': 0.5785224987353927, 'min_child_weight': 14}. Best is trial 17 with value: 7816.380207722764.\n",
      "[I 2025-07-25 19:13:51,894] Trial 25 finished with value: 7841.642203413242 and parameters: {'eta': 0.02907712559336433, 'max_depth': 3, 'subsample': 0.7648556929841253, 'colsample_bytree': 0.6393282578926136, 'lambda': 36.056132233471835, 'alpha': 13.81768146826432, 'min_child_weight': 10}. Best is trial 17 with value: 7816.380207722764.\n",
      "[I 2025-07-25 19:14:02,863] Trial 26 finished with value: 7824.934911297983 and parameters: {'eta': 0.019711738758727688, 'max_depth': 4, 'subsample': 0.7127235126449457, 'colsample_bytree': 0.6840659220942071, 'lambda': 4.091464015811959, 'alpha': 0.15872166558603817, 'min_child_weight': 6}. Best is trial 17 with value: 7816.380207722764.\n",
      "[I 2025-07-25 19:14:18,064] Trial 27 finished with value: 7827.792768788613 and parameters: {'eta': 0.014307986636835687, 'max_depth': 5, 'subsample': 0.6870324728919053, 'colsample_bytree': 0.7491684165319993, 'lambda': 1.1760949489396315, 'alpha': 0.8430047975102273, 'min_child_weight': 3}. Best is trial 17 with value: 7816.380207722764.\n",
      "[I 2025-07-25 19:14:30,269] Trial 28 finished with value: 7831.884371420515 and parameters: {'eta': 0.03221323873652307, 'max_depth': 3, 'subsample': 0.601258537628494, 'colsample_bytree': 0.6315443975337923, 'lambda': 36.01124105930579, 'alpha': 3.4783105821600997, 'min_child_weight': 13}. Best is trial 17 with value: 7816.380207722764.\n",
      "[I 2025-07-25 19:14:53,698] Trial 29 finished with value: 7841.558649212101 and parameters: {'eta': 0.017644616221207988, 'max_depth': 4, 'subsample': 0.7944892050719489, 'colsample_bytree': 0.6591341289489548, 'lambda': 11.03192916272721, 'alpha': 52.22792650918843, 'min_child_weight': 13}. Best is trial 17 with value: 7816.380207722764.\n",
      "[I 2025-07-25 19:14:59,694] Trial 30 finished with value: 7883.5925113651365 and parameters: {'eta': 0.0500927277919884, 'max_depth': 5, 'subsample': 0.7626357509575227, 'colsample_bytree': 0.6953485395934076, 'lambda': 0.050078382689291226, 'alpha': 0.3182157973816707, 'min_child_weight': 12}. Best is trial 17 with value: 7816.380207722764.\n",
      "[I 2025-07-25 19:15:25,624] Trial 31 finished with value: 7859.517709880276 and parameters: {'eta': 0.010237555166687796, 'max_depth': 7, 'subsample': 0.8365709371733712, 'colsample_bytree': 0.6050561646859726, 'lambda': 6.014286442721382, 'alpha': 8.766435858681104, 'min_child_weight': 18}. Best is trial 17 with value: 7816.380207722764.\n",
      "[I 2025-07-25 19:15:58,351] Trial 32 finished with value: 7837.509054965806 and parameters: {'eta': 0.01129294896024203, 'max_depth': 3, 'subsample': 0.9111346245951302, 'colsample_bytree': 0.6006547805698682, 'lambda': 53.27085885107614, 'alpha': 10.504520216388357, 'min_child_weight': 15}. Best is trial 17 with value: 7816.380207722764.\n",
      "[I 2025-07-25 19:16:18,985] Trial 33 finished with value: 7831.107505203736 and parameters: {'eta': 0.013442201568169186, 'max_depth': 5, 'subsample': 0.907817216746285, 'colsample_bytree': 0.6347513834974859, 'lambda': 7.970662202951077, 'alpha': 1.4280211999000674, 'min_child_weight': 17}. Best is trial 17 with value: 7816.380207722764.\n",
      "[I 2025-07-25 19:16:46,864] Trial 34 finished with value: 7829.556773093399 and parameters: {'eta': 0.01012731185553584, 'max_depth': 4, 'subsample': 0.9017231450113867, 'colsample_bytree': 0.6322308147068265, 'lambda': 21.05687572341465, 'alpha': 4.730315580992984, 'min_child_weight': 16}. Best is trial 17 with value: 7816.380207722764.\n",
      "[I 2025-07-25 19:16:52,987] Trial 35 finished with value: 7845.206269166258 and parameters: {'eta': 0.04533406769682889, 'max_depth': 6, 'subsample': 0.8549439685492077, 'colsample_bytree': 0.6569337683144512, 'lambda': 96.64018730627366, 'alpha': 17.13335827238592, 'min_child_weight': 12}. Best is trial 17 with value: 7816.380207722764.\n",
      "[I 2025-07-25 19:17:04,303] Trial 36 finished with value: 7856.567868050683 and parameters: {'eta': 0.022517104745073242, 'max_depth': 7, 'subsample': 0.8887801268776584, 'colsample_bytree': 0.7029357035673813, 'lambda': 3.497249304547296, 'alpha': 34.11872573476128, 'min_child_weight': 18}. Best is trial 17 with value: 7816.380207722764.\n",
      "[I 2025-07-25 19:17:21,711] Trial 37 finished with value: 7856.713167146431 and parameters: {'eta': 0.016025187382037703, 'max_depth': 6, 'subsample': 0.8538720309826905, 'colsample_bytree': 0.7793705804761553, 'lambda': 1.148745705227861, 'alpha': 2.9088841118702993, 'min_child_weight': 10}. Best is trial 17 with value: 7816.380207722764.\n",
      "[I 2025-07-25 19:17:47,046] Trial 38 finished with value: 7900.789124492413 and parameters: {'eta': 0.011571095270956504, 'max_depth': 7, 'subsample': 0.9251697459824327, 'colsample_bytree': 0.8191236156844768, 'lambda': 51.90346270350844, 'alpha': 0.01020965264107366, 'min_child_weight': 14}. Best is trial 17 with value: 7816.380207722764.\n",
      "[I 2025-07-25 19:17:59,145] Trial 39 finished with value: 7856.902243333673 and parameters: {'eta': 0.027032560786134256, 'max_depth': 4, 'subsample': 0.6383828981622536, 'colsample_bytree': 0.8684646849511076, 'lambda': 10.089092214862687, 'alpha': 34.03943123131988, 'min_child_weight': 19}. Best is trial 17 with value: 7816.380207722764.\n",
      "[I 2025-07-25 19:18:07,953] Trial 40 finished with value: 7845.5805176459435 and parameters: {'eta': 0.03661039294673412, 'max_depth': 5, 'subsample': 0.8082334400775848, 'colsample_bytree': 0.6236879536499206, 'lambda': 24.478360038338387, 'alpha': 7.035154585464774, 'min_child_weight': 16}. Best is trial 17 with value: 7816.380207722764.\n",
      "[I 2025-07-25 19:18:28,769] Trial 41 finished with value: 7820.255690667024 and parameters: {'eta': 0.010612401570255857, 'max_depth': 5, 'subsample': 0.6651750047986933, 'colsample_bytree': 0.6485907348305062, 'lambda': 71.13372719447504, 'alpha': 2.157259674023288, 'min_child_weight': 12}. Best is trial 17 with value: 7816.380207722764.\n",
      "[I 2025-07-25 19:18:47,926] Trial 42 finished with value: 7816.875043588069 and parameters: {'eta': 0.011377439443924128, 'max_depth': 5, 'subsample': 0.6476924239947529, 'colsample_bytree': 0.6889457126286068, 'lambda': 57.47091742418317, 'alpha': 1.8751688177064758, 'min_child_weight': 15}. Best is trial 17 with value: 7816.380207722764.\n",
      "[I 2025-07-25 19:19:03,893] Trial 43 finished with value: 7829.73977446163 and parameters: {'eta': 0.014620883388285884, 'max_depth': 5, 'subsample': 0.6425990634389773, 'colsample_bytree': 0.690437860884059, 'lambda': 59.30289263364443, 'alpha': 0.5328952898199264, 'min_child_weight': 12}. Best is trial 17 with value: 7816.380207722764.\n",
      "[I 2025-07-25 19:19:19,219] Trial 44 finished with value: 7817.46787761733 and parameters: {'eta': 0.013184981377131848, 'max_depth': 5, 'subsample': 0.6671101557957975, 'colsample_bytree': 0.6685796009709343, 'lambda': 62.40344593895166, 'alpha': 0.08702225470138399, 'min_child_weight': 8}. Best is trial 17 with value: 7816.380207722764.\n",
      "[I 2025-07-25 19:19:39,656] Trial 45 finished with value: 7827.458200071213 and parameters: {'eta': 0.011526034281150182, 'max_depth': 5, 'subsample': 0.6723890862975632, 'colsample_bytree': 0.7302965707340454, 'lambda': 69.61237430137656, 'alpha': 0.02754948447680369, 'min_child_weight': 8}. Best is trial 17 with value: 7816.380207722764.\n",
      "[I 2025-07-25 19:19:56,971] Trial 46 finished with value: 7809.714911546214 and parameters: {'eta': 0.0134666689700431, 'max_depth': 5, 'subsample': 0.6337831141892081, 'colsample_bytree': 0.6703460539244901, 'lambda': 39.61701007964543, 'alpha': 0.05566359409074608, 'min_child_weight': 6}. Best is trial 46 with value: 7809.714911546214.\n",
      "[I 2025-07-25 19:20:12,515] Trial 47 finished with value: 7821.25553926391 and parameters: {'eta': 0.013323878553502254, 'max_depth': 5, 'subsample': 0.6301071013329321, 'colsample_bytree': 0.7558497929427648, 'lambda': 96.2473926691154, 'alpha': 0.05868902577803763, 'min_child_weight': 6}. Best is trial 46 with value: 7809.714911546214.\n",
      "[I 2025-07-25 19:20:25,570] Trial 48 finished with value: 7827.8598448884995 and parameters: {'eta': 0.015646796259132727, 'max_depth': 5, 'subsample': 0.6562697888176182, 'colsample_bytree': 0.6609060243283373, 'lambda': 38.88023294245317, 'alpha': 0.08065895655323618, 'min_child_weight': 9}. Best is trial 46 with value: 7809.714911546214.\n",
      "[I 2025-07-25 19:20:43,749] Trial 49 finished with value: 7840.356129161602 and parameters: {'eta': 0.01214794748086836, 'max_depth': 6, 'subsample': 0.6003492844101037, 'colsample_bytree': 0.7174246692510149, 'lambda': 51.170929070852644, 'alpha': 0.03413882246400826, 'min_child_weight': 5}. Best is trial 46 with value: 7809.714911546214.\n",
      "\n",
      "Upper-Bound Model Tuning Complete.\n",
      "Best Validation Score (Pinball Loss): 7,809.7149\n",
      "Best Parameters Found: {'eta': 0.0134666689700431, 'max_depth': 5, 'subsample': 0.6337831141892081, 'colsample_bytree': 0.6703460539244901, 'lambda': 39.61701007964543, 'alpha': 0.05566359409074608, 'min_child_weight': 6, 'n_estimators': 1071}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "32"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# BLOCK 6: HYPERPARAMETER TUNING FOR QUANTILE MODELS WITH OPTUNA\n",
    "# =============================================================================\n",
    "# We will now use Optuna to find the best hyperparameters for our two specialist\n",
    "# quantile models. We tune them independently because the optimal parameters for\n",
    "# predicting the lower tail of the distribution might differ from those for the\n",
    "# upper tail.\n",
    "#\n",
    "# We use the native XGBoost API (`xgb.train`) because it allows us to use\n",
    "# callbacks like `early_stopping`, which significantly speeds up the tuning\n",
    "# process by not training for a fixed, excessive number of rounds.\n",
    "\n",
    "# --- Step 1: Create a Holdout Set for Tuning ---\n",
    "# For hyperparameter tuning, we need a single, consistent validation set to\n",
    "# evaluate each trial's performance. We'll split off 20% of our elite\n",
    "# training data for this purpose.\n",
    "print(\"\\n--- Preparing data for Optuna tuning ---\")\n",
    "X_train_opt, X_val_opt, y_train_opt, y_val_opt = train_test_split(\n",
    "    X_for_quantile,\n",
    "    y_true,\n",
    "    test_size=0.20,\n",
    "    random_state=RANDOM_STATE\n",
    ")\n",
    "\n",
    "# The native XGBoost API is most efficient with its internal DMatrix format.\n",
    "dtrain_opt = xgb.DMatrix(X_train_opt, label=y_train_opt)\n",
    "dval_opt = xgb.DMatrix(X_val_opt, label=y_val_opt)\n",
    "\n",
    "print(f\"Data split for tuning: {len(X_train_opt)} train, {len(X_val_opt)} validation samples.\")\n",
    "\n",
    "# --- Step 2: Tune the Lower-Bound Model (alpha=0.05) ---\n",
    "\n",
    "def objective_lower(trial):\n",
    "    \"\"\"Optuna objective function for the lower-bound quantile model.\"\"\"\n",
    "    params = {\n",
    "        'objective': 'reg:quantileerror',\n",
    "        'quantile_alpha': 0.05,\n",
    "        # The 'quantile' eval_metric is the pinball loss, which is exactly\n",
    "        # what we want to minimize for this objective.\n",
    "        'eval_metric': 'quantile',\n",
    "        'tree_method': 'hist',\n",
    "        'seed': RANDOM_STATE,\n",
    "        'n_jobs': -1,\n",
    "\n",
    "        # --- Hyperparameters to Tune ---\n",
    "        'eta': trial.suggest_float('eta', 0.01, 0.1, log=True),\n",
    "        'max_depth': trial.suggest_int('max_depth', 3, 8),\n",
    "        'subsample': trial.suggest_float('subsample', 0.6, 0.95),\n",
    "        'colsample_bytree': trial.suggest_float('colsample_bytree', 0.6, 0.95),\n",
    "        'lambda': trial.suggest_float('lambda', 1e-2, 100.0, log=True), # L2 Reg\n",
    "        'alpha': trial.suggest_float('alpha', 1e-2, 100.0, log=True),   # L1 Reg\n",
    "        'min_child_weight': trial.suggest_int('min_child_weight', 1, 20),\n",
    "    }\n",
    "\n",
    "    # Train the model with early stopping\n",
    "    bst = xgb.train(\n",
    "        params,\n",
    "        dtrain_opt,\n",
    "        num_boost_round=2000,  # High number, early stopping will find the best\n",
    "        evals=[(dval_opt, 'validation')],\n",
    "        early_stopping_rounds=50,\n",
    "        verbose_eval=False\n",
    "    )\n",
    "\n",
    "    # Store the best number of boosting rounds in the trial attributes\n",
    "    trial.set_user_attr('best_iteration', bst.best_iteration)\n",
    "\n",
    "    # Return the score to be minimized (pinball loss on the validation set)\n",
    "    return bst.best_score\n",
    "\n",
    "print(\"\\n--- Tuning the XGBoost Lower-Bound Model (alpha=0.05)... ---\")\n",
    "N_TRIALS = 50 # A good number for a solid search\n",
    "study_lower = optuna.create_study(direction='minimize')\n",
    "study_lower.optimize(objective_lower, n_trials=N_TRIALS, show_progress_bar=True)\n",
    "\n",
    "# Extract best parameters and add the optimal number of estimators\n",
    "best_params_lower = study_lower.best_params\n",
    "best_params_lower['n_estimators'] = study_lower.best_trial.user_attrs['best_iteration']\n",
    "\n",
    "print(f\"\\nLower-Bound Model Tuning Complete.\")\n",
    "print(f\"Best Validation Score (Pinball Loss): {study_lower.best_value:,.4f}\")\n",
    "print(f\"Best Parameters Found: {best_params_lower}\")\n",
    "\n",
    "# --- Step 3: Tune the Upper-Bound Model (alpha=0.95) ---\n",
    "\n",
    "def objective_upper(trial):\n",
    "    \"\"\"Optuna objective function for the upper-bound quantile model.\"\"\"\n",
    "    # The parameter space is identical, only quantile_alpha changes.\n",
    "    params = {\n",
    "        'objective': 'reg:quantileerror',\n",
    "        'quantile_alpha': 0.95, # The only key difference from the lower model\n",
    "        'eval_metric': 'quantile',\n",
    "        'tree_method': 'hist',\n",
    "        'seed': RANDOM_STATE,\n",
    "        'n_jobs': -1,\n",
    "\n",
    "        # --- Hyperparameters to Tune ---\n",
    "        'eta': trial.suggest_float('eta', 0.01, 0.1, log=True),\n",
    "        'max_depth': trial.suggest_int('max_depth', 3, 8),\n",
    "        'subsample': trial.suggest_float('subsample', 0.6, 0.95),\n",
    "        'colsample_bytree': trial.suggest_float('colsample_bytree', 0.6, 0.95),\n",
    "        'lambda': trial.suggest_float('lambda', 1e-2, 100.0, log=True),\n",
    "        'alpha': trial.suggest_float('alpha', 1e-2, 100.0, log=True),\n",
    "        'min_child_weight': trial.suggest_int('min_child_weight', 1, 20),\n",
    "    }\n",
    "\n",
    "    bst = xgb.train(\n",
    "        params,\n",
    "        dtrain_opt,\n",
    "        num_boost_round=2000,\n",
    "        evals=[(dval_opt, 'validation')],\n",
    "        early_stopping_rounds=50,\n",
    "        verbose_eval=False\n",
    "    )\n",
    "    trial.set_user_attr('best_iteration', bst.best_iteration)\n",
    "    return bst.best_score\n",
    "\n",
    "print(\"\\n--- Tuning the XGBoost Upper-Bound Model (alpha=0.95)... ---\")\n",
    "study_upper = optuna.create_study(direction='minimize')\n",
    "study_upper.optimize(objective_upper, n_trials=N_TRIALS, show_progress_bar=True)\n",
    "\n",
    "# Extract best parameters\n",
    "best_params_upper = study_upper.best_params\n",
    "best_params_upper['n_estimators'] = study_upper.best_trial.user_attrs['best_iteration']\n",
    "\n",
    "print(f\"\\nUpper-Bound Model Tuning Complete.\")\n",
    "print(f\"Best Validation Score (Pinball Loss): {study_upper.best_value:,.4f}\")\n",
    "print(f\"Best Parameters Found: {best_params_upper}\")\n",
    "\n",
    "# Clean up to free memory\n",
    "del dtrain_opt, dval_opt, X_train_opt, X_val_opt, y_train_opt, y_val_opt\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "59e26397-9fc6-4e8c-82aa-7cbefe906e4f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Initializing K-Fold training for tuned quantile models ---\n",
      "Prediction artifacts will be saved to: './meta_quantile_models/'\n",
      "\n",
      "===== FOLD 1/5 =====\n",
      "  -> Training lower-bound model (alpha=0.05)...\n",
      "  -> Training upper-bound model (alpha=0.95)...\n",
      "\n",
      "===== FOLD 2/5 =====\n",
      "  -> Training lower-bound model (alpha=0.05)...\n",
      "  -> Training upper-bound model (alpha=0.95)...\n",
      "\n",
      "===== FOLD 3/5 =====\n",
      "  -> Training lower-bound model (alpha=0.05)...\n",
      "  -> Training upper-bound model (alpha=0.95)...\n",
      "\n",
      "===== FOLD 4/5 =====\n",
      "  -> Training lower-bound model (alpha=0.05)...\n",
      "  -> Training upper-bound model (alpha=0.95)...\n",
      "\n",
      "===== FOLD 5/5 =====\n",
      "  -> Training lower-bound model (alpha=0.05)...\n",
      "  -> Training upper-bound model (alpha=0.95)...\n",
      "\n",
      "--- K-Fold training complete. ---\n",
      "\n",
      "--- Saving quantile prediction arrays to disk... ---\n",
      "Saved 'oof_lower_preds.npy' successfully.\n",
      "Saved 'test_lower_preds.npy' successfully.\n",
      "Saved 'oof_upper_preds.npy' successfully.\n",
      "Saved 'test_upper_preds.npy' successfully.\n",
      "\n",
      "All prediction artifacts are now ready for the final calibration and submission step.\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# BLOCK 7: K-FOLD TRAINING & PREDICTION FOR QUANTILE MODELS (Corrected)\n",
    "# =============================================================================\n",
    "# With our tuned hyperparameters, we now perform the final model training using\n",
    "# a 5-Fold cross-validation strategy.\n",
    "#\n",
    "# THIS IS THE CORRECTED VERSION using the native `xgb.train` API to properly\n",
    "# support early stopping during the final K-Fold training loop.\n",
    "\n",
    "# --- Step 1: Setup and Initialization ---\n",
    "print(\"\\n--- Initializing K-Fold training for tuned quantile models ---\")\n",
    "\n",
    "# Define and create the directory to save our new prediction artifacts\n",
    "META_QUANTILE_PATH = './meta_quantile_models/'\n",
    "os.makedirs(META_QUANTILE_PATH, exist_ok=True)\n",
    "print(f\"Prediction artifacts will be saved to: '{META_QUANTILE_PATH}'\")\n",
    "\n",
    "# Initialize arrays to store the predictions\n",
    "oof_lower_preds = np.zeros(len(X_for_quantile))\n",
    "oof_upper_preds = np.zeros(len(X_for_quantile))\n",
    "test_lower_preds = np.zeros(len(X_test_for_quantile))\n",
    "test_upper_preds = np.zeros(len(X_test_for_quantile))\n",
    "\n",
    "# Initialize the consistent StratifiedKFold splitter\n",
    "skf = StratifiedKFold(n_splits=N_SPLITS, shuffle=True, random_state=RANDOM_STATE)\n",
    "\n",
    "# Prepare the full test data as a DMatrix once, for efficiency\n",
    "dtest = xgb.DMatrix(X_test_for_quantile)\n",
    "\n",
    "# --- Step 2: K-Fold Training Loop with Native API ---\n",
    "for fold, (train_idx, val_idx) in enumerate(skf.split(X_for_quantile, grade_for_stratify)):\n",
    "    print(f\"\\n===== FOLD {fold+1}/{N_SPLITS} =====\")\n",
    "\n",
    "    # Split the data for this fold\n",
    "    X_train, X_val = X_for_quantile.iloc[train_idx], X_for_quantile.iloc[val_idx]\n",
    "    y_train, y_val = y_true.iloc[train_idx], y_true.iloc[val_idx]\n",
    "\n",
    "    # Convert fold data to DMatrix format\n",
    "    dtrain = xgb.DMatrix(X_train, label=y_train)\n",
    "    dval = xgb.DMatrix(X_val, label=y_val)\n",
    "\n",
    "    # --- Train and Predict Lower-Bound Model ---\n",
    "    print(\"  -> Training lower-bound model (alpha=0.05)...\")\n",
    "    # We remove 'n_estimators' as it's not a valid param for xgb.train\n",
    "    # It's controlled by num_boost_round instead.\n",
    "    num_boost_round_lower = best_params_lower.pop('n_estimators')\n",
    "    \n",
    "    lower_model = xgb.train(\n",
    "        params=best_params_lower,\n",
    "        dtrain=dtrain,\n",
    "        num_boost_round=num_boost_round_lower,\n",
    "        evals=[(dval, 'validation')],\n",
    "        early_stopping_rounds=50,\n",
    "        verbose_eval=False\n",
    "    )\n",
    "    \n",
    "    # Restore n_estimators for the next fold to avoid a pop error\n",
    "    best_params_lower['n_estimators'] = num_boost_round_lower\n",
    "    \n",
    "    # Predict on the validation set to generate OOF predictions\n",
    "    oof_lower_preds[val_idx] = lower_model.predict(dval, iteration_range=(0, lower_model.best_iteration))\n",
    "    # Predict on the test set and add to the running average\n",
    "    test_lower_preds += lower_model.predict(dtest, iteration_range=(0, lower_model.best_iteration)) / N_SPLITS\n",
    "\n",
    "    # --- Train and Predict Upper-Bound Model ---\n",
    "    print(\"  -> Training upper-bound model (alpha=0.95)...\")\n",
    "    num_boost_round_upper = best_params_upper.pop('n_estimators')\n",
    "\n",
    "    upper_model = xgb.train(\n",
    "        params=best_params_upper,\n",
    "        dtrain=dtrain,\n",
    "        num_boost_round=num_boost_round_upper,\n",
    "        evals=[(dval, 'validation')],\n",
    "        early_stopping_rounds=50,\n",
    "        verbose_eval=False\n",
    "    )\n",
    "\n",
    "    best_params_upper['n_estimators'] = num_boost_round_upper\n",
    "    \n",
    "    # Predict on the validation set for OOF\n",
    "    oof_upper_preds[val_idx] = upper_model.predict(dval, iteration_range=(0, upper_model.best_iteration))\n",
    "    # Predict on the test set and add to the running average\n",
    "    test_upper_preds += upper_model.predict(dtest, iteration_range=(0, upper_model.best_iteration)) / N_SPLITS\n",
    "\n",
    "    gc.collect()\n",
    "\n",
    "print(\"\\n--- K-Fold training complete. ---\")\n",
    "\n",
    "# --- Step 3: Save the Prediction Artifacts ---\n",
    "print(\"\\n--- Saving quantile prediction arrays to disk... ---\")\n",
    "\n",
    "np.save(os.path.join(META_QUANTILE_PATH, 'oof_lower_preds.npy'), oof_lower_preds)\n",
    "print(f\"Saved 'oof_lower_preds.npy' successfully.\")\n",
    "\n",
    "np.save(os.path.join(META_QUANTILE_PATH, 'test_lower_preds.npy'), test_lower_preds)\n",
    "print(f\"Saved 'test_lower_preds.npy' successfully.\")\n",
    "\n",
    "np.save(os.path.join(META_QUANTILE_PATH, 'oof_upper_preds.npy'), oof_upper_preds)\n",
    "print(f\"Saved 'oof_upper_preds.npy' successfully.\")\n",
    "\n",
    "np.save(os.path.join(META_QUANTILE_PATH, 'test_upper_preds.npy'), test_upper_preds)\n",
    "print(f\"Saved 'test_upper_preds.npy' successfully.\")\n",
    "\n",
    "print(\"\\nAll prediction artifacts are now ready for the final calibration and submission step.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e6a1b2ed-e304-4a00-ab91-fa2d3bb84334",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Loading final bounds from both pipelines ---\n",
      "All necessary prediction bounds loaded/recreated successfully.\n",
      "\n",
      "--- Optimizing the blend weight for the two interval models ---\n",
      "\n",
      "============================================================\n",
      "FINAL ENSEMBLE RESULTS\n",
      "============================================================\n",
      "Original Error Model OOF Score:     $292,680.00\n",
      "Direct Quantile Model OOF Score:  $349,061.10\n",
      "Final BLENDED OOF Winkler Score:    $294,673.93\n",
      "------------------------------------------------------------\n",
      "Optimal Blend Weights:\n",
      "  -> Mean+Error Model:   93.23%\n",
      "  -> Direct Quantile Model: 6.77%\n",
      "\n",
      "--- Creating final blended submission file... ---\n",
      "\n",
      "'submission_FINAL_BLEND_294673.csv' created successfully! This is your best shot!\n",
      "\n",
      "Final Submission Head:\n",
      "       id       pi_lower      pi_upper\n",
      "0  200000  812909.311068  1.016441e+06\n",
      "1  200001  576970.144618  7.998915e+05\n",
      "2  200002  450018.505119  6.540708e+05\n",
      "3  200003  294414.465019  4.243956e+05\n",
      "4  200004  354485.328737  7.902132e+05\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# BLOCK 9: FINAL ENSEMBLE OF INTERVALS\n",
    "# =============================================================================\n",
    "# This is the definitive final step. We have two high-quality but different\n",
    "# interval predictions:\n",
    "# 1. The original \"Mean + Error Model\" method (which is our current best).\n",
    "# 2. The new \"Direct Quantile Model\" method (our second-best, robust alternative).\n",
    "#\n",
    "# By blending these two sets of interval bounds, we can create a final submission\n",
    "# that is more robust and likely more accurate than either method alone.\n",
    "\n",
    "# --- Step 1: Load All Final Prediction Bounds ---\n",
    "print(\"\\n--- Loading final bounds from both pipelines ---\")\n",
    "\n",
    "# Define paths and file names - ADJUST THESE IF YOUR FILENAMES ARE DIFFERENT\n",
    "ERROR_MODEL_SUB_FILE = 'submission_final_OptimalEoE_292680.csv' # From your original best pipeline\n",
    "QUANTILE_MODEL_SUB_FILE = 'submission_direct_quantile_robust_349061.csv' # From our new pipeline\n",
    "\n",
    "try:\n",
    "    # Load the submission files which contain the final, calibrated bounds\n",
    "    df_error_model = pd.read_csv(ERROR_MODEL_SUB_FILE)\n",
    "    df_quantile_model = pd.read_csv(QUANTILE_MODEL_SUB_FILE)\n",
    "\n",
    "    # For blending the test set predictions\n",
    "    test_lower_error = df_error_model['pi_lower'].values\n",
    "    test_upper_error = df_error_model['pi_upper'].values\n",
    "    test_lower_quantile = df_quantile_model['pi_lower'].values\n",
    "    test_upper_quantile = df_quantile_model['pi_upper'].values\n",
    "\n",
    "    # To find the optimal blend weight, we need the OOF predictions that\n",
    "    # CORRESPOND to these submission files. We will recreate them.\n",
    "    # NOTE: This assumes the variables from the previous blocks are available.\n",
    "    # If not, they would need to be loaded from .npy files.\n",
    "    \n",
    "    # Recreate the OOF bounds from the \"Mean+Error\" model\n",
    "    # These values were found in your `submission_final_OptimalEoE_292680.csv` run\n",
    "    best_a_error_model = 1.9799\n",
    "    best_b_error_model = 2.1755\n",
    "    oof_error_final = np.clip((oof_error_preds_xgb * 0.60 + oof_error_preds_cb * 0.40), 0, None)\n",
    "    oof_lower_error = oof_ensemble_mean - oof_error_final * best_a_error_model\n",
    "    oof_upper_error = oof_ensemble_mean + oof_error_final * best_b_error_model\n",
    "    \n",
    "    # Recreate the OOF bounds from the \"Direct Quantile\" model\n",
    "    best_a_quantile = 0.8118\n",
    "    best_b_quantile = 1.1960\n",
    "    oof_lower_quantile_raw = np.minimum(oof_lower_preds, oof_upper_preds)\n",
    "    oof_upper_quantile_raw = np.maximum(oof_lower_preds, oof_upper_preds)\n",
    "    oof_lower_quantile = oof_lower_quantile_raw * best_a_quantile\n",
    "    oof_upper_quantile = oof_upper_quantile_raw * best_b_quantile\n",
    "\n",
    "    print(\"All necessary prediction bounds loaded/recreated successfully.\")\n",
    "\n",
    "except FileNotFoundError as e:\n",
    "    print(f\"\\nERROR: Could not find a required submission file. {e}\")\n",
    "    print(\"Please ensure both pipelines have been run and their submissions are present.\")\n",
    "    # exit()\n",
    "\n",
    "\n",
    "# --- Step 2: Find the Optimal Blend Weight ---\n",
    "print(\"\\n--- Optimizing the blend weight for the two interval models ---\")\n",
    "\n",
    "def get_blended_winkler(weights, y_true_oof, lower_a, upper_a, lower_b, upper_b):\n",
    "    \"\"\"\n",
    "    Calculates Winkler score for a weighted blend of two intervals.\n",
    "    `weights[0]` is the weight for model A (error model).\n",
    "    Weight for model B (quantile model) is `1 - weights[0]`.\n",
    "    \"\"\"\n",
    "    weight_error_model = weights[0]\n",
    "    weight_quantile_model = 1 - weight_error_model\n",
    "\n",
    "    # Calculate the blended interval bounds\n",
    "    final_lower = (lower_a * weight_error_model) + (lower_b * weight_quantile_model)\n",
    "    final_upper = (upper_a * weight_error_model) + (upper_b * weight_quantile_model)\n",
    "\n",
    "    return winkler_score(y_true_oof, final_lower, final_upper)\n",
    "\n",
    "# We are only optimizing one parameter: the weight.\n",
    "# The constraint ensures the weights for the two models sum to 1.\n",
    "initial_guess = [0.5] # Start with a 50/50 blend\n",
    "bounds = [(0, 1)]     # Weight must be between 0 and 1\n",
    "\n",
    "# Run the optimizer\n",
    "result_blend = minimize(\n",
    "    fun=get_blended_winkler,\n",
    "    x0=initial_guess,\n",
    "    args=(y_true, oof_lower_error, oof_upper_error, oof_lower_quantile, oof_upper_quantile),\n",
    "    method='L-BFGS-B', # A good method for single-variable, bounded problems\n",
    "    bounds=bounds\n",
    ")\n",
    "\n",
    "best_weight_error_model = result_blend.x[0]\n",
    "best_weight_quantile_model = 1 - best_weight_error_model\n",
    "best_blended_score = result_blend.fun\n",
    "\n",
    "# --- Step 3: Display Final Results ---\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"FINAL ENSEMBLE RESULTS\")\n",
    "print(\"=\"*60)\n",
    "print(f\"Original Error Model OOF Score:     $292,680.00\") # From your filename\n",
    "print(f\"Direct Quantile Model OOF Score:  $349,061.10\")\n",
    "print(f\"Final BLENDED OOF Winkler Score:    ${best_blended_score:,.2f}\")\n",
    "print(\"-\" * 60)\n",
    "print(f\"Optimal Blend Weights:\")\n",
    "print(f\"  -> Mean+Error Model:   {best_weight_error_model:.2%}\")\n",
    "print(f\"  -> Direct Quantile Model: {best_weight_quantile_model:.2%}\")\n",
    "\n",
    "\n",
    "# --- Step 4: Create and Save the Final Blended Submission ---\n",
    "print(\"\\n--- Creating final blended submission file... ---\")\n",
    "\n",
    "# Apply the optimal blend weights to the test set predictions\n",
    "final_test_lower = (test_lower_error * best_weight_error_model) + (test_lower_quantile * best_weight_quantile_model)\n",
    "final_test_upper = (test_upper_error * best_weight_error_model) + (test_upper_quantile * best_weight_quantile_model)\n",
    "\n",
    "# Final sanity check: ensure pi_upper is always greater than pi_lower\n",
    "final_test_upper = np.maximum(final_test_lower + 1, final_test_upper)\n",
    "\n",
    "# Create the submission DataFrame using the original IDs\n",
    "submission_df_final = pd.DataFrame({\n",
    "    'id': df_error_model['id'],\n",
    "    'pi_lower': final_test_lower,\n",
    "    'pi_upper': final_test_upper\n",
    "})\n",
    "\n",
    "# Save the submission file\n",
    "submission_filename = f'submission_FINAL_BLEND_{int(best_blended_score)}.csv'\n",
    "submission_df_final.to_csv(submission_filename, index=False)\n",
    "\n",
    "print(f\"\\n'{submission_filename}' created successfully! This is your best shot!\")\n",
    "print(\"\\nFinal Submission Head:\")\n",
    "print(submission_df_final.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "335fddf3-c89a-46ff-8cb3-bb30507f8cf2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (Kaggle Comp)",
   "language": "python",
   "name": "kaggle-comp"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c523310f-4b1a-4a46-a800-76c75d17ddce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Libraries imported successfully.\n",
      "Raw data loaded successfully.\n",
      "Setup complete.\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# BLOCK 1: SETUP, IMPORTS, AND DATA LOADING\n",
    "# =============================================================================\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "import time\n",
    "import os\n",
    "# --- Library Imports ---\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import gc\n",
    "from sklearn.model_selection import StratifiedKFold, train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import xgboost as xgb\n",
    "import catboost as cb\n",
    "import optuna\n",
    "print(\"Libraries imported successfully.\")\n",
    "# --- Helper Function for Winkler Score ---\n",
    "def winkler_score(y_true, lower, upper, alpha=0.1, return_coverage=False):\n",
    "    width = upper - lower\n",
    "    penalty_lower = np.where(y_true < lower, (2 / alpha) * (lower - y_true), 0)\n",
    "    penalty_upper = np.where(y_true > upper, (2 / alpha) * (y_true - upper), 0)\n",
    "    score = width + penalty_lower + penalty_upper\n",
    "    if return_coverage:\n",
    "        coverage = np.mean((y_true >= lower) & (y_true <= upper))\n",
    "        return np.mean(score), coverage\n",
    "    return np.mean(score)\n",
    "# --- Global Constants ---\n",
    "N_SPLITS = 5\n",
    "RANDOM_STATE = 42\n",
    "DATA_PATH = './'\n",
    "N_OPTUNA_TRIALS = 75 # A strong number for a comprehensive search\n",
    "COMPETITION_ALPHA = 0.1\n",
    "\n",
    "# --- Load Raw Data ---\n",
    "try:\n",
    "    # We drop the low-variance columns they identified right away\n",
    "    drop_cols=['id', 'golf', 'view_rainier', 'view_skyline', 'view_lakesamm','view_otherwater', 'view_other']\n",
    "    df_train = pd.read_csv(DATA_PATH + 'dataset.csv').drop(columns=drop_cols)\n",
    "    df_test = pd.read_csv(DATA_PATH + 'test.csv').drop(columns=drop_cols)\n",
    "    print(\"Raw data loaded successfully.\")\n",
    "except FileNotFoundError:\n",
    "    print(\"ERROR: Could not find 'dataset.csv' or 'test.csv'.\")\n",
    "    exit()\n",
    "# --- Prepare Target Variable ---\n",
    "y_true = df_train['sale_price'].copy()\n",
    "grade_for_stratify = df_train['grade'].copy()\n",
    "# The mean-error model works best when predicting the raw price directly\n",
    "# So, we will NOT log-transform the target this time.\n",
    "# df_train.drop('sale_price', axis=1, inplace=True) # We keep sale_price for FE\n",
    "print(\"Setup complete.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3d31642f-c332-4019-9c08-1c7ab4ebc8ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Starting Block 2.5: Executing Feature Engineering Pipeline ---\n",
      "--- Starting Comprehensive Feature Engineering ---\n",
      "Step 1: Creating brute-force numerical interaction features...\n",
      "Step 2: Creating date features...\n",
      "Step 3: Creating TF-IDF features for text columns...\n",
      "Step 4: Creating group-by aggregation features...\n",
      "Step 5: Creating ratio features...\n",
      "Step 6: Creating geospatial clustering features...\n",
      "Step 7: Finalizing feature set...\n",
      "\n",
      "Comprehensive FE complete. Total features: 233\n",
      "Feature engineering complete. X shape: (200000, 233), X_test shape: (200000, 233)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Make sure to have these libraries installed\n",
    "# pip install pandas numpy scikit-learn\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.cluster import KMeans\n",
    "import gc\n",
    "\n",
    "# Define a random state for reproducibility\n",
    "RANDOM_STATE = 42\n",
    "\n",
    "def create_comprehensive_features(df_train, df_test):\n",
    "    \"\"\"\n",
    "    Combines original and new advanced feature engineering steps into a single pipeline.\n",
    "    \"\"\"\n",
    "    print(\"--- Starting Comprehensive Feature Engineering ---\")\n",
    "\n",
    "    # Store original indices and target variable\n",
    "    train_ids = df_train.index\n",
    "    test_ids = df_test.index\n",
    "    y_train = df_train['sale_price'].copy() # Keep the target separate\n",
    "\n",
    "    # Combine for consistent processing\n",
    "    df_train_temp = df_train.drop(columns=['sale_price'])\n",
    "    all_data = pd.concat([df_train_temp, df_test], axis=0, ignore_index=True)\n",
    "\n",
    "    # --- Original Feature Engineering ---\n",
    "\n",
    "    # A) Brute-Force Numerical Interactions\n",
    "    print(\"Step 1: Creating brute-force numerical interaction features...\")\n",
    "    NUMS = ['area', 'land_val', 'imp_val', 'sqft_lot', 'sqft', 'sqft_1', 'grade', 'year_built']\n",
    "    # Ensure all columns exist and are numeric, fill missing with 0 for safety\n",
    "    for col in NUMS:\n",
    "        if col not in all_data.columns:\n",
    "            all_data[col] = 0\n",
    "        else:\n",
    "            all_data[col] = pd.to_numeric(all_data[col], errors='coerce').fillna(0)\n",
    "            \n",
    "    for i in range(len(NUMS)):\n",
    "        for j in range(i + 1, len(NUMS)):\n",
    "            all_data[f'{NUMS[i]}_x_{NUMS[j]}'] = all_data[NUMS[i]] * all_data[NUMS[j]]\n",
    "\n",
    "    # B) Date Features\n",
    "    print(\"Step 2: Creating date features...\")\n",
    "    all_data['sale_date'] = pd.to_datetime(all_data['sale_date'])\n",
    "    all_data['sale_year'] = all_data['sale_date'].dt.year\n",
    "    all_data['sale_month'] = all_data['sale_date'].dt.month\n",
    "    all_data['sale_dayofyear'] = all_data['sale_date'].dt.dayofyear\n",
    "    all_data['age_at_sale'] = all_data['sale_year'] - all_data['year_built']\n",
    "\n",
    "    # C) TF-IDF Text Features\n",
    "    print(\"Step 3: Creating TF-IDF features for text columns...\")\n",
    "    text_cols = ['subdivision', 'zoning', 'city', 'sale_warning', 'join_status', 'submarket']\n",
    "    all_data[text_cols] = all_data[text_cols].fillna('missing').astype(str)\n",
    "    \n",
    "    for col in text_cols:\n",
    "        tfidf = TfidfVectorizer(analyzer='char', ngram_range=(3, 5), max_features=128, binary=True)\n",
    "        svd = TruncatedSVD(n_components=8, random_state=RANDOM_STATE)\n",
    "        \n",
    "        tfidf_matrix = tfidf.fit_transform(all_data[col])\n",
    "        tfidf_svd = svd.fit_transform(tfidf_matrix)\n",
    "        \n",
    "        tfidf_df = pd.DataFrame(tfidf_svd, columns=[f'{col}_tfidf_svd_{i}' for i in range(8)])\n",
    "        all_data = pd.concat([all_data, tfidf_df], axis=1)\n",
    "\n",
    "    # D) Log transform some interaction features\n",
    "    for c in ['land_val_x_imp_val', 'land_val_x_sqft', 'imp_val_x_sqft']:\n",
    "        if c in all_data.columns:\n",
    "            all_data[c] = np.log1p(all_data[c].fillna(0))\n",
    "\n",
    "    # --- New Feature Engineering Ideas ---\n",
    "\n",
    "    # F) Group-By Aggregation Features\n",
    "    print(\"Step 4: Creating group-by aggregation features...\")\n",
    "    group_cols = ['submarket', 'city', 'zoning']\n",
    "    num_cols_for_agg = ['grade', 'sqft', 'imp_val', 'land_val', 'age_at_sale']\n",
    "\n",
    "    for group_col in group_cols:\n",
    "        for num_col in num_cols_for_agg:\n",
    "            agg_stats = all_data.groupby(group_col)[num_col].agg(['mean', 'std', 'max', 'min']).reset_index()\n",
    "            agg_stats.columns = [group_col] + [f'{group_col}_{num_col}_{stat}' for stat in ['mean', 'std', 'max', 'min']]\n",
    "            all_data = pd.merge(all_data, agg_stats, on=group_col, how='left')\n",
    "            all_data[f'{num_col}_minus_{group_col}_mean'] = all_data[num_col] - all_data[f'{group_col}_{num_col}_mean']\n",
    "\n",
    "    # G) Ratio Features\n",
    "    print(\"Step 5: Creating ratio features...\")\n",
    "    # Add a small epsilon to prevent division by zero\n",
    "    epsilon = 1e-6 \n",
    "    all_data['total_val'] = all_data['imp_val'] + all_data['land_val']\n",
    "    all_data['imp_val_to_land_val_ratio'] = all_data['imp_val'] / (all_data['land_val'] + epsilon)\n",
    "    all_data['land_val_ratio'] = all_data['land_val'] / (all_data['total_val'] + epsilon)\n",
    "    all_data['sqft_to_lot_ratio'] = all_data['sqft'] / (all_data['sqft_lot'] + epsilon)\n",
    "    all_data['was_renovated'] = (all_data['year_reno'] > 0).astype(int)\n",
    "    all_data['reno_age_at_sale'] = np.where(all_data['was_renovated'] == 1, all_data['sale_year'] - all_data['year_reno'], -1)\n",
    "\n",
    "    # H) Geospatial Clustering Features\n",
    "    print(\"Step 6: Creating geospatial clustering features...\")\n",
    "    coords = all_data[['latitude', 'longitude']].copy()\n",
    "    coords.fillna(coords.median(), inplace=True) # Simple imputation\n",
    "\n",
    "    # KMeans is sensitive to feature scaling, but for lat/lon it's often okay without it.\n",
    "    kmeans = KMeans(n_clusters=20, random_state=RANDOM_STATE, n_init=10) \n",
    "    all_data['location_cluster'] = kmeans.fit_predict(coords)\n",
    "    \n",
    "    # Calculate distance to each cluster center\n",
    "    cluster_centers = kmeans.cluster_centers_\n",
    "    for i in range(len(cluster_centers)):\n",
    "        center = cluster_centers[i]\n",
    "        all_data[f'dist_to_cluster_{i}'] = np.sqrt((coords['latitude'] - center[0])**2 + (coords['longitude'] - center[1])**2)\n",
    "\n",
    "    # --- Final Cleanup ---\n",
    "    print(\"Step 7: Finalizing feature set...\")\n",
    "    cols_to_drop = ['sale_date', 'subdivision', 'zoning', 'city', 'sale_warning', 'join_status', 'submarket']\n",
    "    all_data = all_data.drop(columns=cols_to_drop)\n",
    "\n",
    "    # One-hot encode the new cluster feature\n",
    "    all_data = pd.get_dummies(all_data, columns=['location_cluster'], prefix='loc_cluster')\n",
    "    \n",
    "    # Final check for any remaining object columns to be safe (besides index)\n",
    "    object_cols = all_data.select_dtypes(include='object').columns\n",
    "    if len(object_cols) > 0:\n",
    "        print(f\"Warning: Found unexpected object columns: {object_cols}. Dropping them.\")\n",
    "        all_data = all_data.drop(columns=object_cols)\n",
    "        \n",
    "    all_data.fillna(0, inplace=True)\n",
    "\n",
    "    # Separate back into train and test sets\n",
    "    train_len = len(train_ids)\n",
    "    X = all_data.iloc[:train_len].copy()\n",
    "    X_test = all_data.iloc[train_len:].copy()\n",
    "    \n",
    "    # Restore original indices\n",
    "    X.index = train_ids\n",
    "    X_test.index = test_ids\n",
    "    \n",
    "    # Align columns - crucial for model prediction\n",
    "    X_test = X_test[X.columns]\n",
    "    \n",
    "    print(f\"\\nComprehensive FE complete. Total features: {X.shape[1]}\")\n",
    "    gc.collect()\n",
    "    \n",
    "    return X, X_test, y_train\n",
    "# =============================================================================\n",
    "# BLOCK 2.5: EXECUTE FEATURE ENGINEERING\n",
    "# =============================================================================\n",
    "print(\"\\n--- Starting Block 2.5: Executing Feature Engineering Pipeline ---\")\n",
    "\n",
    "# This is the crucial step that was missing.\n",
    "# We call the function to create our training and testing dataframes.\n",
    "X, X_test, y_train = create_comprehensive_features(df_train, df_test)\n",
    "\n",
    "# Let's verify the output\n",
    "print(f\"Feature engineering complete. X shape: {X.shape}, X_test shape: {X_test.shape}\")\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "791f2c7f-718d-4b8c-a4c9-e81252a169e7",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-07-24 12:55:54,236] A new study created in memory with name: no-name-fcab0ed8-99b2-43ad-9fc6-16471ba7d77f\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Starting XGBoost Hyperparameter Tuning... ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-07-24 12:56:41,062] Trial 0 finished with value: 100074.45292381068 and parameters: {'learning_rate': 0.08443487752486299, 'max_depth': 9, 'lambda': 3.340441544565785, 'alpha': 0.004250127879555757, 'subsample': 0.9879905530373351, 'colsample_bytree': 0.9975926226984286}. Best is trial 0 with value: 100074.45292381068.\n",
      "[I 2025-07-24 12:57:32,012] Trial 1 finished with value: 98936.7604078484 and parameters: {'learning_rate': 0.061070515835173894, 'max_depth': 6, 'lambda': 0.008098523552529477, 'alpha': 0.0034115202023448886, 'subsample': 0.9617865579778011, 'colsample_bytree': 0.7724826615844702}. Best is trial 1 with value: 98936.7604078484.\n",
      "[I 2025-07-24 12:58:44,709] Trial 2 finished with value: 98057.25804855039 and parameters: {'learning_rate': 0.04113171607826967, 'max_depth': 8, 'lambda': 0.010519921186851675, 'alpha': 1.7075068460265435, 'subsample': 0.8519866462663147, 'colsample_bytree': 0.7826855118657211}. Best is trial 2 with value: 98057.25804855039.\n",
      "[I 2025-07-24 12:59:46,050] Trial 3 finished with value: 97945.50256137339 and parameters: {'learning_rate': 0.043830030843289174, 'max_depth': 7, 'lambda': 0.30975304470723936, 'alpha': 0.0249376593233516, 'subsample': 0.7759834485979021, 'colsample_bytree': 0.9210218426484282}. Best is trial 3 with value: 97945.50256137339.\n",
      "[I 2025-07-24 13:01:21,935] Trial 4 finished with value: 99351.67759026517 and parameters: {'learning_rate': 0.01275281301897497, 'max_depth': 9, 'lambda': 0.006344482482805716, 'alpha': 0.3725781998548447, 'subsample': 0.9948770744418958, 'colsample_bytree': 0.9730420323501336}. Best is trial 3 with value: 97945.50256137339.\n",
      "[I 2025-07-24 13:02:22,719] Trial 5 finished with value: 97890.95516951503 and parameters: {'learning_rate': 0.02460791973345851, 'max_depth': 7, 'lambda': 0.5936208789633219, 'alpha': 0.8559628801234108, 'subsample': 0.7749485687117332, 'colsample_bytree': 0.9153229382287092}. Best is trial 5 with value: 97890.95516951503.\n",
      "[I 2025-07-24 13:03:25,126] Trial 6 finished with value: 99757.14510750596 and parameters: {'learning_rate': 0.053387093331739, 'max_depth': 9, 'lambda': 0.1296590071232141, 'alpha': 0.5223896411626594, 'subsample': 0.9711043836612034, 'colsample_bytree': 0.9055093778382178}. Best is trial 5 with value: 97890.95516951503.\n",
      "[I 2025-07-24 13:05:27,396] Trial 7 finished with value: 99648.015414257 and parameters: {'learning_rate': 0.019401348683441563, 'max_depth': 10, 'lambda': 0.0030958641865697183, 'alpha': 0.1660096130447074, 'subsample': 0.9735335853518507, 'colsample_bytree': 0.8303932877464955}. Best is trial 5 with value: 97890.95516951503.\n",
      "[I 2025-07-24 13:06:33,787] Trial 8 finished with value: 98841.28691999108 and parameters: {'learning_rate': 0.05028153634903697, 'max_depth': 9, 'lambda': 0.4039063517755741, 'alpha': 1.5164989959341708, 'subsample': 0.9150545613024023, 'colsample_bytree': 0.8089398384873283}. Best is trial 5 with value: 97890.95516951503.\n",
      "[I 2025-07-24 13:07:35,713] Trial 9 finished with value: 98673.69546135384 and parameters: {'learning_rate': 0.014213387907866271, 'max_depth': 7, 'lambda': 0.04920600690184531, 'alpha': 0.1267486513338846, 'subsample': 0.7136553130276067, 'colsample_bytree': 0.7153758520560984}. Best is trial 5 with value: 97890.95516951503.\n",
      "[I 2025-07-24 13:08:26,991] Trial 10 finished with value: 99141.7377293741 and parameters: {'learning_rate': 0.024272516958355906, 'max_depth': 6, 'lambda': 8.716573744169585, 'alpha': 7.320621222531015, 'subsample': 0.7928025255353469, 'colsample_bytree': 0.8875613812512045}. Best is trial 5 with value: 97890.95516951503.\n",
      "[I 2025-07-24 13:09:27,735] Trial 11 finished with value: 98126.3290253946 and parameters: {'learning_rate': 0.030547497989052312, 'max_depth': 7, 'lambda': 0.807627428440078, 'alpha': 0.016520413841436817, 'subsample': 0.7651088260408946, 'colsample_bytree': 0.9316961389239194}. Best is trial 5 with value: 97890.95516951503.\n",
      "[I 2025-07-24 13:10:28,345] Trial 12 finished with value: 97803.37106664575 and parameters: {'learning_rate': 0.03320679408443967, 'max_depth': 7, 'lambda': 0.7600368862666125, 'alpha': 0.029128368960816088, 'subsample': 0.8147183286970915, 'colsample_bytree': 0.872318057887031}. Best is trial 12 with value: 97803.37106664575.\n",
      "[I 2025-07-24 13:11:42,150] Trial 13 finished with value: 97552.54452857701 and parameters: {'learning_rate': 0.020006617500781015, 'max_depth': 8, 'lambda': 1.5854766776332596, 'alpha': 0.03731814396179682, 'subsample': 0.8402635863108263, 'colsample_bytree': 0.8594519709634867}. Best is trial 13 with value: 97552.54452857701.\n",
      "[I 2025-07-24 13:12:56,394] Trial 14 finished with value: 97838.07814956302 and parameters: {'learning_rate': 0.017735393651776534, 'max_depth': 8, 'lambda': 2.615792410591728, 'alpha': 0.013933156003336323, 'subsample': 0.8525727350119323, 'colsample_bytree': 0.8683590770009739}. Best is trial 13 with value: 97552.54452857701.\n",
      "[I 2025-07-24 13:14:13,431] Trial 15 finished with value: 98970.19058282145 and parameters: {'learning_rate': 0.010104737825967953, 'max_depth': 8, 'lambda': 1.9513400655294277, 'alpha': 0.036811578185592206, 'subsample': 0.8864582707042018, 'colsample_bytree': 0.8527223596785787}. Best is trial 13 with value: 97552.54452857701.\n",
      "[I 2025-07-24 13:15:25,959] Trial 16 finished with value: 97807.2605893857 and parameters: {'learning_rate': 0.03330980484975516, 'max_depth': 8, 'lambda': 0.140281133415157, 'alpha': 0.0011019942215691446, 'subsample': 0.823399191211047, 'colsample_bytree': 0.8244039229540674}. Best is trial 13 with value: 97552.54452857701.\n",
      "[I 2025-07-24 13:16:17,879] Trial 17 finished with value: 98855.26163032497 and parameters: {'learning_rate': 0.029466092252151674, 'max_depth': 6, 'lambda': 0.04209858775864735, 'alpha': 0.06267376396792146, 'subsample': 0.7162047580670772, 'colsample_bytree': 0.953041404982483}. Best is trial 13 with value: 97552.54452857701.\n",
      "[I 2025-07-24 13:18:18,624] Trial 18 finished with value: 98572.88227499487 and parameters: {'learning_rate': 0.018724509052825713, 'max_depth': 10, 'lambda': 7.45052336768821, 'alpha': 0.009066929013555203, 'subsample': 0.9096394918902156, 'colsample_bytree': 0.7284356939517312}. Best is trial 13 with value: 97552.54452857701.\n",
      "[I 2025-07-24 13:18:55,418] Trial 19 finished with value: 98545.85872577295 and parameters: {'learning_rate': 0.08091865468162798, 'max_depth': 7, 'lambda': 1.072676356331732, 'alpha': 0.0665092935206055, 'subsample': 0.822299049493985, 'colsample_bytree': 0.875729032537643}. Best is trial 13 with value: 97552.54452857701.\n",
      "[I 2025-07-24 13:19:54,653] Trial 20 finished with value: 97869.39869029542 and parameters: {'learning_rate': 0.036234133343126314, 'max_depth': 7, 'lambda': 0.23493820295745804, 'alpha': 0.21185252412834024, 'subsample': 0.8844990575925094, 'colsample_bytree': 0.7682761528287703}. Best is trial 13 with value: 97552.54452857701.\n",
      "[I 2025-07-24 13:21:07,707] Trial 21 finished with value: 97788.1936432001 and parameters: {'learning_rate': 0.025605177278481283, 'max_depth': 8, 'lambda': 0.04619667163464484, 'alpha': 0.002523185569533612, 'subsample': 0.8182229794920791, 'colsample_bytree': 0.8150296596940281}. Best is trial 13 with value: 97552.54452857701.\n",
      "[I 2025-07-24 13:22:20,302] Trial 22 finished with value: 97729.68695335108 and parameters: {'learning_rate': 0.024626884798487438, 'max_depth': 8, 'lambda': 0.03482265457973711, 'alpha': 0.0017559488787355206, 'subsample': 0.8124310601772764, 'colsample_bytree': 0.7968231803807644}. Best is trial 13 with value: 97552.54452857701.\n",
      "[I 2025-07-24 13:23:33,427] Trial 23 finished with value: 98269.95243715141 and parameters: {'learning_rate': 0.023146107449160718, 'max_depth': 8, 'lambda': 0.027601080561224928, 'alpha': 0.001088946273255685, 'subsample': 0.8702282631349318, 'colsample_bytree': 0.7956779366973619}. Best is trial 13 with value: 97552.54452857701.\n",
      "[I 2025-07-24 13:24:45,876] Trial 24 finished with value: 98062.79783893585 and parameters: {'learning_rate': 0.01501694798181738, 'max_depth': 8, 'lambda': 0.0012095500548382882, 'alpha': 0.0034092009751864045, 'subsample': 0.8010788806267182, 'colsample_bytree': 0.7432861762002855}. Best is trial 13 with value: 97552.54452857701.\n",
      "[I 2025-07-24 13:26:18,417] Trial 25 finished with value: 98019.18636675169 and parameters: {'learning_rate': 0.0213704067665827, 'max_depth': 9, 'lambda': 0.019766592776652377, 'alpha': 0.0055547484277120324, 'subsample': 0.7515903684159521, 'colsample_bytree': 0.8318708059678612}. Best is trial 13 with value: 97552.54452857701.\n",
      "[I 2025-07-24 13:27:29,085] Trial 26 finished with value: 97796.5339263105 and parameters: {'learning_rate': 0.028664400207051664, 'max_depth': 8, 'lambda': 0.06658284065725077, 'alpha': 0.0018616744044187793, 'subsample': 0.833669459031576, 'colsample_bytree': 0.7506997048136842}. Best is trial 13 with value: 97552.54452857701.\n",
      "[I 2025-07-24 13:28:43,486] Trial 27 finished with value: 98377.18629845031 and parameters: {'learning_rate': 0.015978783849780992, 'max_depth': 8, 'lambda': 0.016413272815533196, 'alpha': 0.008512818518261224, 'subsample': 0.7974742482037022, 'colsample_bytree': 0.8441439032006276}. Best is trial 13 with value: 97552.54452857701.\n",
      "[I 2025-07-24 13:30:15,357] Trial 28 finished with value: 98299.66657115374 and parameters: {'learning_rate': 0.027531379009848846, 'max_depth': 9, 'lambda': 0.09948138086058865, 'alpha': 0.0031181304973187567, 'subsample': 0.7454963001334962, 'colsample_bytree': 0.8074056580057841}. Best is trial 13 with value: 97552.54452857701.\n",
      "[I 2025-07-24 13:31:51,194] Trial 29 finished with value: 98576.64794463241 and parameters: {'learning_rate': 0.011219673884092726, 'max_depth': 9, 'lambda': 0.03519271252479343, 'alpha': 0.0022278344729115503, 'subsample': 0.8455058707391563, 'colsample_bytree': 0.8523573684507201}. Best is trial 13 with value: 97552.54452857701.\n",
      "[I 2025-07-24 13:33:02,611] Trial 30 finished with value: 98118.74210363686 and parameters: {'learning_rate': 0.021032708552980194, 'max_depth': 8, 'lambda': 0.0029150870890884964, 'alpha': 0.006325603996868016, 'subsample': 0.9230953278544985, 'colsample_bytree': 0.7954534901435285}. Best is trial 13 with value: 97552.54452857701.\n",
      "[I 2025-07-24 13:34:13,632] Trial 31 finished with value: 97984.86217778744 and parameters: {'learning_rate': 0.026005106770970815, 'max_depth': 8, 'lambda': 0.057449016682446216, 'alpha': 0.0017533049144411608, 'subsample': 0.8350915528707062, 'colsample_bytree': 0.7570798523989162}. Best is trial 13 with value: 97552.54452857701.\n",
      "[I 2025-07-24 13:35:26,347] Trial 32 finished with value: 97746.72774062568 and parameters: {'learning_rate': 0.03801107127983326, 'max_depth': 8, 'lambda': 0.08254765183811193, 'alpha': 0.002351729508938878, 'subsample': 0.858265543533738, 'colsample_bytree': 0.7027104178750689}. Best is trial 13 with value: 97552.54452857701.\n",
      "[I 2025-07-24 13:36:37,542] Trial 33 finished with value: 97752.50511367983 and parameters: {'learning_rate': 0.038066182694178524, 'max_depth': 8, 'lambda': 0.23388096073882536, 'alpha': 0.004234018943269024, 'subsample': 0.8763925102591772, 'colsample_bytree': 0.7149516768620201}. Best is trial 13 with value: 97552.54452857701.\n",
      "[I 2025-07-24 13:37:47,576] Trial 34 finished with value: 98571.07989669181 and parameters: {'learning_rate': 0.04057248801458243, 'max_depth': 8, 'lambda': 0.2285229675434674, 'alpha': 0.014190499909513207, 'subsample': 0.9429040899836535, 'colsample_bytree': 0.7263146716194209}. Best is trial 13 with value: 97552.54452857701.\n",
      "[I 2025-07-24 13:38:59,106] Trial 35 finished with value: 98312.18195117022 and parameters: {'learning_rate': 0.047820409922600254, 'max_depth': 9, 'lambda': 1.4318980345172456, 'alpha': 0.004424325365453647, 'subsample': 0.8656041894202621, 'colsample_bytree': 0.7109490096063587}. Best is trial 13 with value: 97552.54452857701.\n",
      "[I 2025-07-24 13:39:58,221] Trial 36 finished with value: 97851.06593185381 and parameters: {'learning_rate': 0.06733866809263496, 'max_depth': 7, 'lambda': 0.4090610817249433, 'alpha': 0.0013148684972905601, 'subsample': 0.8965776884083612, 'colsample_bytree': 0.7043926880706732}. Best is trial 13 with value: 97552.54452857701.\n",
      "[I 2025-07-24 13:41:23,995] Trial 37 finished with value: 98299.46864556288 and parameters: {'learning_rate': 0.03836422595886608, 'max_depth': 9, 'lambda': 5.657797221479473, 'alpha': 0.008943596802619155, 'subsample': 0.8701125656301353, 'colsample_bytree': 0.7798360286270871}. Best is trial 13 with value: 97552.54452857701.\n",
      "[I 2025-07-24 13:42:19,380] Trial 38 finished with value: 99623.99710913029 and parameters: {'learning_rate': 0.0573545824785361, 'max_depth': 8, 'lambda': 0.09398887871427299, 'alpha': 0.0039890356555256, 'subsample': 0.9407769563146625, 'colsample_bytree': 0.7406095607966261}. Best is trial 13 with value: 97552.54452857701.\n",
      "[I 2025-07-24 13:43:14,765] Trial 39 finished with value: 99573.3413319047 and parameters: {'learning_rate': 0.06702006340036301, 'max_depth': 9, 'lambda': 0.011110261923942582, 'alpha': 0.039413922218357655, 'subsample': 0.8562767846092154, 'colsample_bytree': 0.9911653079067005}. Best is trial 13 with value: 97552.54452857701.\n",
      "[I 2025-07-24 13:44:15,775] Trial 40 finished with value: 97326.88343926358 and parameters: {'learning_rate': 0.03552717465641824, 'max_depth': 7, 'lambda': 3.7544787161366617, 'alpha': 0.019811345139349127, 'subsample': 0.8907068186135738, 'colsample_bytree': 0.7265925353844132}. Best is trial 40 with value: 97326.88343926358.\n",
      "[I 2025-07-24 13:45:06,031] Trial 41 finished with value: 98049.80677186468 and parameters: {'learning_rate': 0.04514233438660444, 'max_depth': 6, 'lambda': 4.5368594149968695, 'alpha': 0.02084313727505442, 'subsample': 0.8906460199443851, 'colsample_bytree': 0.7280312924307446}. Best is trial 40 with value: 97326.88343926358.\n",
      "[I 2025-07-24 13:46:04,741] Trial 42 finished with value: 97823.66503050271 and parameters: {'learning_rate': 0.03746710122151408, 'max_depth': 7, 'lambda': 0.15798814685129883, 'alpha': 0.06411067661882684, 'subsample': 0.8388354923506196, 'colsample_bytree': 0.7013378151094254}. Best is trial 40 with value: 97326.88343926358.\n",
      "[I 2025-07-24 13:47:03,605] Trial 43 finished with value: 97489.24338612953 and parameters: {'learning_rate': 0.03377044148068544, 'max_depth': 7, 'lambda': 2.4442319686222422, 'alpha': 0.006343293115527683, 'subsample': 0.9016486095401396, 'colsample_bytree': 0.7229397635226325}. Best is trial 40 with value: 97326.88343926358.\n",
      "[I 2025-07-24 13:48:01,108] Trial 44 finished with value: 97713.8221542889 and parameters: {'learning_rate': 0.03351386625462314, 'max_depth': 7, 'lambda': 2.9121691371647724, 'alpha': 0.011805079553190915, 'subsample': 0.9002833897476321, 'colsample_bytree': 0.7675478979487808}. Best is trial 40 with value: 97326.88343926358.\n",
      "[I 2025-07-24 13:48:50,079] Trial 45 finished with value: 98531.41403633666 and parameters: {'learning_rate': 0.032890045176491765, 'max_depth': 6, 'lambda': 3.2436654785183467, 'alpha': 0.011525616991233277, 'subsample': 0.9044778036822787, 'colsample_bytree': 0.7594236135831507}. Best is trial 40 with value: 97326.88343926358.\n",
      "[I 2025-07-24 13:49:49,942] Trial 46 finished with value: 98093.74389837509 and parameters: {'learning_rate': 0.021439303042981344, 'max_depth': 7, 'lambda': 1.8714275701947045, 'alpha': 0.022163477489664962, 'subsample': 0.9277267918536968, 'colsample_bytree': 0.7709132918013512}. Best is trial 40 with value: 97326.88343926358.\n",
      "[I 2025-07-24 13:50:47,671] Trial 47 finished with value: 97963.76538292105 and parameters: {'learning_rate': 0.031037613581842653, 'max_depth': 7, 'lambda': 3.838569328505021, 'alpha': 0.03383315321917732, 'subsample': 0.9775124951657276, 'colsample_bytree': 0.7957908428558484}. Best is trial 40 with value: 97326.88343926358.\n",
      "[I 2025-07-24 13:51:36,449] Trial 48 finished with value: 98079.56672008701 and parameters: {'learning_rate': 0.0427396441187352, 'max_depth': 6, 'lambda': 2.3049485006349997, 'alpha': 0.09948313596678285, 'subsample': 0.9539869268872223, 'colsample_bytree': 0.7353917874039794}. Best is trial 40 with value: 97326.88343926358.\n",
      "[I 2025-07-24 13:52:35,122] Trial 49 finished with value: 98347.29775647118 and parameters: {'learning_rate': 0.016641434955964294, 'max_depth': 7, 'lambda': 1.231049531233, 'alpha': 0.00792737406006203, 'subsample': 0.7820791778011951, 'colsample_bytree': 0.8862375298002725}. Best is trial 40 with value: 97326.88343926358.\n",
      "[I 2025-07-24 13:53:34,582] Trial 50 finished with value: 99482.83807773078 and parameters: {'learning_rate': 0.013251117340059602, 'max_depth': 7, 'lambda': 8.875745553838188, 'alpha': 0.019223201368192326, 'subsample': 0.918110136380773, 'colsample_bytree': 0.7827180942982339}. Best is trial 40 with value: 97326.88343926358.\n",
      "[I 2025-07-24 13:54:31,718] Trial 51 finished with value: 97593.00696258928 and parameters: {'learning_rate': 0.03565613694074822, 'max_depth': 7, 'lambda': 0.5975247621819936, 'alpha': 0.006500577351620261, 'subsample': 0.8553153732675116, 'colsample_bytree': 0.7509696099452505}. Best is trial 40 with value: 97326.88343926358.\n",
      "[I 2025-07-24 13:55:28,368] Trial 52 finished with value: 97739.54090336214 and parameters: {'learning_rate': 0.03411541511637406, 'max_depth': 7, 'lambda': 0.7814613177251005, 'alpha': 5.3043876046575695, 'subsample': 0.8819782048606335, 'colsample_bytree': 0.7529017239897243}. Best is trial 40 with value: 97326.88343926358.\n",
      "[I 2025-07-24 13:56:25,574] Trial 53 finished with value: 97531.84768064224 and parameters: {'learning_rate': 0.028474441839347327, 'max_depth': 7, 'lambda': 0.5646719257107288, 'alpha': 0.005963291195329675, 'subsample': 0.9030616569181887, 'colsample_bytree': 0.7225918129297513}. Best is trial 40 with value: 97326.88343926358.\n",
      "[I 2025-07-24 13:57:22,381] Trial 54 finished with value: 97744.44917231874 and parameters: {'learning_rate': 0.027686226427114508, 'max_depth': 7, 'lambda': 0.4996451936023197, 'alpha': 0.04694078067877254, 'subsample': 0.904586119383019, 'colsample_bytree': 0.724598472711473}. Best is trial 40 with value: 97326.88343926358.\n",
      "[I 2025-07-24 13:58:10,616] Trial 55 finished with value: 98012.93890094307 and parameters: {'learning_rate': 0.04854028616801269, 'max_depth': 6, 'lambda': 1.6060053347615622, 'alpha': 0.00586134805135998, 'subsample': 0.9341015036560245, 'colsample_bytree': 0.7171597237081709}. Best is trial 40 with value: 97326.88343926358.\n",
      "[I 2025-07-24 13:59:08,727] Trial 56 finished with value: 97732.98219127461 and parameters: {'learning_rate': 0.030720803104146523, 'max_depth': 7, 'lambda': 2.716364489820448, 'alpha': 0.013139433949677783, 'subsample': 0.9011698705711346, 'colsample_bytree': 0.9038022873257392}. Best is trial 40 with value: 97326.88343926358.\n",
      "[I 2025-07-24 14:00:05,096] Trial 57 finished with value: 97420.44072985915 and parameters: {'learning_rate': 0.042149053832764516, 'max_depth': 7, 'lambda': 0.9340386687593139, 'alpha': 0.011336703696580762, 'subsample': 0.9145148884290806, 'colsample_bytree': 0.7417402967794119}. Best is trial 40 with value: 97326.88343926358.\n",
      "[I 2025-07-24 14:00:53,546] Trial 58 finished with value: 98303.80729147777 and parameters: {'learning_rate': 0.041340561374054265, 'max_depth': 6, 'lambda': 0.9819916228061534, 'alpha': 0.028877948281794217, 'subsample': 0.952456976380671, 'colsample_bytree': 0.742319392218723}. Best is trial 40 with value: 97326.88343926358.\n",
      "[I 2025-07-24 14:01:43,653] Trial 59 finished with value: 97943.73568534131 and parameters: {'learning_rate': 0.053149076714090734, 'max_depth': 7, 'lambda': 0.526383276470137, 'alpha': 0.016907531128979584, 'subsample': 0.9109764594279562, 'colsample_bytree': 0.7339172306475265}. Best is trial 40 with value: 97326.88343926358.\n",
      "[I 2025-07-24 14:02:41,308] Trial 60 finished with value: 97549.75755992426 and parameters: {'learning_rate': 0.035875821976133086, 'max_depth': 7, 'lambda': 0.619067463160714, 'alpha': 0.09726496382163988, 'subsample': 0.8862986326654586, 'colsample_bytree': 0.7203356098054023}. Best is trial 40 with value: 97326.88343926358.\n",
      "[I 2025-07-24 14:03:40,511] Trial 61 finished with value: 97479.19605741525 and parameters: {'learning_rate': 0.03559394130635165, 'max_depth': 7, 'lambda': 0.6288322432463379, 'alpha': 0.10283145832644819, 'subsample': 0.8800465193924638, 'colsample_bytree': 0.7194328174636765}. Best is trial 40 with value: 97326.88343926358.\n",
      "[I 2025-07-24 14:04:37,965] Trial 62 finished with value: 97639.07414554892 and parameters: {'learning_rate': 0.04454926348835113, 'max_depth': 7, 'lambda': 0.9410941995216731, 'alpha': 0.2683168732150513, 'subsample': 0.8883504044418263, 'colsample_bytree': 0.7180911033026903}. Best is trial 40 with value: 97326.88343926358.\n",
      "[I 2025-07-24 14:05:36,778] Trial 63 finished with value: 98175.41068923521 and parameters: {'learning_rate': 0.023177826079990693, 'max_depth': 7, 'lambda': 0.3286694552642671, 'alpha': 0.10451716914490701, 'subsample': 0.8744871101552457, 'colsample_bytree': 0.7103673784962838}. Best is trial 40 with value: 97326.88343926358.\n",
      "[I 2025-07-24 14:06:34,745] Trial 64 finished with value: 97713.47108766528 and parameters: {'learning_rate': 0.029260906621319457, 'max_depth': 7, 'lambda': 5.02872847948194, 'alpha': 0.15723243029061923, 'subsample': 0.9171256511024554, 'colsample_bytree': 0.7253785904791731}. Best is trial 40 with value: 97326.88343926358.\n",
      "[I 2025-07-24 14:07:25,123] Trial 65 finished with value: 97957.77571994987 and parameters: {'learning_rate': 0.03522331329817011, 'max_depth': 6, 'lambda': 0.6648763682863005, 'alpha': 0.08526047377807999, 'subsample': 0.8927137925142693, 'colsample_bytree': 0.7359435060129463}. Best is trial 40 with value: 97326.88343926358.\n",
      "[I 2025-07-24 14:09:26,575] Trial 66 finished with value: 98605.82830644444 and parameters: {'learning_rate': 0.026173289733980885, 'max_depth': 10, 'lambda': 6.70384890633381, 'alpha': 0.32465030527437155, 'subsample': 0.8783005073685897, 'colsample_bytree': 0.7435062809009525}. Best is trial 40 with value: 97326.88343926358.\n",
      "[I 2025-07-24 14:10:25,497] Trial 67 finished with value: 97558.86871012804 and parameters: {'learning_rate': 0.04039475934757276, 'max_depth': 7, 'lambda': 1.9873982595275579, 'alpha': 0.6114297745111358, 'subsample': 0.8660687219498082, 'colsample_bytree': 0.8685328408724572}. Best is trial 40 with value: 97326.88343926358.\n",
      "[I 2025-07-24 14:11:24,024] Trial 68 finished with value: 97513.76649478781 and parameters: {'learning_rate': 0.03208173252839302, 'max_depth': 7, 'lambda': 1.306412732882253, 'alpha': 0.05154351344780488, 'subsample': 0.927782867241462, 'colsample_bytree': 0.7617156080743996}. Best is trial 40 with value: 97326.88343926358.\n",
      "[I 2025-07-24 14:12:21,874] Trial 69 finished with value: 97941.81194974903 and parameters: {'learning_rate': 0.03112698043724012, 'max_depth': 7, 'lambda': 1.1602994915054403, 'alpha': 0.13858853895420892, 'subsample': 0.9311023392640602, 'colsample_bytree': 0.7592641890941122}. Best is trial 40 with value: 97326.88343926358.\n",
      "[I 2025-07-24 14:13:19,835] Trial 70 finished with value: 97886.37854165409 and parameters: {'learning_rate': 0.03175998831935803, 'max_depth': 7, 'lambda': 0.3221356905437163, 'alpha': 0.04310200954147924, 'subsample': 0.9240671006932404, 'colsample_bytree': 0.710330309683705}. Best is trial 40 with value: 97326.88343926358.\n",
      "[I 2025-07-24 14:14:17,556] Trial 71 finished with value: 97374.97469062572 and parameters: {'learning_rate': 0.04565302675175198, 'max_depth': 7, 'lambda': 1.4776001136696308, 'alpha': 0.05707680485076642, 'subsample': 0.9082932455738033, 'colsample_bytree': 0.7226592664483708}. Best is trial 40 with value: 97326.88343926358.\n",
      "[I 2025-07-24 14:15:14,854] Trial 72 finished with value: 97839.10383890482 and parameters: {'learning_rate': 0.0525351106938538, 'max_depth': 7, 'lambda': 1.3102509004857217, 'alpha': 0.05469240925117825, 'subsample': 0.9157713438228694, 'colsample_bytree': 0.7204908061042211}. Best is trial 40 with value: 97326.88343926358.\n",
      "[I 2025-07-24 14:16:12,086] Trial 73 finished with value: 97630.87249430889 and parameters: {'learning_rate': 0.04430045687683379, 'max_depth': 7, 'lambda': 0.8372841322222425, 'alpha': 0.08270910698115667, 'subsample': 0.91146938874865, 'colsample_bytree': 0.7339848440273058}. Best is trial 40 with value: 97326.88343926358.\n",
      "[I 2025-07-24 14:17:08,736] Trial 74 finished with value: 97876.8637012854 and parameters: {'learning_rate': 0.048045457238742595, 'max_depth': 7, 'lambda': 1.6881119800710684, 'alpha': 0.027954436368572, 'subsample': 0.9395309833610522, 'colsample_bytree': 0.7480663996082146}. Best is trial 40 with value: 97326.88343926358.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- XGBoost Tuning Complete ---\n",
      "Best trial validation RMSE: $97,326.88\n",
      "Best hyperparameters found for XGBoost:\n",
      "  'learning_rate': 0.03552717465641824,\n",
      "  'max_depth': 7,\n",
      "  'lambda': 3.7544787161366617,\n",
      "  'alpha': 0.019811345139349127,\n",
      "  'subsample': 0.8907068186135738,\n",
      "  'colsample_bytree': 0.7265925353844132,\n",
      "  'n_estimators': 2999,\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# BLOCK 3: TUNE XGBOOST MEAN MODEL (NATIVE API)\n",
    "# =============================================================================\n",
    "import xgboost as xgb\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import optuna\n",
    "\n",
    "# --- 1. Prepare Data for Tuning ---\n",
    "X_train_opt, X_val_opt, y_train_opt, y_val_opt = train_test_split(\n",
    "    X, y_true, test_size=0.2, random_state=RANDOM_STATE\n",
    ")\n",
    "\n",
    "# Convert data to DMatrix format for the native API\n",
    "dtrain_opt = xgb.DMatrix(X_train_opt, label=y_train_opt)\n",
    "dval_opt = xgb.DMatrix(X_val_opt, label=y_val_opt)\n",
    "\n",
    "# --- 2. Define the Optuna Objective Function for XGBoost ---\n",
    "def objective_xgboost(trial):\n",
    "    params = {\n",
    "        'learning_rate': trial.suggest_float('learning_rate', 0.01, 0.1, log=True),\n",
    "        'max_depth': trial.suggest_int('max_depth', 6, 10),\n",
    "        'lambda': trial.suggest_float('lambda', 1e-3, 10.0, log=True),\n",
    "        'alpha': trial.suggest_float('alpha', 1e-3, 10.0, log=True),\n",
    "        'subsample': trial.suggest_float('subsample', 0.7, 1.0),\n",
    "        'colsample_bytree': trial.suggest_float('colsample_bytree', 0.7, 1.0),\n",
    "        'objective': 'reg:squarederror', 'eval_metric': 'rmse',\n",
    "        'random_state': RANDOM_STATE, 'n_jobs': -1, 'tree_method': 'hist'\n",
    "    }\n",
    "\n",
    "    model = xgb.train(\n",
    "        params=params,\n",
    "        dtrain=dtrain_opt,\n",
    "        num_boost_round=3000,\n",
    "        evals=[(dval_opt, 'validation')],\n",
    "        early_stopping_rounds=100,\n",
    "        verbose_eval=False\n",
    "    )\n",
    "    \n",
    "    preds = model.predict(dval_opt, iteration_range=(0, model.best_iteration))\n",
    "    rmse = np.sqrt(mean_squared_error(y_val_opt, preds))\n",
    "    trial.set_user_attr(\"best_iteration\", model.best_iteration)\n",
    "    return rmse\n",
    "\n",
    "# --- 3. Create and Run the Optuna Study ---\n",
    "study_xgboost = optuna.create_study(direction='minimize')\n",
    "print(\"--- Starting XGBoost Hyperparameter Tuning... ---\")\n",
    "study_xgboost.optimize(objective_xgboost, n_trials=N_OPTUNA_TRIALS)\n",
    "\n",
    "# --- 4. Print and Store the Best Results ---\n",
    "print(\"\\n--- XGBoost Tuning Complete ---\")\n",
    "print(f\"Best trial validation RMSE: ${study_xgboost.best_value:,.2f}\")\n",
    "print(\"Best hyperparameters found for XGBoost:\")\n",
    "\n",
    "best_params_xgboost = study_xgboost.best_params\n",
    "best_params_xgboost['n_estimators'] = study_xgboost.best_trial.user_attrs['best_iteration']\n",
    "\n",
    "for key, value in best_params_xgboost.items():\n",
    "    print(f\"  '{key}': {value},\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b56c5478-5e75-4112-8dca-6ec297bdc921",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "--- Step 4: K-Fold Cross-Validation with Optimal Hyperparameters ---\n",
      "================================================================================\n",
      "\n",
      "--- Training Fold 1/5 ---\n",
      "  Fold 1 Validation RMSE: $97,274.25\n",
      "\n",
      "--- Training Fold 2/5 ---\n",
      "  Fold 2 Validation RMSE: $96,974.68\n",
      "\n",
      "--- Training Fold 3/5 ---\n",
      "  Fold 3 Validation RMSE: $97,263.04\n",
      "\n",
      "--- Training Fold 4/5 ---\n",
      "  Fold 4 Validation RMSE: $97,126.81\n",
      "\n",
      "--- Training Fold 5/5 ---\n",
      "  Fold 5 Validation RMSE: $96,735.00\n",
      "\n",
      "================================================================================\n",
      "--- Step 5: Final Evaluation and Saving Predictions ---\n",
      "================================================================================\n",
      "Final XGBoost OOF RMSE across all 5 folds: $97,074.97\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# BLOCK 4: K-FOLD TRAINING & SAVING WITH OPTIMAL XGBOOST PARAMETERS\n",
    "# =============================================================================\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "import gc\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"--- Step 4: K-Fold Cross-Validation with Optimal Hyperparameters ---\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Initialize arrays to store the predictions\n",
    "oof_xgb_preds = np.zeros(len(X))\n",
    "test_xgb_preds = np.zeros(len(X_test))\n",
    "\n",
    "# Use StratifiedKFold for the final training, as per the winning strategy\n",
    "skf = StratifiedKFold(n_splits=N_SPLITS, shuffle=True, random_state=RANDOM_STATE)\n",
    "\n",
    "for fold, (train_idx, val_idx) in enumerate(skf.split(X, grade_for_stratify)):\n",
    "    print(f\"\\n--- Training Fold {fold+1}/{N_SPLITS} ---\")\n",
    "    X_train, X_val = X.iloc[train_idx], X.iloc[val_idx]\n",
    "    y_train_fold, y_val_fold = y_true.iloc[train_idx], y_true.iloc[val_idx]\n",
    "\n",
    "    # Initialize and train the model for this fold using the best parameters\n",
    "    # The number of estimators was already determined by early stopping during tuning\n",
    "    model = xgb.XGBRegressor(**best_params_xgboost)\n",
    "    model.fit(X_train, y_train_fold)\n",
    "    \n",
    "    # Generate predictions for the validation set (this fold's OOF part)\n",
    "    oof_preds_fold = model.predict(X_val)\n",
    "    oof_xgb_preds[val_idx] = np.clip(oof_preds_fold, 0, None)\n",
    "    \n",
    "    # Generate predictions for the test set and accumulate them\n",
    "    test_xgb_preds += np.clip(model.predict(X_test), 0, None) / N_SPLITS\n",
    "    \n",
    "    fold_rmse = np.sqrt(mean_squared_error(y_val_fold, oof_preds_fold))\n",
    "    print(f\"  Fold {fold+1} Validation RMSE: ${fold_rmse:,.2f}\")\n",
    "    del model, X_train, X_val, y_train_fold, y_val_fold\n",
    "    gc.collect()\n",
    "\n",
    "# --- Final Evaluation and Saving ---\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"--- Step 5: Final Evaluation and Saving Predictions ---\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "final_oof_rmse = np.sqrt(mean_squared_error(y_true, oof_xgb_preds))\n",
    "print(f\"Final XGBoost OOF RMSE across all {N_SPLITS} folds: ${final_oof_rmse:,.2f}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "facd87b9-b45a-41dd-bb26-5dc40f540209",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "'oof_xgb_preds.npy' and 'test_xgb_preds.npy' saved successfully.\n"
     ]
    }
   ],
   "source": [
    "# Define the save path and save the final prediction arrays\n",
    "SAVE_PATH = './mean_models_v1/'\n",
    "os.makedirs(SAVE_PATH, exist_ok=True)\n",
    "np.save(os.path.join(SAVE_PATH, 'oof_xgb_preds.npy'), oof_xgb_preds)\n",
    "np.save(os.path.join(SAVE_PATH, 'test_xgb_preds.npy'), test_xgb_preds)\n",
    "print(\"\\n'oof_xgb_preds.npy' and 'test_xgb_preds.npy' saved successfully.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7ca44c5-9241-44aa-a6b0-37709a74f105",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (Kaggle Comp)",
   "language": "python",
   "name": "kaggle-comp"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

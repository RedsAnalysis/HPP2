{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "638ecf42-9062-4d63-b745-4404dde5f540",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Libraries imported successfully.\n",
      "Raw data loaded successfully.\n",
      "Setup complete.\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# BLOCK 1: SETUP, IMPORTS, AND DATA LOADING\n",
    "# =============================================================================\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "import time\n",
    "import os\n",
    "# --- Library Imports ---\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import gc\n",
    "from sklearn.model_selection import StratifiedKFold, train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import xgboost as xgb\n",
    "import catboost as cb\n",
    "import optuna\n",
    "from scipy.optimize import minimize\n",
    "print(\"Libraries imported successfully.\")\n",
    "# --- Helper Function for Winkler Score ---\n",
    "def winkler_score(y_true, lower, upper, alpha=0.1, return_coverage=False):\n",
    "    width = upper - lower\n",
    "    penalty_lower = np.where(y_true < lower, (2 / alpha) * (lower - y_true), 0)\n",
    "    penalty_upper = np.where(y_true > upper, (2 / alpha) * (y_true - upper), 0)\n",
    "    score = width + penalty_lower + penalty_upper\n",
    "    if return_coverage:\n",
    "        coverage = np.mean((y_true >= lower) & (y_true <= upper))\n",
    "        return np.mean(score), coverage\n",
    "    return np.mean(score)\n",
    "# --- Global Constants ---\n",
    "N_SPLITS = 5\n",
    "RANDOM_STATE = 42\n",
    "DATA_PATH = './'\n",
    "N_OPTUNA_TRIALS = 30 # A strong number for a comprehensive search\n",
    "COMPETITION_ALPHA = 0.1\n",
    "\n",
    "# --- Load Raw Data ---\n",
    "try:\n",
    "    # We drop the low-variance columns they identified right away\n",
    "    drop_cols=['id', 'golf', 'view_rainier', 'view_skyline', 'view_lakesamm','view_otherwater', 'view_other']\n",
    "    df_train = pd.read_csv(DATA_PATH + 'dataset.csv').drop(columns=drop_cols)\n",
    "    df_test = pd.read_csv(DATA_PATH + 'test.csv').drop(columns=drop_cols)\n",
    "    print(\"Raw data loaded successfully.\")\n",
    "except FileNotFoundError:\n",
    "    print(\"ERROR: Could not find 'dataset.csv' or 'test.csv'.\")\n",
    "    exit()\n",
    "# --- Prepare Target Variable ---\n",
    "y_true = df_train['sale_price'].copy()\n",
    "grade_for_stratify = df_train['grade'].copy()\n",
    "# The mean-error model works best when predicting the raw price directly\n",
    "# So, we will NOT log-transform the target this time.\n",
    "# df_train.drop('sale_price', axis=1, inplace=True) # We keep sale_price for FE\n",
    "print(\"Setup complete.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6e238bea-a0b6-47c2-b9ee-4173028ff907",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Starting Block 2.5: Executing Feature Engineering Pipeline ---\n",
      "--- Starting Comprehensive Feature Engineering ---\n",
      "Step 1: Creating brute-force numerical interaction features...\n",
      "Step 2: Creating date features...\n",
      "Step 3: Creating TF-IDF features for text columns...\n",
      "Step 4: Creating group-by aggregation features...\n",
      "Step 5: Creating ratio features...\n",
      "Step 6: Creating geospatial clustering features...\n",
      "Step 7: Finalizing feature set...\n",
      "\n",
      "Comprehensive FE complete. Total features: 233\n",
      "Feature engineering complete. X shape: (200000, 233), X_test shape: (200000, 233)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Make sure to have these libraries installed\n",
    "# pip install pandas numpy scikit-learn\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.cluster import KMeans\n",
    "import gc\n",
    "\n",
    "# Define a random state for reproducibility\n",
    "RANDOM_STATE = 42\n",
    "\n",
    "def create_comprehensive_features(df_train, df_test):\n",
    "    \"\"\"\n",
    "    Combines original and new advanced feature engineering steps into a single pipeline.\n",
    "    \"\"\"\n",
    "    print(\"--- Starting Comprehensive Feature Engineering ---\")\n",
    "\n",
    "    # Store original indices and target variable\n",
    "    train_ids = df_train.index\n",
    "    test_ids = df_test.index\n",
    "    y_train = df_train['sale_price'].copy() # Keep the target separate\n",
    "\n",
    "    # Combine for consistent processing\n",
    "    df_train_temp = df_train.drop(columns=['sale_price'])\n",
    "    all_data = pd.concat([df_train_temp, df_test], axis=0, ignore_index=True)\n",
    "\n",
    "    # --- Original Feature Engineering ---\n",
    "\n",
    "    # A) Brute-Force Numerical Interactions\n",
    "    print(\"Step 1: Creating brute-force numerical interaction features...\")\n",
    "    NUMS = ['area', 'land_val', 'imp_val', 'sqft_lot', 'sqft', 'sqft_1', 'grade', 'year_built']\n",
    "    # Ensure all columns exist and are numeric, fill missing with 0 for safety\n",
    "    for col in NUMS:\n",
    "        if col not in all_data.columns:\n",
    "            all_data[col] = 0\n",
    "        else:\n",
    "            all_data[col] = pd.to_numeric(all_data[col], errors='coerce').fillna(0)\n",
    "            \n",
    "    for i in range(len(NUMS)):\n",
    "        for j in range(i + 1, len(NUMS)):\n",
    "            all_data[f'{NUMS[i]}_x_{NUMS[j]}'] = all_data[NUMS[i]] * all_data[NUMS[j]]\n",
    "\n",
    "    # B) Date Features\n",
    "    print(\"Step 2: Creating date features...\")\n",
    "    all_data['sale_date'] = pd.to_datetime(all_data['sale_date'])\n",
    "    all_data['sale_year'] = all_data['sale_date'].dt.year\n",
    "    all_data['sale_month'] = all_data['sale_date'].dt.month\n",
    "    all_data['sale_dayofyear'] = all_data['sale_date'].dt.dayofyear\n",
    "    all_data['age_at_sale'] = all_data['sale_year'] - all_data['year_built']\n",
    "\n",
    "    # C) TF-IDF Text Features\n",
    "    print(\"Step 3: Creating TF-IDF features for text columns...\")\n",
    "    text_cols = ['subdivision', 'zoning', 'city', 'sale_warning', 'join_status', 'submarket']\n",
    "    all_data[text_cols] = all_data[text_cols].fillna('missing').astype(str)\n",
    "    \n",
    "    for col in text_cols:\n",
    "        tfidf = TfidfVectorizer(analyzer='char', ngram_range=(3, 5), max_features=128, binary=True)\n",
    "        svd = TruncatedSVD(n_components=8, random_state=RANDOM_STATE)\n",
    "        \n",
    "        tfidf_matrix = tfidf.fit_transform(all_data[col])\n",
    "        tfidf_svd = svd.fit_transform(tfidf_matrix)\n",
    "        \n",
    "        tfidf_df = pd.DataFrame(tfidf_svd, columns=[f'{col}_tfidf_svd_{i}' for i in range(8)])\n",
    "        all_data = pd.concat([all_data, tfidf_df], axis=1)\n",
    "\n",
    "    # D) Log transform some interaction features\n",
    "    for c in ['land_val_x_imp_val', 'land_val_x_sqft', 'imp_val_x_sqft']:\n",
    "        if c in all_data.columns:\n",
    "            all_data[c] = np.log1p(all_data[c].fillna(0))\n",
    "\n",
    "    # --- New Feature Engineering Ideas ---\n",
    "\n",
    "    # F) Group-By Aggregation Features\n",
    "    print(\"Step 4: Creating group-by aggregation features...\")\n",
    "    group_cols = ['submarket', 'city', 'zoning']\n",
    "    num_cols_for_agg = ['grade', 'sqft', 'imp_val', 'land_val', 'age_at_sale']\n",
    "\n",
    "    for group_col in group_cols:\n",
    "        for num_col in num_cols_for_agg:\n",
    "            agg_stats = all_data.groupby(group_col)[num_col].agg(['mean', 'std', 'max', 'min']).reset_index()\n",
    "            agg_stats.columns = [group_col] + [f'{group_col}_{num_col}_{stat}' for stat in ['mean', 'std', 'max', 'min']]\n",
    "            all_data = pd.merge(all_data, agg_stats, on=group_col, how='left')\n",
    "            all_data[f'{num_col}_minus_{group_col}_mean'] = all_data[num_col] - all_data[f'{group_col}_{num_col}_mean']\n",
    "\n",
    "    # G) Ratio Features\n",
    "    print(\"Step 5: Creating ratio features...\")\n",
    "    # Add a small epsilon to prevent division by zero\n",
    "    epsilon = 1e-6 \n",
    "    all_data['total_val'] = all_data['imp_val'] + all_data['land_val']\n",
    "    all_data['imp_val_to_land_val_ratio'] = all_data['imp_val'] / (all_data['land_val'] + epsilon)\n",
    "    all_data['land_val_ratio'] = all_data['land_val'] / (all_data['total_val'] + epsilon)\n",
    "    all_data['sqft_to_lot_ratio'] = all_data['sqft'] / (all_data['sqft_lot'] + epsilon)\n",
    "    all_data['was_renovated'] = (all_data['year_reno'] > 0).astype(int)\n",
    "    all_data['reno_age_at_sale'] = np.where(all_data['was_renovated'] == 1, all_data['sale_year'] - all_data['year_reno'], -1)\n",
    "\n",
    "    # H) Geospatial Clustering Features\n",
    "    print(\"Step 6: Creating geospatial clustering features...\")\n",
    "    coords = all_data[['latitude', 'longitude']].copy()\n",
    "    coords.fillna(coords.median(), inplace=True) # Simple imputation\n",
    "\n",
    "    # KMeans is sensitive to feature scaling, but for lat/lon it's often okay without it.\n",
    "    kmeans = KMeans(n_clusters=20, random_state=RANDOM_STATE, n_init=10) \n",
    "    all_data['location_cluster'] = kmeans.fit_predict(coords)\n",
    "    \n",
    "    # Calculate distance to each cluster center\n",
    "    cluster_centers = kmeans.cluster_centers_\n",
    "    for i in range(len(cluster_centers)):\n",
    "        center = cluster_centers[i]\n",
    "        all_data[f'dist_to_cluster_{i}'] = np.sqrt((coords['latitude'] - center[0])**2 + (coords['longitude'] - center[1])**2)\n",
    "\n",
    "    # --- Final Cleanup ---\n",
    "    print(\"Step 7: Finalizing feature set...\")\n",
    "    cols_to_drop = ['sale_date', 'subdivision', 'zoning', 'city', 'sale_warning', 'join_status', 'submarket']\n",
    "    all_data = all_data.drop(columns=cols_to_drop)\n",
    "\n",
    "    # One-hot encode the new cluster feature\n",
    "    all_data = pd.get_dummies(all_data, columns=['location_cluster'], prefix='loc_cluster')\n",
    "    \n",
    "    # Final check for any remaining object columns to be safe (besides index)\n",
    "    object_cols = all_data.select_dtypes(include='object').columns\n",
    "    if len(object_cols) > 0:\n",
    "        print(f\"Warning: Found unexpected object columns: {object_cols}. Dropping them.\")\n",
    "        all_data = all_data.drop(columns=object_cols)\n",
    "        \n",
    "    all_data.fillna(0, inplace=True)\n",
    "\n",
    "    # Separate back into train and test sets\n",
    "    train_len = len(train_ids)\n",
    "    X = all_data.iloc[:train_len].copy()\n",
    "    X_test = all_data.iloc[train_len:].copy()\n",
    "    \n",
    "    # Restore original indices\n",
    "    X.index = train_ids\n",
    "    X_test.index = test_ids\n",
    "    \n",
    "    # Align columns - crucial for model prediction\n",
    "    X_test = X_test[X.columns]\n",
    "    \n",
    "    print(f\"\\nComprehensive FE complete. Total features: {X.shape[1]}\")\n",
    "    gc.collect()\n",
    "    \n",
    "    return X, X_test, y_train\n",
    "# =============================================================================\n",
    "# BLOCK 2.5: EXECUTE FEATURE ENGINEERING\n",
    "# =============================================================================\n",
    "print(\"\\n--- Starting Block 2.5: Executing Feature Engineering Pipeline ---\")\n",
    "\n",
    "# This is the crucial step that was missing.\n",
    "# We call the function to create our training and testing dataframes.\n",
    "X, X_test, y_train = create_comprehensive_features(df_train, df_test)\n",
    "\n",
    "# Let's verify the output\n",
    "print(f\"Feature engineering complete. X shape: {X.shape}, X_test shape: {X_test.shape}\")\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bca9c850-0f15-4a05-b4ad-63c44f7874a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Loading all base model predictions from saved .npy files... ---\n",
      "All MEAN model predictions loaded successfully.\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# BLOCK 3: LOAD ALL PRE-TRAINED MODEL PREDICTIONS\n",
    "# =============================================================================\n",
    "\n",
    "# Define paths to your saved prediction files\n",
    "PREDS_SAVE_PATH = './mean_models_v1/' # For XGB and CatBoost preds\n",
    "NN_PREDS_PATH = './NN_model_predictions/' # For NN preds\n",
    "\n",
    "print(\"--- Loading all base model predictions from saved .npy files... ---\")\n",
    "try:\n",
    "    # Load Mean Model OOF (Out-of-Fold) Predictions\n",
    "    oof_xgb_preds = np.load(os.path.join(PREDS_SAVE_PATH, 'oof_xgb_preds.npy'))\n",
    "    oof_cb_preds = np.load(os.path.join(PREDS_SAVE_PATH, 'oof_cb_preds.npy'))\n",
    "    oof_lgbm_preds = np.load(os.path.join(PREDS_SAVE_PATH, 'oof_lgbm_preds.npy'))\n",
    "    oof_nn_preds = np.load(os.path.join(NN_PREDS_PATH, 'oof_nn_preds.npy'))\n",
    "    \n",
    "    # Load Mean Model Test Predictions\n",
    "    test_xgb_preds = np.load(os.path.join(PREDS_SAVE_PATH, 'test_xgb_preds.npy'))\n",
    "    test_cb_preds = np.load(os.path.join(PREDS_SAVE_PATH, 'test_cb_preds.npy'))\n",
    "    test_lgbm_preds = np.load(os.path.join(PREDS_SAVE_PATH, 'test_lgbm_preds.npy'))\n",
    "    test_nn_preds = np.load(os.path.join(NN_PREDS_PATH, 'test_nn_preds.npy'))\n",
    "    \n",
    "    print(\"All MEAN model predictions loaded successfully.\")\n",
    "    \n",
    "except FileNotFoundError as e:\n",
    "    print(f\"\\nERROR: Could not find a required prediction file. {e}\")\n",
    "    print(\"Please ensure you have run all training notebooks and saved their predictions first.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "95994879-e2a8-4910-a387-ac7db62213f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Finding optimal weights for the 4-model mean ensemble ---\n",
      "\n",
      "--- STAGE 1 (MEAN) ENSEMBLE RESULTS ---\n",
      "Optimal Weights (XGB/CB/LGBM/NN): 0.2107 / 0.4508 / 0.2485 / 0.0900\n",
      "Final Ensemble Mean OOF RMSE: $94,960.11\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# BLOCK 4: BUILD AND EVALUATE THE STAGE 1 (MEAN) ENSEMBLE\n",
    "# =============================================================================\n",
    "\n",
    "print(\"\\n--- Finding optimal weights for the 4-model mean ensemble ---\")\n",
    "\n",
    "# Stack the OOF predictions for easy matrix multiplication\n",
    "oof_preds_stack_mean = np.vstack([oof_xgb_preds, oof_cb_preds, oof_lgbm_preds, oof_nn_preds]).T\n",
    "\n",
    "# Define the function for the optimizer to minimize (RMSE)\n",
    "def get_ensemble_rmse(weights):\n",
    "    final_prediction = np.dot(oof_preds_stack_mean, weights)\n",
    "    return np.sqrt(mean_squared_error(y_true, final_prediction))\n",
    "\n",
    "# Run the optimizer to find the best weights\n",
    "# Constraints ensure weights sum to 1 and are between 0 and 1\n",
    "result = minimize(get_ensemble_rmse, [1/4]*4, method='SLSQP', bounds=[(0,1)]*4, \n",
    "                  constraints=({'type': 'eq', 'fun': lambda w: 1 - np.sum(w)}))\n",
    "best_mean_weights = result.x\n",
    "\n",
    "# Create the final blended mean predictions for both OOF and test sets\n",
    "oof_ensemble_mean = np.dot(oof_preds_stack_mean, best_mean_weights)\n",
    "test_ensemble_mean = np.dot(np.vstack([test_xgb_preds, test_cb_preds, test_lgbm_preds, test_nn_preds]).T, best_mean_weights)\n",
    "\n",
    "print(\"\\n--- STAGE 1 (MEAN) ENSEMBLE RESULTS ---\")\n",
    "print(f\"Optimal Weights (XGB/CB/LGBM/NN): {best_mean_weights[0]:.4f} / {best_mean_weights[1]:.4f} / {best_mean_weights[2]:.4f} / {best_mean_weights[3]:.4f}\")\n",
    "print(f\"Final Ensemble Mean OOF RMSE: ${np.sqrt(mean_squared_error(y_true, oof_ensemble_mean)):,.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d0bd975f-034f-4583-a83b-24c1edb6b391",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Preparing features for the XGBoost error model ---\n",
      "\n",
      "--- Tuning the XGBoost Error Model with Optuna... ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-07-24 16:50:43,423] A new study created in memory with name: no-name-df0278e3-1ad7-4b3f-a7e5-264898c14c2d\n",
      "[I 2025-07-24 16:51:06,779] Trial 0 finished with value: 61688.205873656196 and parameters: {'eta': 0.021548068707164303, 'max_depth': 3, 'subsample': 0.7438109931926822, 'colsample_bytree': 0.8965328190899706, 'lambda': 2.132582652112447, 'alpha': 23.57962404931865}. Best is trial 0 with value: 61688.205873656196.\n",
      "[I 2025-07-24 16:51:17,000] Trial 1 finished with value: 61442.25476444019 and parameters: {'eta': 0.036417918440802534, 'max_depth': 5, 'subsample': 0.822708661840741, 'colsample_bytree': 0.6789665907117031, 'lambda': 22.94781809488733, 'alpha': 45.089964533130185}. Best is trial 1 with value: 61442.25476444019.\n",
      "[I 2025-07-24 16:51:32,513] Trial 2 finished with value: 61370.7229494235 and parameters: {'eta': 0.020160851841022898, 'max_depth': 6, 'subsample': 0.7532713847261041, 'colsample_bytree': 0.7707238539393368, 'lambda': 38.5562419564283, 'alpha': 9.72823233202442}. Best is trial 2 with value: 61370.7229494235.\n",
      "[I 2025-07-24 16:51:41,272] Trial 3 finished with value: 61681.47744880568 and parameters: {'eta': 0.03655016015860764, 'max_depth': 4, 'subsample': 0.7424947462253012, 'colsample_bytree': 0.8751666418643921, 'lambda': 1.3001699443637438, 'alpha': 3.2083700313985473}. Best is trial 2 with value: 61370.7229494235.\n",
      "[I 2025-07-24 16:51:58,479] Trial 4 finished with value: 61562.459039735375 and parameters: {'eta': 0.02559362186137399, 'max_depth': 4, 'subsample': 0.6946787141584326, 'colsample_bytree': 0.6710155998238062, 'lambda': 3.8457046554189978, 'alpha': 27.10314957223392}. Best is trial 2 with value: 61370.7229494235.\n",
      "[I 2025-07-24 16:52:22,982] Trial 5 finished with value: 61606.667441988444 and parameters: {'eta': 0.01406074973332954, 'max_depth': 4, 'subsample': 0.8462514736947376, 'colsample_bytree': 0.6667412645767682, 'lambda': 1.5632656982344233, 'alpha': 43.33984465588574}. Best is trial 2 with value: 61370.7229494235.\n",
      "[I 2025-07-24 16:52:52,781] Trial 6 finished with value: 61352.62033219648 and parameters: {'eta': 0.011395173066730158, 'max_depth': 5, 'subsample': 0.7879889846792921, 'colsample_bytree': 0.8531517792567727, 'lambda': 34.599242123097326, 'alpha': 16.172012551330454}. Best is trial 6 with value: 61352.62033219648.\n",
      "[I 2025-07-24 16:53:14,028] Trial 7 finished with value: 61734.45626229958 and parameters: {'eta': 0.023226075467124685, 'max_depth': 3, 'subsample': 0.7453667230458756, 'colsample_bytree': 0.8745613300994943, 'lambda': 28.936778836787074, 'alpha': 2.3245501128084576}. Best is trial 6 with value: 61352.62033219648.\n",
      "[I 2025-07-24 16:53:26,685] Trial 8 finished with value: 61669.334665437294 and parameters: {'eta': 0.04341779881949006, 'max_depth': 3, 'subsample': 0.7669273891406628, 'colsample_bytree': 0.6230346252174472, 'lambda': 7.213703773087549, 'alpha': 10.666704736788798}. Best is trial 6 with value: 61352.62033219648.\n",
      "[I 2025-07-24 16:53:52,190] Trial 9 finished with value: 61558.74464180959 and parameters: {'eta': 0.011070195862101563, 'max_depth': 4, 'subsample': 0.6540098385621328, 'colsample_bytree': 0.885316341379256, 'lambda': 7.8796306148636415, 'alpha': 1.791663456772488}. Best is trial 6 with value: 61352.62033219648.\n",
      "[I 2025-07-24 16:54:24,469] Trial 10 finished with value: 61189.61976248996 and parameters: {'eta': 0.010069946843152133, 'max_depth': 6, 'subsample': 0.8867668514230476, 'colsample_bytree': 0.800710184048534, 'lambda': 15.19243676927325, 'alpha': 5.371214258083405}. Best is trial 10 with value: 61189.61976248996.\n",
      "[I 2025-07-24 16:54:52,887] Trial 11 finished with value: 61213.42721686542 and parameters: {'eta': 0.010386225288822151, 'max_depth': 6, 'subsample': 0.8966821891785263, 'colsample_bytree': 0.7932557669136469, 'lambda': 14.087545175519606, 'alpha': 4.910307217108193}. Best is trial 10 with value: 61189.61976248996.\n",
      "[I 2025-07-24 16:55:19,747] Trial 12 finished with value: 61135.760611284095 and parameters: {'eta': 0.014926285934404726, 'max_depth': 6, 'subsample': 0.8735438251780129, 'colsample_bytree': 0.7952584913735182, 'lambda': 13.829109904527948, 'alpha': 4.790408539037167}. Best is trial 12 with value: 61135.760611284095.\n",
      "[I 2025-07-24 16:55:42,199] Trial 13 finished with value: 61196.336301833195 and parameters: {'eta': 0.01597849959918994, 'max_depth': 6, 'subsample': 0.8937151872464603, 'colsample_bytree': 0.8176968058646703, 'lambda': 12.316897504570772, 'alpha': 5.130338086122618}. Best is trial 12 with value: 61135.760611284095.\n",
      "[I 2025-07-24 16:56:08,835] Trial 14 finished with value: 61170.795340014396 and parameters: {'eta': 0.014525440536523247, 'max_depth': 5, 'subsample': 0.8515142042076148, 'colsample_bytree': 0.7218733577829823, 'lambda': 14.212456212442966, 'alpha': 1.0309387912688033}. Best is trial 12 with value: 61135.760611284095.\n",
      "[I 2025-07-24 16:56:32,752] Trial 15 finished with value: 61287.57047227433 and parameters: {'eta': 0.015494001307151862, 'max_depth': 5, 'subsample': 0.841026105811029, 'colsample_bytree': 0.7294796044615128, 'lambda': 3.2953113632028783, 'alpha': 1.138348166704893}. Best is trial 12 with value: 61135.760611284095.\n",
      "[I 2025-07-24 16:56:45,204] Trial 16 finished with value: 61487.05895154577 and parameters: {'eta': 0.01675586923845255, 'max_depth': 5, 'subsample': 0.6014763157471976, 'colsample_bytree': 0.7408080035913793, 'lambda': 19.862818036060084, 'alpha': 1.004969994316719}. Best is trial 12 with value: 61135.760611284095.\n",
      "[I 2025-07-24 16:57:11,583] Trial 17 finished with value: 61271.219602811616 and parameters: {'eta': 0.013190097710156373, 'max_depth': 6, 'subsample': 0.8142664013022693, 'colsample_bytree': 0.7311195025596345, 'lambda': 48.376226336190356, 'alpha': 1.8066874448582828}. Best is trial 12 with value: 61135.760611284095.\n",
      "[I 2025-07-24 16:57:29,677] Trial 18 finished with value: 61283.09367601232 and parameters: {'eta': 0.018479159584700974, 'max_depth': 5, 'subsample': 0.8699265931015918, 'colsample_bytree': 0.7044963785592823, 'lambda': 9.771316808318758, 'alpha': 3.356043216680388}. Best is trial 12 with value: 61135.760611284095.\n",
      "[I 2025-07-24 16:57:41,528] Trial 19 finished with value: 61297.154907610886 and parameters: {'eta': 0.025850862689389912, 'max_depth': 6, 'subsample': 0.801717980221948, 'colsample_bytree': 0.8293228161342858, 'lambda': 5.512354902366961, 'alpha': 9.436599024058745}. Best is trial 12 with value: 61135.760611284095.\n",
      "[I 2025-07-24 16:58:09,150] Trial 20 finished with value: 61217.92269590848 and parameters: {'eta': 0.013062543634803553, 'max_depth': 5, 'subsample': 0.8568561203565177, 'colsample_bytree': 0.7673357814460225, 'lambda': 19.822210968711993, 'alpha': 1.451251318569732}. Best is trial 12 with value: 61135.760611284095.\n",
      "[I 2025-07-24 16:58:37,593] Trial 21 finished with value: 61180.506968415495 and parameters: {'eta': 0.012132727223060708, 'max_depth': 6, 'subsample': 0.8760784946550804, 'colsample_bytree': 0.7935529988477659, 'lambda': 13.348647616946794, 'alpha': 5.275030210654921}. Best is trial 12 with value: 61135.760611284095.\n",
      "[I 2025-07-24 16:58:56,580] Trial 22 finished with value: 61226.1877026312 and parameters: {'eta': 0.012440535251774769, 'max_depth': 6, 'subsample': 0.8660226268340205, 'colsample_bytree': 0.780957495600933, 'lambda': 10.677230974189035, 'alpha': 3.0750188591191985}. Best is trial 12 with value: 61135.760611284095.\n",
      "[I 2025-07-24 16:59:14,468] Trial 23 finished with value: 61359.098716059256 and parameters: {'eta': 0.014938911444719356, 'max_depth': 6, 'subsample': 0.8318436097349425, 'colsample_bytree': 0.836986389538429, 'lambda': 5.119244924767009, 'alpha': 7.05924907118547}. Best is trial 12 with value: 61135.760611284095.\n",
      "[I 2025-07-24 16:59:41,649] Trial 24 finished with value: 61173.77129036329 and parameters: {'eta': 0.018247424820279086, 'max_depth': 5, 'subsample': 0.8616232361795161, 'colsample_bytree': 0.7544120488966514, 'lambda': 18.162717567892532, 'alpha': 14.347111759704125}. Best is trial 12 with value: 61135.760611284095.\n",
      "[I 2025-07-24 17:00:02,999] Trial 25 finished with value: 61303.64271673738 and parameters: {'eta': 0.019100006359187946, 'max_depth': 5, 'subsample': 0.7840675079067425, 'colsample_bytree': 0.7136386320504396, 'lambda': 25.296484635033007, 'alpha': 15.231413875166282}. Best is trial 12 with value: 61135.760611284095.\n",
      "[I 2025-07-24 17:00:25,598] Trial 26 finished with value: 61299.56723125403 and parameters: {'eta': 0.017745265656484155, 'max_depth': 5, 'subsample': 0.708358144776693, 'colsample_bytree': 0.756792608851362, 'lambda': 18.214583704287975, 'alpha': 14.811915552784464}. Best is trial 12 with value: 61135.760611284095.\n",
      "[I 2025-07-24 17:00:40,098] Trial 27 finished with value: 61444.00419355991 and parameters: {'eta': 0.02968031820269611, 'max_depth': 4, 'subsample': 0.841129931450746, 'colsample_bytree': 0.7058913808214067, 'lambda': 10.151445284812407, 'alpha': 27.231108044351807}. Best is trial 12 with value: 61135.760611284095.\n",
      "[I 2025-07-24 17:01:04,713] Trial 28 finished with value: 61280.7328255182 and parameters: {'eta': 0.014126451891167277, 'max_depth': 5, 'subsample': 0.8130550795416946, 'colsample_bytree': 0.6240612520780786, 'lambda': 16.03525912846113, 'alpha': 19.92326850983956}. Best is trial 12 with value: 61135.760611284095.\n",
      "[I 2025-07-24 17:01:26,790] Trial 29 finished with value: 61284.42935515835 and parameters: {'eta': 0.021886508634417608, 'max_depth': 5, 'subsample': 0.8584681798885709, 'colsample_bytree': 0.750476571620699, 'lambda': 8.594035912804184, 'alpha': 7.417188654013607}. Best is trial 12 with value: 61135.760611284095.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Error model tuning complete. Best validation RMSE: $61,135.76\n",
      "\n",
      "--- K-Fold training the final error model... ---\n",
      "  Training XGB error model for fold 1/5...\n",
      "  Training XGB error model for fold 2/5...\n",
      "  Training XGB error model for fold 3/5...\n",
      "  Training XGB error model for fold 4/5...\n",
      "  Training XGB error model for fold 5/5...\n",
      "\n",
      "Final Error Model OOF RMSE: $60,421.11\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# BLOCK 5: TUNE AND TRAIN THE XGBOOST ERROR MODEL\n",
    "# =============================================================================\n",
    "\n",
    "# --- 1. Prepare Features and Target for the Error Model ---\n",
    "print(\"\\n--- Preparing features for the XGBoost error model ---\")\n",
    "# The error target is the absolute difference between the blended mean and the true price\n",
    "error_target = np.abs(y_true - oof_ensemble_mean)\n",
    "\n",
    "# The features are the full feature set PLUS the OOF predictions of the mean models\n",
    "X_for_error = X.copy()\n",
    "X_for_error['oof_xgb'] = oof_xgb_preds\n",
    "X_for_error['oof_cb'] = oof_cb_preds\n",
    "X_for_error['oof_lgbm'] = oof_lgbm_preds\n",
    "X_for_error['oof_nn'] = oof_nn_preds\n",
    "\n",
    "# Do the same for the test set, ensuring column names are consistent\n",
    "X_test_for_error = X_test.copy()\n",
    "X_test_for_error['oof_xgb'] = test_xgb_preds\n",
    "X_test_for_error['oof_cb'] = test_cb_preds\n",
    "X_test_for_error['oof_lgbm'] = test_lgbm_preds\n",
    "X_test_for_error['oof_nn'] = test_nn_preds\n",
    "\n",
    "# --- 2. Tune the Error Model with Optuna ---\n",
    "print(\"\\n--- Tuning the XGBoost Error Model with Optuna... ---\")\n",
    "X_train_err_opt, X_val_err_opt, y_train_err_opt, y_val_err_opt = train_test_split(\n",
    "    X_for_error, error_target, test_size=0.2, random_state=RANDOM_STATE\n",
    ")\n",
    "dtrain_err_opt = xgb.DMatrix(X_train_err_opt, label=y_train_err_opt)\n",
    "dval_err_opt = xgb.DMatrix(X_val_err_opt, label=y_val_err_opt)\n",
    "\n",
    "def objective_error_model(trial):\n",
    "    params = {\n",
    "        'objective': 'reg:squarederror', 'eval_metric': 'rmse', 'tree_method': 'hist',\n",
    "        'n_jobs': -1, 'seed': RANDOM_STATE,\n",
    "        'eta': trial.suggest_float('eta', 0.01, 0.05, log=True),\n",
    "        'max_depth': trial.suggest_int('max_depth', 3, 6),\n",
    "        'subsample': trial.suggest_float('subsample', 0.6, 0.9),\n",
    "        'colsample_bytree': trial.suggest_float('colsample_bytree', 0.6, 0.9),\n",
    "        'lambda': trial.suggest_float('lambda', 1.0, 50.0, log=True),\n",
    "        'alpha': trial.suggest_float('alpha', 1.0, 50.0, log=True)\n",
    "    }\n",
    "    bst = xgb.train(params, dtrain_err_opt, num_boost_round=2000, \n",
    "                    evals=[(dval_err_opt, 'val')], early_stopping_rounds=75, verbose_eval=False)\n",
    "    preds = bst.predict(dval_err_opt, iteration_range=(0, bst.best_iteration))\n",
    "    return np.sqrt(mean_squared_error(y_val_err_opt, preds))\n",
    "\n",
    "study_error = optuna.create_study(direction='minimize')\n",
    "study_error.optimize(objective_error_model, n_trials=30)\n",
    "best_params_error = study_error.best_params\n",
    "best_params_error['n_estimators'] = study_error.best_trial.user_attrs.get('best_iteration', 2000)\n",
    "print(f\"\\nError model tuning complete. Best validation RMSE: ${study_error.best_value:,.2f}\")\n",
    "\n",
    "# --- 3. Final K-Fold Training of the Error Model ---\n",
    "print(\"\\n--- K-Fold training the final error model... ---\")\n",
    "oof_error_preds = np.zeros(len(X))\n",
    "test_error_preds = np.zeros(len(X_test))\n",
    "skf = StratifiedKFold(n_splits=N_SPLITS, shuffle=True, random_state=RANDOM_STATE)\n",
    "\n",
    "for fold, (train_idx, val_idx) in enumerate(skf.split(X_for_error, grade_for_stratify)):\n",
    "    print(f\"  Training XGB error model for fold {fold+1}/{N_SPLITS}...\")\n",
    "    X_train_err, X_val_err = X_for_error.iloc[train_idx], X_for_error.iloc[val_idx]\n",
    "    y_train_err, y_val_err = error_target.iloc[train_idx], error_target.iloc[val_idx]\n",
    "\n",
    "    model_err = xgb.XGBRegressor(**best_params_error)\n",
    "    model_err.fit(X_train_err, y_train_err)\n",
    "    \n",
    "    oof_error_preds[val_idx] = np.clip(model_err.predict(X_val_err), 0, None)\n",
    "    test_error_preds += np.clip(model_err.predict(X_test_for_error), 0, None) / N_SPLITS\n",
    "\n",
    "print(f\"\\nFinal Error Model OOF RMSE: ${np.sqrt(mean_squared_error(error_target, oof_error_preds)):,.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "01f24825-8632-4af8-a9ea-1f9121437e7b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Calibrating the final intervals with an Optimizer ---\n",
      "\n",
      "============================================================\n",
      "FINAL RESULTS\n",
      "============================================================\n",
      "Final OOF Winkler Score (4 Mean + 1 XGB Error): $293,029.10\n",
      "Optimal Multipliers: a=1.9829, b=2.1768\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# BLOCK 6: FINAL CALIBRATION AND SUBMISSION\n",
    "# =============================================================================\n",
    "\n",
    "print(\"\\n--- Calibrating the final intervals with an Optimizer ---\")\n",
    "oof_error_final = np.clip(oof_error_preds, 0, None)\n",
    "\n",
    "def get_winkler_from_multipliers(multipliers):\n",
    "    a, b = multipliers[0], multipliers[1]\n",
    "    low = oof_ensemble_mean - oof_error_final * a\n",
    "    high = oof_ensemble_mean + oof_error_final * b\n",
    "    return winkler_score(y_true, low, high)\n",
    "\n",
    "initial_guess = [1.5, 1.5]\n",
    "bounds = [(0.5, 4.0), (0.5, 4.0)]\n",
    "result_calib = minimize(get_winkler_from_multipliers, initial_guess, method='L-BFGS-B', bounds=bounds)\n",
    "best_a, best_b = result_calib.x\n",
    "best_score = result_calib.fun\n",
    "\n",
    "print(\"\\n\" + \"=\"*60); print(\"FINAL RESULTS\"); print(\"=\"*60)\n",
    "print(f\"Final OOF Winkler Score (4 Mean + 1 XGB Error): ${best_score:,.2f}\")\n",
    "print(f\"Optimal Multipliers: a={best_a:.4f}, b={best_b:.4f}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31e53007-0d58-48b3-9634-0e4b4419ff25",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Create Submission File ---\n",
    "print(\"\\n--- Creating final submission file... ---\")\n",
    "final_lower = test_ensemble_mean - np.clip(test_error_preds, 0, None) * best_a\n",
    "final_upper = test_ensemble_mean + np.clip(test_error_preds, 0, None) * best_b\n",
    "final_upper = np.maximum(final_lower + 1, final_upper) # Safety check\n",
    "\n",
    "submission_df = pd.DataFrame({'id': pd.read_csv(os.path.join(DATA_PATH, 'test.csv'))['id'], \n",
    "                              'pi_lower': final_lower, \n",
    "                              'pi_upper': final_upper})\n",
    "submission_filename = f'submission_3M1E_XGB_{int(best_score)}.csv'\n",
    "submission_df.to_csv(submission_filename, index=False)\n",
    "print(f\"\\n'{submission_filename}' created successfully!\")\n",
    "display(submission_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e3a28a8f-b2d6-474f-98bd-e450b6159e9b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-07-24 17:04:57,904] A new study created in memory with name: no-name-52821495-c0ae-482a-9356-2c9c315f8dc3\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Preparing features for the CatBoost error model ---\n",
      "\n",
      "--- Tuning the CatBoost Error Model with Optuna... ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-07-24 17:05:00,390] Trial 0 finished with value: 62652.15216929171 and parameters: {'iterations': 2195, 'learning_rate': 0.04118414164847515, 'depth': 4, 'l2_leaf_reg': 13.840045600193827, 'subsample': 0.6884053677728654, 'random_strength': 8.977447039906842}. Best is trial 0 with value: 62652.15216929171.\n",
      "[I 2025-07-24 17:05:15,516] Trial 1 finished with value: 61491.36935724671 and parameters: {'iterations': 2116, 'learning_rate': 0.044481495599913025, 'depth': 6, 'l2_leaf_reg': 36.90413239331086, 'subsample': 0.6408066371531128, 'random_strength': 1.0561516095645194}. Best is trial 1 with value: 61491.36935724671.\n",
      "[I 2025-07-24 17:05:20,584] Trial 2 finished with value: 61650.34658498198 and parameters: {'iterations': 1963, 'learning_rate': 0.046783934813471456, 'depth': 6, 'l2_leaf_reg': 2.982916907541343, 'subsample': 0.7480469164745622, 'random_strength': 4.264299637234698}. Best is trial 1 with value: 61491.36935724671.\n",
      "[I 2025-07-24 17:05:25,585] Trial 3 finished with value: 62592.573618848895 and parameters: {'iterations': 1037, 'learning_rate': 0.014755289608312372, 'depth': 3, 'l2_leaf_reg': 4.4670976444425685, 'subsample': 0.7167685667501111, 'random_strength': 3.5896168031753573}. Best is trial 1 with value: 61491.36935724671.\n",
      "[I 2025-07-24 17:05:36,422] Trial 4 finished with value: 61527.08717929881 and parameters: {'iterations': 2579, 'learning_rate': 0.04038631010305496, 'depth': 5, 'l2_leaf_reg': 10.462615963274594, 'subsample': 0.8613799910098513, 'random_strength': 5.727979346214108}. Best is trial 1 with value: 61491.36935724671.\n",
      "[I 2025-07-24 17:05:42,827] Trial 5 finished with value: 62806.88093625254 and parameters: {'iterations': 2397, 'learning_rate': 0.011742108020669443, 'depth': 3, 'l2_leaf_reg': 18.87316384821732, 'subsample': 0.8489322225581076, 'random_strength': 2.5777149908854}. Best is trial 1 with value: 61491.36935724671.\n",
      "[I 2025-07-24 17:06:00,927] Trial 6 finished with value: 61406.66781307514 and parameters: {'iterations': 2774, 'learning_rate': 0.024378567052635164, 'depth': 6, 'l2_leaf_reg': 7.281366667983205, 'subsample': 0.7428493008303473, 'random_strength': 2.0767351841406114}. Best is trial 6 with value: 61406.66781307514.\n",
      "[I 2025-07-24 17:06:13,962] Trial 7 finished with value: 61735.814564400476 and parameters: {'iterations': 2018, 'learning_rate': 0.014930153672609593, 'depth': 5, 'l2_leaf_reg': 2.7851717790863733, 'subsample': 0.7820036486686708, 'random_strength': 1.391802893904097}. Best is trial 6 with value: 61406.66781307514.\n",
      "[I 2025-07-24 17:06:21,875] Trial 8 finished with value: 61977.82651759558 and parameters: {'iterations': 1117, 'learning_rate': 0.011213455536066945, 'depth': 6, 'l2_leaf_reg': 3.8655092026909577, 'subsample': 0.6065219851066392, 'random_strength': 3.654211152342617}. Best is trial 6 with value: 61406.66781307514.\n",
      "[I 2025-07-24 17:06:32,775] Trial 9 finished with value: 61648.15506324089 and parameters: {'iterations': 2291, 'learning_rate': 0.03406711932302045, 'depth': 5, 'l2_leaf_reg': 2.8038278012339144, 'subsample': 0.8784386277378757, 'random_strength': 1.223188427700896}. Best is trial 6 with value: 61406.66781307514.\n",
      "[I 2025-07-24 17:06:43,829] Trial 10 finished with value: 61745.83482108931 and parameters: {'iterations': 2986, 'learning_rate': 0.02489643824117728, 'depth': 4, 'l2_leaf_reg': 1.028966268671823, 'subsample': 0.798234278218428, 'random_strength': 2.0182132980832295}. Best is trial 6 with value: 61406.66781307514.\n",
      "[I 2025-07-24 17:06:53,659] Trial 11 finished with value: 61846.2830448629 and parameters: {'iterations': 1521, 'learning_rate': 0.026175209553931048, 'depth': 6, 'l2_leaf_reg': 39.18769419652026, 'subsample': 0.6287006814954674, 'random_strength': 1.042748088921466}. Best is trial 6 with value: 61406.66781307514.\n",
      "[I 2025-07-24 17:07:11,446] Trial 12 finished with value: 61744.521705523846 and parameters: {'iterations': 2973, 'learning_rate': 0.018128737906295923, 'depth': 6, 'l2_leaf_reg': 42.68312558022112, 'subsample': 0.6607428097030951, 'random_strength': 1.7875563272076656}. Best is trial 6 with value: 61406.66781307514.\n",
      "[I 2025-07-24 17:07:27,863] Trial 13 finished with value: 61455.70040894031 and parameters: {'iterations': 2641, 'learning_rate': 0.03058713814449274, 'depth': 6, 'l2_leaf_reg': 20.518494060957195, 'subsample': 0.6721713308963864, 'random_strength': 1.8750212524960654}. Best is trial 6 with value: 61406.66781307514.\n",
      "[I 2025-07-24 17:07:39,223] Trial 14 finished with value: 61561.46028735635 and parameters: {'iterations': 2742, 'learning_rate': 0.02849584283782949, 'depth': 5, 'l2_leaf_reg': 7.1871543288427855, 'subsample': 0.7114442698060731, 'random_strength': 2.2849285664052976}. Best is trial 6 with value: 61406.66781307514.\n",
      "[I 2025-07-24 17:07:58,239] Trial 15 finished with value: 61615.79398486081 and parameters: {'iterations': 2652, 'learning_rate': 0.020569805668325798, 'depth': 6, 'l2_leaf_reg': 21.413151571644487, 'subsample': 0.8099256011256557, 'random_strength': 1.7210454674940945}. Best is trial 6 with value: 61406.66781307514.\n",
      "[I 2025-07-24 17:08:06,540] Trial 16 finished with value: 61799.13498752124 and parameters: {'iterations': 1701, 'learning_rate': 0.03129161956080554, 'depth': 4, 'l2_leaf_reg': 7.702943024366946, 'subsample': 0.7588341831508604, 'random_strength': 1.5342160094650426}. Best is trial 6 with value: 61406.66781307514.\n",
      "[I 2025-07-24 17:08:19,427] Trial 17 finished with value: 61769.22797142154 and parameters: {'iterations': 2495, 'learning_rate': 0.021508912530298394, 'depth': 5, 'l2_leaf_reg': 23.247568624788133, 'subsample': 0.6766278305631066, 'random_strength': 2.7154070307733593}. Best is trial 6 with value: 61406.66781307514.\n",
      "[I 2025-07-24 17:08:29,657] Trial 18 finished with value: 61445.9858777286 and parameters: {'iterations': 2749, 'learning_rate': 0.03512325159005981, 'depth': 6, 'l2_leaf_reg': 1.224508333608985, 'subsample': 0.724085336337098, 'random_strength': 4.85845622283608}. Best is trial 6 with value: 61406.66781307514.\n",
      "[I 2025-07-24 17:08:44,274] Trial 19 finished with value: 61644.94859898023 and parameters: {'iterations': 2834, 'learning_rate': 0.01681454858952043, 'depth': 5, 'l2_leaf_reg': 1.2033606843762177, 'subsample': 0.7346660293936114, 'random_strength': 5.210174439562049}. Best is trial 6 with value: 61406.66781307514.\n",
      "[I 2025-07-24 17:08:53,101] Trial 20 finished with value: 61539.421886815246 and parameters: {'iterations': 1720, 'learning_rate': 0.036253950697526335, 'depth': 6, 'l2_leaf_reg': 1.84258698561309, 'subsample': 0.8297757814260616, 'random_strength': 7.703280827584042}. Best is trial 6 with value: 61406.66781307514.\n",
      "[I 2025-07-24 17:09:06,287] Trial 21 finished with value: 61520.30184839431 and parameters: {'iterations': 2786, 'learning_rate': 0.03000281391866767, 'depth': 6, 'l2_leaf_reg': 11.053335315251383, 'subsample': 0.7024830043400135, 'random_strength': 3.0731304579899987}. Best is trial 6 with value: 61406.66781307514.\n",
      "[I 2025-07-24 17:09:20,745] Trial 22 finished with value: 61465.96177250993 and parameters: {'iterations': 2445, 'learning_rate': 0.023726414378110145, 'depth': 6, 'l2_leaf_reg': 5.231192697600119, 'subsample': 0.7588665496269291, 'random_strength': 6.001681180562539}. Best is trial 6 with value: 61406.66781307514.\n",
      "[I 2025-07-24 17:09:28,471] Trial 23 finished with value: 61530.22399726034 and parameters: {'iterations': 2620, 'learning_rate': 0.035898556413996405, 'depth': 6, 'l2_leaf_reg': 1.589725354442593, 'subsample': 0.6637926788422399, 'random_strength': 2.1404899020033428}. Best is trial 6 with value: 61406.66781307514.\n",
      "[I 2025-07-24 17:09:33,112] Trial 24 finished with value: 62411.17603322978 and parameters: {'iterations': 2881, 'learning_rate': 0.026915801856900276, 'depth': 5, 'l2_leaf_reg': 28.875423671040448, 'subsample': 0.7234495065042045, 'random_strength': 4.387280458501618}. Best is trial 6 with value: 61406.66781307514.\n",
      "[I 2025-07-24 17:09:46,597] Trial 25 finished with value: 61453.783954917635 and parameters: {'iterations': 2719, 'learning_rate': 0.03382950234393751, 'depth': 6, 'l2_leaf_reg': 14.478886344998115, 'subsample': 0.6945771003493946, 'random_strength': 2.8450281711984844}. Best is trial 6 with value: 61406.66781307514.\n",
      "[I 2025-07-24 17:09:57,299] Trial 26 finished with value: 61512.56241913922 and parameters: {'iterations': 2271, 'learning_rate': 0.04968000321630043, 'depth': 5, 'l2_leaf_reg': 13.705813502851429, 'subsample': 0.6967114283086896, 'random_strength': 3.0025275533488918}. Best is trial 6 with value: 61406.66781307514.\n",
      "[I 2025-07-24 17:10:07,506] Trial 27 finished with value: 61494.63355479057 and parameters: {'iterations': 2406, 'learning_rate': 0.038103421468349066, 'depth': 6, 'l2_leaf_reg': 5.919685892513625, 'subsample': 0.7667490189961319, 'random_strength': 6.931877215377522}. Best is trial 6 with value: 61406.66781307514.\n",
      "[I 2025-07-24 17:10:20,577] Trial 28 finished with value: 61781.225744589574 and parameters: {'iterations': 2767, 'learning_rate': 0.01972032268785886, 'depth': 4, 'l2_leaf_reg': 9.73491249577669, 'subsample': 0.7316614454536555, 'random_strength': 4.5356490535306255}. Best is trial 6 with value: 61406.66781307514.\n",
      "[I 2025-07-24 17:10:24,637] Trial 29 finished with value: 62318.95689225365 and parameters: {'iterations': 2169, 'learning_rate': 0.0326827338935857, 'depth': 6, 'l2_leaf_reg': 15.199269056760674, 'subsample': 0.6885465640208749, 'random_strength': 9.849971859651287}. Best is trial 6 with value: 61406.66781307514.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "CatBoost Error model tuning complete. Best validation RMSE: $61,406.67\n",
      "\n",
      "--- K-Fold training the final CatBoost error model... ---\n",
      "  Training CatBoost error model for fold 1/5...\n",
      "  Training CatBoost error model for fold 2/5...\n",
      "  Training CatBoost error model for fold 3/5...\n",
      "  Training CatBoost error model for fold 4/5...\n",
      "  Training CatBoost error model for fold 5/5...\n",
      "\n",
      "Final CatBoost Error Model OOF RMSE: $60,567.25\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# BLOCK 5 (CATBOOST): TUNE AND TRAIN THE CATBOOST ERROR MODEL\n",
    "# =============================================================================\n",
    "\n",
    "# --- 1. Prepare Features and Target for the Error Model ---\n",
    "print(\"\\n--- Preparing features for the CatBoost error model ---\")\n",
    "# The error target is the same for all error models\n",
    "# The features are also the same (full feature set + OOF mean preds)\n",
    "\n",
    "# --- 2. Tune the Error Model with Optuna ---\n",
    "print(\"\\n--- Tuning the CatBoost Error Model with Optuna... ---\")\n",
    "# We can use the same train/validation split for tuning efficiency\n",
    "# X_train_err_opt, X_val_err_opt, y_train_err_opt, y_val_err_opt are already defined from the XGBoost block\n",
    "\n",
    "def objective_error_model_cb(trial):\n",
    "    params = {\n",
    "        'loss_function': 'RMSE',\n",
    "        'eval_metric': 'RMSE',\n",
    "        'random_seed': RANDOM_STATE,\n",
    "        'verbose': 0,\n",
    "        'iterations': trial.suggest_int('iterations', 1000, 3000),\n",
    "        'learning_rate': trial.suggest_float('learning_rate', 0.01, 0.05, log=True),\n",
    "        'depth': trial.suggest_int('depth', 3, 6),\n",
    "        'l2_leaf_reg': trial.suggest_float('l2_leaf_reg', 1.0, 50.0, log=True),\n",
    "        'subsample': trial.suggest_float('subsample', 0.6, 0.9),\n",
    "        'random_strength': trial.suggest_float('random_strength', 1.0, 10.0, log=True)\n",
    "    }\n",
    "    \n",
    "    model = cb.CatBoostRegressor(**params)\n",
    "    model.fit(\n",
    "        X_train_err_opt, y_train_err_opt,\n",
    "        eval_set=[(X_val_err_opt, y_val_err_opt)],\n",
    "        early_stopping_rounds=75,\n",
    "        use_best_model=True\n",
    "    )\n",
    "    \n",
    "    preds = model.predict(X_val_err_opt)\n",
    "    rmse = np.sqrt(mean_squared_error(y_val_err_opt, preds))\n",
    "    return rmse\n",
    "\n",
    "study_error_cb = optuna.create_study(direction='minimize')\n",
    "study_error_cb.optimize(objective_error_model_cb, n_trials=30)\n",
    "best_params_error_cb = study_error_cb.best_params\n",
    "print(f\"\\nCatBoost Error model tuning complete. Best validation RMSE: ${study_error_cb.best_value:,.2f}\")\n",
    "\n",
    "# --- 3. Final K-Fold Training of the Error Model ---\n",
    "print(\"\\n--- K-Fold training the final CatBoost error model... ---\")\n",
    "oof_error_preds_cb = np.zeros(len(X))\n",
    "test_error_preds_cb = np.zeros(len(X_test))\n",
    "skf_cb = StratifiedKFold(n_splits=N_SPLITS, shuffle=True, random_state=RANDOM_STATE)\n",
    "\n",
    "for fold, (train_idx, val_idx) in enumerate(skf_cb.split(X_for_error, grade_for_stratify)):\n",
    "    print(f\"  Training CatBoost error model for fold {fold+1}/{N_SPLITS}...\")\n",
    "    X_train_err, X_val_err = X_for_error.iloc[train_idx], X_for_error.iloc[val_idx]\n",
    "    y_train_err, y_val_err = error_target.iloc[train_idx], error_target.iloc[val_idx]\n",
    "\n",
    "    model_err_cb = cb.CatBoostRegressor(**best_params_error_cb, early_stopping_rounds=75, verbose=0)\n",
    "    model_err_cb.fit(X_train_err, y_train_err, eval_set=[(X_val_err, y_val_err)])\n",
    "    \n",
    "    oof_error_preds_cb[val_idx] = np.clip(model_err_cb.predict(X_val_err), 0, None)\n",
    "    test_error_preds_cb += np.clip(model_err_cb.predict(X_test_for_error), 0, None) / N_SPLITS\n",
    "\n",
    "print(f\"\\nFinal CatBoost Error Model OOF RMSE: ${np.sqrt(mean_squared_error(error_target, oof_error_preds_cb)):,.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8c945b98-9ec2-4066-afb2-2eee49625898",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-07-24 17:14:55,327] A new study created in memory with name: no-name-ba92639c-e5cd-4056-9d88-270b2c0882ee\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Preparing features for the LightGBM error model ---\n",
      "\n",
      "--- Tuning the LightGBM Error Model with Optuna... ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-07-24 17:15:12,321] Trial 0 finished with value: 63094.69733961664 and parameters: {'learning_rate': 0.034793142258152235, 'n_estimators': 2339, 'num_leaves': 37, 'max_depth': 5, 'lambda_l1': 2.4131530218075854, 'lambda_l2': 7.969999565684957, 'feature_fraction': 0.7255296573441212, 'bagging_fraction': 0.8051794666918468, 'bagging_freq': 2}. Best is trial 0 with value: 63094.69733961664.\n",
      "[I 2025-07-24 17:15:32,145] Trial 1 finished with value: 63243.20911394577 and parameters: {'learning_rate': 0.030072630780943625, 'n_estimators': 2389, 'num_leaves': 31, 'max_depth': 6, 'lambda_l1': 14.575271598553988, 'lambda_l2': 8.70101780320695, 'feature_fraction': 0.7608235405531482, 'bagging_fraction': 0.7987340416954959, 'bagging_freq': 4}. Best is trial 0 with value: 63094.69733961664.\n",
      "[I 2025-07-24 17:15:52,071] Trial 2 finished with value: 63157.0332211606 and parameters: {'learning_rate': 0.025100431663170962, 'n_estimators': 2711, 'num_leaves': 35, 'max_depth': 5, 'lambda_l1': 5.1602327333263505, 'lambda_l2': 4.4744240200803285, 'feature_fraction': 0.6355570100055112, 'bagging_fraction': 0.7143138442682604, 'bagging_freq': 3}. Best is trial 0 with value: 63094.69733961664.\n",
      "[I 2025-07-24 17:15:58,535] Trial 3 finished with value: 63468.36951338092 and parameters: {'learning_rate': 0.04008250502533983, 'n_estimators': 1908, 'num_leaves': 19, 'max_depth': 5, 'lambda_l1': 1.641247090307387, 'lambda_l2': 9.082389709179097, 'feature_fraction': 0.6206160371026568, 'bagging_fraction': 0.6197383123495145, 'bagging_freq': 5}. Best is trial 0 with value: 63094.69733961664.\n",
      "[I 2025-07-24 17:16:11,269] Trial 4 finished with value: 63438.3800311932 and parameters: {'learning_rate': 0.029518737064993687, 'n_estimators': 1574, 'num_leaves': 36, 'max_depth': 6, 'lambda_l1': 14.274038697502057, 'lambda_l2': 2.0570347177507795, 'feature_fraction': 0.6322642074771909, 'bagging_fraction': 0.6430004070059273, 'bagging_freq': 5}. Best is trial 0 with value: 63094.69733961664.\n",
      "[I 2025-07-24 17:16:20,655] Trial 5 finished with value: 63902.75545295489 and parameters: {'learning_rate': 0.01844772259731284, 'n_estimators': 1292, 'num_leaves': 16, 'max_depth': 5, 'lambda_l1': 1.5237485901696142, 'lambda_l2': 3.4557211875803238, 'feature_fraction': 0.7701281878359707, 'bagging_fraction': 0.8817627039461247, 'bagging_freq': 3}. Best is trial 0 with value: 63094.69733961664.\n",
      "[I 2025-07-24 17:16:34,649] Trial 6 finished with value: 63282.85240242327 and parameters: {'learning_rate': 0.0366056530883761, 'n_estimators': 2397, 'num_leaves': 22, 'max_depth': 4, 'lambda_l1': 4.613853180455433, 'lambda_l2': 3.3598850120026422, 'feature_fraction': 0.7845688086177285, 'bagging_fraction': 0.8694634114863122, 'bagging_freq': 7}. Best is trial 0 with value: 63094.69733961664.\n",
      "[I 2025-07-24 17:16:49,891] Trial 7 finished with value: 63818.51257542186 and parameters: {'learning_rate': 0.010264288743509879, 'n_estimators': 1883, 'num_leaves': 34, 'max_depth': 6, 'lambda_l1': 18.524551066533952, 'lambda_l2': 23.1843167260933, 'feature_fraction': 0.6919953560296707, 'bagging_fraction': 0.804999739140085, 'bagging_freq': 1}. Best is trial 0 with value: 63094.69733961664.\n",
      "[I 2025-07-24 17:17:02,275] Trial 8 finished with value: 63673.44480467395 and parameters: {'learning_rate': 0.03912649263353702, 'n_estimators': 1874, 'num_leaves': 14, 'max_depth': 5, 'lambda_l1': 35.05207653892775, 'lambda_l2': 1.542828633828302, 'feature_fraction': 0.8895985083585273, 'bagging_fraction': 0.8298792356144324, 'bagging_freq': 4}. Best is trial 0 with value: 63094.69733961664.\n",
      "[I 2025-07-24 17:17:10,571] Trial 9 finished with value: 63572.81338688303 and parameters: {'learning_rate': 0.021218641637461615, 'n_estimators': 1462, 'num_leaves': 25, 'max_depth': 5, 'lambda_l1': 4.671064702597409, 'lambda_l2': 2.1628211039091947, 'feature_fraction': 0.6078893622921143, 'bagging_fraction': 0.6383266202301608, 'bagging_freq': 4}. Best is trial 0 with value: 63094.69733961664.\n",
      "[I 2025-07-24 17:17:26,782] Trial 10 finished with value: 63920.38301662622 and parameters: {'learning_rate': 0.016771776906458247, 'n_estimators': 2911, 'num_leaves': 40, 'max_depth': 3, 'lambda_l1': 1.0720795024682261, 'lambda_l2': 44.75844750232675, 'feature_fraction': 0.8485964597760469, 'bagging_fraction': 0.7254357486424421, 'bagging_freq': 1}. Best is trial 0 with value: 63094.69733961664.\n",
      "[I 2025-07-24 17:17:34,554] Trial 11 finished with value: 63421.3892335355 and parameters: {'learning_rate': 0.0491471830465828, 'n_estimators': 2825, 'num_leaves': 30, 'max_depth': 4, 'lambda_l1': 3.5544781778266787, 'lambda_l2': 5.7731953804450304, 'feature_fraction': 0.6956218356686343, 'bagging_fraction': 0.7141487888234115, 'bagging_freq': 2}. Best is trial 0 with value: 63094.69733961664.\n",
      "[I 2025-07-24 17:17:49,357] Trial 12 finished with value: 63392.21875043384 and parameters: {'learning_rate': 0.026566001910164082, 'n_estimators': 2488, 'num_leaves': 40, 'max_depth': 4, 'lambda_l1': 2.5436693365116962, 'lambda_l2': 14.877890774639255, 'feature_fraction': 0.693058449561369, 'bagging_fraction': 0.7518211508500713, 'bagging_freq': 2}. Best is trial 0 with value: 63094.69733961664.\n",
      "[I 2025-07-24 17:18:04,602] Trial 13 finished with value: 64117.333612491275 and parameters: {'learning_rate': 0.012548213094047564, 'n_estimators': 2645, 'num_leaves': 29, 'max_depth': 3, 'lambda_l1': 6.578209206708469, 'lambda_l2': 4.7134510992118805, 'feature_fraction': 0.6618528884597717, 'bagging_fraction': 0.6950962298155957, 'bagging_freq': 2}. Best is trial 0 with value: 63094.69733961664.\n",
      "[I 2025-07-24 17:18:19,518] Trial 14 finished with value: 63309.56232048446 and parameters: {'learning_rate': 0.025078090691969753, 'n_estimators': 2152, 'num_leaves': 35, 'max_depth': 5, 'lambda_l1': 9.325931612917211, 'lambda_l2': 1.0807714856861264, 'feature_fraction': 0.7347255258447033, 'bagging_fraction': 0.773424922064964, 'bagging_freq': 3}. Best is trial 0 with value: 63094.69733961664.\n",
      "[I 2025-07-24 17:18:32,929] Trial 15 finished with value: 63279.445158517956 and parameters: {'learning_rate': 0.033917491032570114, 'n_estimators': 2175, 'num_leaves': 26, 'max_depth': 4, 'lambda_l1': 2.3964486640357685, 'lambda_l2': 14.403539399984645, 'feature_fraction': 0.819887316750445, 'bagging_fraction': 0.6774987632247064, 'bagging_freq': 3}. Best is trial 0 with value: 63094.69733961664.\n",
      "[I 2025-07-24 17:18:46,995] Trial 16 finished with value: 63008.046216028415 and parameters: {'learning_rate': 0.04967623217499028, 'n_estimators': 2653, 'num_leaves': 36, 'max_depth': 6, 'lambda_l1': 7.972816399005995, 'lambda_l2': 12.933271498661698, 'feature_fraction': 0.7214563879929922, 'bagging_fraction': 0.8441199586595552, 'bagging_freq': 7}. Best is trial 16 with value: 63008.046216028415.\n",
      "[I 2025-07-24 17:19:02,413] Trial 17 finished with value: 63053.40026415354 and parameters: {'learning_rate': 0.04982670203493395, 'n_estimators': 2982, 'num_leaves': 39, 'max_depth': 6, 'lambda_l1': 8.223939654058768, 'lambda_l2': 28.142058251834772, 'feature_fraction': 0.7297456778269419, 'bagging_fraction': 0.8448014014734647, 'bagging_freq': 7}. Best is trial 16 with value: 63008.046216028415.\n",
      "[I 2025-07-24 17:19:17,260] Trial 18 finished with value: 63392.98374635538 and parameters: {'learning_rate': 0.049446865759079986, 'n_estimators': 2963, 'num_leaves': 10, 'max_depth': 6, 'lambda_l1': 8.58905291275253, 'lambda_l2': 47.06782639868541, 'feature_fraction': 0.7226715283483399, 'bagging_fraction': 0.8523950385975548, 'bagging_freq': 7}. Best is trial 16 with value: 63008.046216028415.\n",
      "[I 2025-07-24 17:19:33,952] Trial 19 finished with value: 63485.35944484732 and parameters: {'learning_rate': 0.04414641200971131, 'n_estimators': 2635, 'num_leaves': 32, 'max_depth': 6, 'lambda_l1': 34.44228031938613, 'lambda_l2': 27.83509265561365, 'feature_fraction': 0.8024887521855901, 'bagging_fraction': 0.8893989657636784, 'bagging_freq': 6}. Best is trial 16 with value: 63008.046216028415.\n",
      "[I 2025-07-24 17:20:25,960] Trial 20 finished with value: 63152.612215969544 and parameters: {'learning_rate': 0.04204041582856716, 'n_estimators': 2997, 'num_leaves': 38, 'max_depth': 6, 'lambda_l1': 20.750862272820676, 'lambda_l2': 15.304791619072496, 'feature_fraction': 0.6634779373173963, 'bagging_fraction': 0.8421465078938832, 'bagging_freq': 6}. Best is trial 16 with value: 63008.046216028415.\n",
      "[I 2025-07-24 17:20:44,565] Trial 21 finished with value: 62960.3741566168 and parameters: {'learning_rate': 0.04688545056677072, 'n_estimators': 2213, 'num_leaves': 38, 'max_depth': 6, 'lambda_l1': 7.706078894719996, 'lambda_l2': 24.023153706694167, 'feature_fraction': 0.7202704523903461, 'bagging_fraction': 0.813016732503191, 'bagging_freq': 6}. Best is trial 21 with value: 62960.3741566168.\n",
      "[I 2025-07-24 17:21:02,633] Trial 22 finished with value: 63044.36158052028 and parameters: {'learning_rate': 0.04946280758701938, 'n_estimators': 2744, 'num_leaves': 40, 'max_depth': 6, 'lambda_l1': 11.233945280445615, 'lambda_l2': 25.785302758337224, 'feature_fraction': 0.7508434860850935, 'bagging_fraction': 0.8210559776523055, 'bagging_freq': 6}. Best is trial 21 with value: 62960.3741566168.\n",
      "[I 2025-07-24 17:21:19,672] Trial 23 finished with value: 63204.15444371771 and parameters: {'learning_rate': 0.04395749324562813, 'n_estimators': 2139, 'num_leaves': 33, 'max_depth': 6, 'lambda_l1': 12.891144234671044, 'lambda_l2': 20.53046495902671, 'feature_fraction': 0.7506112144514282, 'bagging_fraction': 0.7741647429798132, 'bagging_freq': 6}. Best is trial 21 with value: 62960.3741566168.\n",
      "[I 2025-07-24 17:21:42,089] Trial 24 finished with value: 63329.65778091321 and parameters: {'learning_rate': 0.032651822230178346, 'n_estimators': 2547, 'num_leaves': 37, 'max_depth': 6, 'lambda_l1': 25.453698508828214, 'lambda_l2': 11.50042943782441, 'feature_fraction': 0.7050232017091298, 'bagging_fraction': 0.824250577816632, 'bagging_freq': 5}. Best is trial 21 with value: 62960.3741566168.\n",
      "[I 2025-07-24 17:21:53,647] Trial 25 finished with value: 63357.47812398482 and parameters: {'learning_rate': 0.04432728205181521, 'n_estimators': 2763, 'num_leaves': 28, 'max_depth': 6, 'lambda_l1': 11.86889531765298, 'lambda_l2': 34.186436480802165, 'feature_fraction': 0.6681262601325012, 'bagging_fraction': 0.7789697590920241, 'bagging_freq': 6}. Best is trial 21 with value: 62960.3741566168.\n",
      "[I 2025-07-24 17:22:12,675] Trial 26 finished with value: 63055.4194881442 and parameters: {'learning_rate': 0.03814331007330846, 'n_estimators': 2279, 'num_leaves': 40, 'max_depth': 6, 'lambda_l1': 6.25034235314152, 'lambda_l2': 19.89809375576607, 'feature_fraction': 0.7905662559154606, 'bagging_fraction': 0.8613789464710984, 'bagging_freq': 7}. Best is trial 21 with value: 62960.3741566168.\n",
      "[I 2025-07-24 17:22:28,954] Trial 27 finished with value: 63223.484558098 and parameters: {'learning_rate': 0.04964589306190387, 'n_estimators': 2548, 'num_leaves': 33, 'max_depth': 5, 'lambda_l1': 10.896577647766732, 'lambda_l2': 36.51063628218933, 'feature_fraction': 0.8315090563654779, 'bagging_fraction': 0.8237053572619696, 'bagging_freq': 6}. Best is trial 21 with value: 62960.3741566168.\n",
      "[I 2025-07-24 17:22:45,402] Trial 28 finished with value: 63865.17486135632 and parameters: {'learning_rate': 0.03032447038256551, 'n_estimators': 1745, 'num_leaves': 37, 'max_depth': 6, 'lambda_l1': 47.82064162567042, 'lambda_l2': 11.166334292874962, 'feature_fraction': 0.7495132886190248, 'bagging_fraction': 0.7929116089826378, 'bagging_freq': 5}. Best is trial 21 with value: 62960.3741566168.\n",
      "[I 2025-07-24 17:23:30,398] Trial 29 finished with value: 63428.39676362205 and parameters: {'learning_rate': 0.03565240364253416, 'n_estimators': 1056, 'num_leaves': 37, 'max_depth': 5, 'lambda_l1': 3.1963294978129815, 'lambda_l2': 21.900361787277518, 'feature_fraction': 0.7054623619704049, 'bagging_fraction': 0.8992325761984531, 'bagging_freq': 7}. Best is trial 21 with value: 62960.3741566168.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "LightGBM Error model tuning complete. Best validation RMSE: $62,960.37\n",
      "\n",
      "--- K-Fold training the final LightGBM error model... ---\n",
      "  Training LightGBM error model for fold 1/5...\n",
      "Training until validation scores don't improve for 75 rounds\n",
      "  Training LightGBM error model for fold 2/5...\n",
      "Training until validation scores don't improve for 75 rounds\n",
      "Early stopping, best iteration is:\n",
      "[471]\tvalid_0's l2: 3.61934e+09\n",
      "  Training LightGBM error model for fold 3/5...\n",
      "Training until validation scores don't improve for 75 rounds\n",
      "  Training LightGBM error model for fold 4/5...\n",
      "Training until validation scores don't improve for 75 rounds\n",
      "  Training LightGBM error model for fold 5/5...\n",
      "Training until validation scores don't improve for 75 rounds\n",
      "Early stopping, best iteration is:\n",
      "[651]\tvalid_0's l2: 3.6831e+09\n",
      "\n",
      "Final LightGBM Error Model OOF RMSE: $60,583.05\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# BLOCK 5.5 (LIGHTGBM): TUNE AND TRAIN THE LIGHTGBM ERROR MODEL\n",
    "# =============================================================================\n",
    "import lightgbm as lgb\n",
    "\n",
    "# --- 1. Prepare Features and Target for the Error Model ---\n",
    "print(\"\\n--- Preparing features for the LightGBM error model ---\")\n",
    "# The error target and features are the same as the other error models.\n",
    "\n",
    "# --- 2. Tune the Error Model with Optuna ---\n",
    "print(\"\\n--- Tuning the LightGBM Error Model with Optuna... ---\")\n",
    "# We use the same train/validation split for tuning efficiency.\n",
    "# X_train_err_opt, X_val_err_opt, y_train_err_opt, y_val_err_opt are already defined.\n",
    "\n",
    "def objective_error_model_lgbm(trial):\n",
    "    params = {\n",
    "        'objective': 'regression_l1', # MAE is robust for error modeling\n",
    "        'metric': 'rmse',\n",
    "        'random_state': RANDOM_STATE,\n",
    "        'n_jobs': -1,\n",
    "        'boosting_type': 'gbdt',\n",
    "        'verbosity': -1, # Suppress warnings\n",
    "        'learning_rate': trial.suggest_float('learning_rate', 0.01, 0.05, log=True),\n",
    "        'n_estimators': trial.suggest_int('n_estimators', 1000, 3000),\n",
    "        'num_leaves': trial.suggest_int('num_leaves', 10, 40), # Shallower for error model\n",
    "        'max_depth': trial.suggest_int('max_depth', 3, 6),\n",
    "        'lambda_l1': trial.suggest_float('lambda_l1', 1.0, 50.0, log=True),\n",
    "        'lambda_l2': trial.suggest_float('lambda_l2', 1.0, 50.0, log=True),\n",
    "        'feature_fraction': trial.suggest_float('feature_fraction', 0.6, 0.9),\n",
    "        'bagging_fraction': trial.suggest_float('bagging_fraction', 0.6, 0.9),\n",
    "        'bagging_freq': trial.suggest_int('bagging_freq', 1, 7)\n",
    "    }\n",
    "    \n",
    "    model = lgb.LGBMRegressor(**params)\n",
    "    model.fit(\n",
    "        X_train_err_opt, y_train_err_opt,\n",
    "        eval_set=[(X_val_err_opt, y_val_err_opt)],\n",
    "        callbacks=[lgb.early_stopping(75, verbose=False)]\n",
    "    )\n",
    "    \n",
    "    preds = model.predict(X_val_err_opt)\n",
    "    rmse = np.sqrt(mean_squared_error(y_val_err_opt, preds))\n",
    "    return rmse\n",
    "\n",
    "study_error_lgbm = optuna.create_study(direction='minimize')\n",
    "study_error_lgbm.optimize(objective_error_model_lgbm, n_trials=30) # 30 trials is a good balance\n",
    "best_params_error_lgbm = study_error_lgbm.best_params\n",
    "print(f\"\\nLightGBM Error model tuning complete. Best validation RMSE: ${study_error_lgbm.best_value:,.2f}\")\n",
    "\n",
    "# --- 3. Final K-Fold Training of the Error Model ---\n",
    "print(\"\\n--- K-Fold training the final LightGBM error model... ---\")\n",
    "oof_error_preds_lgbm = np.zeros(len(X))\n",
    "test_error_preds_lgbm = np.zeros(len(X_test))\n",
    "skf_lgbm = StratifiedKFold(n_splits=N_SPLITS, shuffle=True, random_state=RANDOM_STATE)\n",
    "\n",
    "for fold, (train_idx, val_idx) in enumerate(skf_lgbm.split(X_for_error, grade_for_stratify)):\n",
    "    print(f\"  Training LightGBM error model for fold {fold+1}/{N_SPLITS}...\")\n",
    "    X_train_err, X_val_err = X_for_error.iloc[train_idx], X_for_error.iloc[val_idx]\n",
    "    y_train_err, y_val_err = error_target.iloc[train_idx], error_target.iloc[val_idx]\n",
    "\n",
    "    model_err_lgbm = lgb.LGBMRegressor(**best_params_error_lgbm, early_stopping_rounds=75)\n",
    "    model_err_lgbm.fit(X_train_err, y_train_err,\n",
    "                       eval_set=[(X_val_err, y_val_err)],\n",
    "                       callbacks=[lgb.early_stopping(75, verbose=False)])\n",
    "    \n",
    "    oof_error_preds_lgbm[val_idx] = np.clip(model_err_lgbm.predict(X_val_err), 0, None)\n",
    "    test_error_preds_lgbm += np.clip(model_err_lgbm.predict(X_test_for_error), 0, None) / N_SPLITS\n",
    "\n",
    "print(f\"\\nFinal LightGBM Error Model OOF RMSE: ${np.sqrt(mean_squared_error(error_target, oof_error_preds_lgbm)):,.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9909782e-7944-4c9f-a684-e41bc5ae01ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Saving all error model prediction arrays to './error_models/' ---\n",
      "XGBoost error predictions saved successfully.\n",
      "CatBoost error predictions saved successfully.\n",
      "\n",
      "All necessary prediction files are now ready for the final blending notebook.\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# BLOCK 7: SAVE ALL SPECIALIST ERROR MODEL PREDICTIONS\n",
    "# =============================================================================\n",
    "import os\n",
    "import numpy as np\n",
    "\n",
    "# --- Define a dedicated path for error model predictions ---\n",
    "ERROR_PREDS_PATH = './error_models/' \n",
    "os.makedirs(ERROR_PREDS_PATH, exist_ok=True) # Create the directory if it doesn't exist\n",
    "\n",
    "print(f\"--- Saving all error model prediction arrays to '{ERROR_PREDS_PATH}' ---\")\n",
    "\n",
    "# --- Save XGBoost Error Model Predictions ---\n",
    "# This assumes your variables are named 'oof_error_preds' and 'test_error_preds'\n",
    "try:\n",
    "    np.save(os.path.join(ERROR_PREDS_PATH, 'oof_error_preds_xgb.npy'), oof_error_preds)\n",
    "    np.save(os.path.join(ERROR_PREDS_PATH, 'test_error_preds_xgb.npy'), test_error_preds)\n",
    "    print(\"XGBoost error predictions saved successfully.\")\n",
    "except NameError:\n",
    "    print(\"ERROR: Could not find XGBoost error prediction variables ('oof_error_preds', 'test_error_preds'). Please ensure the XGBoost error model block was run.\")\n",
    "\n",
    "# --- Save CatBoost Error Model Predictions ---\n",
    "# This assumes your variables are named 'oof_error_preds_cb' and 'test_error_preds_cb'\n",
    "try:\n",
    "    np.save(os.path.join(ERROR_PREDS_PATH, 'oof_error_preds_cb.npy'), oof_error_preds_cb)\n",
    "    np.save(os.path.join(ERROR_PREDS_PATH, 'test_error_preds_cb.npy'), test_error_preds_cb)\n",
    "    print(\"CatBoost error predictions saved successfully.\")\n",
    "except NameError:\n",
    "    print(\"ERROR: Could not find CatBoost error prediction variables ('oof_error_preds_cb', 'test_error_preds_cb'). Please ensure the CatBoost error model block was run.\")\n",
    "\n",
    "try:\n",
    "    np.save(os.path.join(ERROR_PREDS_PATH, 'oof_error_preds_lgbm.npy'), oof_error_preds)\n",
    "    np.save(os.path.join(ERROR_PREDS_PATH, 'test_error_preds_lgbm.npy'), test_error_preds)\n",
    "    print(\"LightGBM error predictions saved successfully.\")\n",
    "except NameError:\n",
    "    print(\"ERROR: Could not find LightGBM error prediction variables ('oof_error_preds', 'test_error_preds'). Please ensure the LightGBM error model block was run.\")\n",
    "\n",
    "\n",
    "print(\"\\nAll necessary prediction files are now ready for the final blending notebook.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a1bcde59-f972-41b4-bae5-cfa1bc414045",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Searching for the optimal blend weight for the two error models... ---\n",
      "  Testing Blend (XGB/CB): 0.00/1.00  |  Score: $294,477.42 (a=1.979, b=2.174)\n",
      "  Testing Blend (XGB/CB): 0.05/0.95  |  Score: $294,230.82 (a=1.976, b=2.173)\n",
      "  Testing Blend (XGB/CB): 0.10/0.90  |  Score: $294,002.98 (a=1.976, b=2.174)\n",
      "  Testing Blend (XGB/CB): 0.15/0.85  |  Score: $293,791.14 (a=1.976, b=2.172)\n",
      "  Testing Blend (XGB/CB): 0.20/0.80  |  Score: $293,598.66 (a=1.978, b=2.173)\n",
      "  Testing Blend (XGB/CB): 0.25/0.75  |  Score: $293,427.28 (a=1.980, b=2.172)\n",
      "  Testing Blend (XGB/CB): 0.30/0.70  |  Score: $293,274.23 (a=1.979, b=2.172)\n",
      "  Testing Blend (XGB/CB): 0.35/0.65  |  Score: $293,136.91 (a=1.979, b=2.172)\n",
      "  Testing Blend (XGB/CB): 0.40/0.60  |  Score: $293,018.14 (a=1.981, b=2.175)\n",
      "  Testing Blend (XGB/CB): 0.45/0.55  |  Score: $292,917.57 (a=1.982, b=2.174)\n",
      "  Testing Blend (XGB/CB): 0.50/0.50  |  Score: $292,834.91 (a=1.982, b=2.174)\n",
      "  Testing Blend (XGB/CB): 0.55/0.45  |  Score: $292,768.15 (a=1.982, b=2.174)\n",
      "  Testing Blend (XGB/CB): 0.60/0.40  |  Score: $292,719.23 (a=1.983, b=2.174)\n",
      "  Testing Blend (XGB/CB): 0.65/0.35  |  Score: $292,690.54 (a=1.984, b=2.173)\n",
      "  Testing Blend (XGB/CB): 0.70/0.30  |  Score: $292,680.50 (a=1.984, b=2.172)\n",
      "  Testing Blend (XGB/CB): 0.75/0.25  |  Score: $292,688.94 (a=1.981, b=2.170)\n",
      "  Testing Blend (XGB/CB): 0.80/0.20  |  Score: $292,718.39 (a=1.981, b=2.172)\n",
      "  Testing Blend (XGB/CB): 0.85/0.15  |  Score: $292,764.84 (a=1.980, b=2.172)\n",
      "  Testing Blend (XGB/CB): 0.90/0.10  |  Score: $292,831.29 (a=1.980, b=2.173)\n",
      "  Testing Blend (XGB/CB): 0.95/0.05  |  Score: $292,919.53 (a=1.981, b=2.175)\n",
      "  Testing Blend (XGB/CB): 1.00/0.00  |  Score: $293,029.10 (a=1.983, b=2.177)\n",
      "\n",
      "--- Building the final error ensemble with the optimal blend... ---\n",
      "\n",
      "============================================================\n",
      "FINAL RESULTS & SUBMISSION\n",
      "============================================================\n",
      "Final OOF Winkler Score (Optimally Blended Ensemble of Ensembles): $292,680.50\n",
      "Optimal Error Blend (XGB/CB): 0.70 / 0.30\n",
      "Optimal Multipliers: a=1.9837, b=2.1720\n"
     ]
    }
   ],
   "source": [
    "# =======================================================================================================\n",
    "# FINAL BLOCK (DEFINITIVE): OPTIMAL ERROR BLEND, PRECISE CALIBRATION, AND SUBMISSION\n",
    "# =======================================================================================================\n",
    "from scipy.optimize import minimize\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "# --- 1. Search for the Optimal Blend of Error Models ---\n",
    "print(\"\\n--- Searching for the optimal blend weight for the two error models... ---\")\n",
    "\n",
    "best_blend_score = float('inf')\n",
    "best_blend_weight = 0.5 # Default to 50/50\n",
    "best_blend_a = 1.0\n",
    "best_blend_b = 1.0\n",
    "\n",
    "# We will test weights for the XGBoost error model from 0% to 100% in 5% increments\n",
    "for xgb_weight in np.arange(0, 1.05, 0.05):\n",
    "    cb_weight = 1.0 - xgb_weight\n",
    "    \n",
    "    # Create the blended error predictions for this specific weight\n",
    "    oof_error_blend = (oof_error_preds * xgb_weight) + (oof_error_preds_cb * cb_weight)\n",
    "    oof_error_final_ensemble = np.clip(oof_error_blend, 0, None)\n",
    "\n",
    "    # --- Calibrate THIS specific blend ---\n",
    "    def get_winkler_from_multipliers(multipliers):\n",
    "        a, b = multipliers[0], multipliers[1]\n",
    "        low = oof_ensemble_mean - oof_error_final_ensemble * a\n",
    "        high = oof_ensemble_mean + oof_error_final_ensemble * b\n",
    "        return winkler_score(y_true, low, high)\n",
    "\n",
    "    initial_guess = [1.9, 2.2]\n",
    "    bounds = [(1.0, 3.0), (1.0, 3.0)]\n",
    "    result_calib = minimize(get_winkler_from_multipliers, initial_guess, method='L-BFGS-B', bounds=bounds)\n",
    "    \n",
    "    current_score = result_calib.fun\n",
    "    current_a, current_b = result_calib.x\n",
    "    \n",
    "    print(f\"  Testing Blend (XGB/CB): {xgb_weight:.2f}/{cb_weight:.2f}  |  Score: ${current_score:,.2f} (a={current_a:.3f}, b={current_b:.3f})\")\n",
    "\n",
    "    # If this blend is the best so far, save its parameters\n",
    "    if current_score < best_blend_score:\n",
    "        best_blend_score = current_score\n",
    "        best_blend_weight = xgb_weight\n",
    "        best_blend_a = current_a\n",
    "        best_blend_b = current_b\n",
    "\n",
    "# --- 2. Build the Final Error Ensemble with the Best Weight ---\n",
    "print(\"\\n--- Building the final error ensemble with the optimal blend... ---\")\n",
    "final_xgb_weight = best_blend_weight\n",
    "final_cb_weight = 1.0 - final_xgb_weight\n",
    "test_error_final_ensemble = (test_error_preds * final_xgb_weight) + (test_error_preds_cb * final_cb_weight)\n",
    "\n",
    "# --- 3. Final Results and Submission ---\n",
    "print(\"\\n\" + \"=\"*60); print(\"FINAL RESULTS & SUBMISSION\"); print(\"=\"*60)\n",
    "print(f\"Final OOF Winkler Score (Optimally Blended Ensemble of Ensembles): ${best_blend_score:,.2f}\")\n",
    "print(f\"Optimal Error Blend (XGB/CB): {final_xgb_weight:.2f} / {final_cb_weight:.2f}\")\n",
    "print(f\"Optimal Multipliers: a={best_blend_a:.4f}, b={best_blend_b:.4f}\")\n",
    "\n",
    "# Create the final prediction intervals for the test set\n",
    "final_lower = test_ensemble_mean - np.clip(test_error_final_ensemble, 0, None) * best_blend_a\n",
    "final_upper = test_ensemble_mean + np.clip(test_error_final_ensemble, 0, None) * best_blend_b\n",
    "final_upper = np.maximum(final_lower + 1, final_upper)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "994c9181-ddc3-45ec-b899-ca392da40594",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "'submission_final_OptimalEoE_292680.csv' created successfully! Good luck on the leaderboard!\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>pi_lower</th>\n",
       "      <th>pi_upper</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>200000</td>\n",
       "      <td>818976.003200</td>\n",
       "      <td>1.012106e+06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>200001</td>\n",
       "      <td>578449.575873</td>\n",
       "      <td>7.983806e+05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>200002</td>\n",
       "      <td>450278.368874</td>\n",
       "      <td>6.537181e+05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>200003</td>\n",
       "      <td>294652.612316</td>\n",
       "      <td>4.240232e+05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>200004</td>\n",
       "      <td>346969.921670</td>\n",
       "      <td>7.985549e+05</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       id       pi_lower      pi_upper\n",
       "0  200000  818976.003200  1.012106e+06\n",
       "1  200001  578449.575873  7.983806e+05\n",
       "2  200002  450278.368874  6.537181e+05\n",
       "3  200003  294652.612316  4.240232e+05\n",
       "4  200004  346969.921670  7.985549e+05"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Create the submission DataFrame\n",
    "submission_df = pd.DataFrame({'id': pd.read_csv('./test.csv')['id'], \n",
    "                              'pi_lower': final_lower, \n",
    "                              'pi_upper': final_upper})\n",
    "\n",
    "# Save the submission file\n",
    "submission_filename = f'submission_final_OptimalEoE_{int(best_blend_score)}.csv'\n",
    "submission_df.to_csv(submission_filename, index=False)\n",
    "\n",
    "print(f\"\\n'{submission_filename}' created successfully! Good luck on the leaderboard!\")\n",
    "display(submission_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d0ab5d8e-9e8b-4b66-b01a-0beb30ad2eb7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Searching for the optimal blend of 3 error models AND their calibration multipliers... ---\n",
      "\n",
      "--- Building the final test set predictions with the optimal blend... ---\n",
      "\n",
      "============================================================\n",
      "FINAL RESULTS & SUBMISSION\n",
      "============================================================\n",
      "Final OOF Winkler Score (4 Mean + 3 Error Ensemble): $292,680.26\n",
      "Optimal Error Blend (XGB/CB/LGBM): 0.7112 / 0.2888 / 0.0000\n",
      "Optimal Multipliers: a=1.9837, b=2.1723\n"
     ]
    }
   ],
   "source": [
    "# =======================================================================================================\n",
    "# FINAL BLOCK (DEFINITIVE): OPTIMAL 3-ERROR-MODEL BLEND, PRECISE CALIBRATION, AND SUBMISSION\n",
    "# =======================================================================================================\n",
    "from scipy.optimize import minimize\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "# --- 1. Define the Objective Function for the Optimizer ---\n",
    "# This master function will find the best blend AND the best calibration simultaneously.\n",
    "\n",
    "print(\"\\n--- Searching for the optimal blend of 3 error models AND their calibration multipliers... ---\")\n",
    "\n",
    "# Stack the OOF error predictions for easy matrix multiplication\n",
    "oof_error_stack = np.vstack([oof_error_preds, oof_error_preds_cb, oof_error_preds_lgbm]).T\n",
    "\n",
    "def get_winkler_from_blend_and_calibration(params):\n",
    "    \"\"\"\n",
    "    This function takes a single array of parameters and unpacks it:\n",
    "    - params[0]: weight for XGB error model\n",
    "    - params[1]: weight for CatBoost error model\n",
    "    - params[2]: weight for LightGBM error model\n",
    "    - params[3]: multiplier 'a' for the lower bound\n",
    "    - params[4]: multiplier 'b' for the upper bound\n",
    "    \"\"\"\n",
    "    # Unpack the parameters\n",
    "    weights = params[:3]\n",
    "    a = params[3]\n",
    "    b = params[4]\n",
    "    \n",
    "    # Create the blended error prediction using the given weights\n",
    "    oof_error_blend = np.dot(oof_error_stack, weights)\n",
    "    oof_error_final_ensemble = np.clip(oof_error_blend, 0, None)\n",
    "    \n",
    "    # Calculate the calibrated prediction intervals\n",
    "    low = oof_ensemble_mean - oof_error_final_ensemble * a\n",
    "    high = oof_ensemble_mean + oof_error_final_ensemble * b\n",
    "    \n",
    "    # Return the Winkler score for this specific combination\n",
    "    return winkler_score(y_true, low, high)\n",
    "\n",
    "# --- 2. Run the Optimizer ---\n",
    "# We need an initial guess and bounds for all 5 parameters (3 weights + 2 multipliers)\n",
    "initial_guess = [1/3, 1/3, 1/3, 1.9, 2.2]\n",
    "bounds = [(0, 1), (0, 1), (0, 1), (1.0, 3.0), (1.0, 3.0)]\n",
    "\n",
    "# Add a constraint that the sum of the weights must equal 1\n",
    "weights_constraint = {'type': 'eq', 'fun': lambda params: 1.0 - np.sum(params[:3])}\n",
    "\n",
    "result = minimize(\n",
    "    fun=get_winkler_from_blend_and_calibration,\n",
    "    x0=initial_guess,\n",
    "    bounds=bounds,\n",
    "    constraints=[weights_constraint],\n",
    "    method='SLSQP'\n",
    ")\n",
    "\n",
    "# Extract the best parameters found by the optimizer\n",
    "best_params = result.x\n",
    "best_score = result.fun\n",
    "best_error_weights = best_params[:3]\n",
    "best_a = best_params[3]\n",
    "best_b = best_params[4]\n",
    "\n",
    "# --- 3. Build the Final Test Set Predictions ---\n",
    "print(\"\\n--- Building the final test set predictions with the optimal blend... ---\")\n",
    "test_error_stack = np.vstack([test_error_preds, test_error_preds_cb, test_error_preds_lgbm]).T\n",
    "test_error_final_ensemble = np.dot(test_error_stack, best_error_weights)\n",
    "\n",
    "# --- 4. Final Results and Submission ---\n",
    "print(\"\\n\" + \"=\"*60); print(\"FINAL RESULTS & SUBMISSION\"); print(\"=\"*60)\n",
    "print(f\"Final OOF Winkler Score (4 Mean + 3 Error Ensemble): ${best_score:,.2f}\")\n",
    "print(f\"Optimal Error Blend (XGB/CB/LGBM): {best_error_weights[0]:.4f} / {best_error_weights[1]:.4f} / {best_error_weights[2]:.4f}\")\n",
    "print(f\"Optimal Multipliers: a={best_a:.4f}, b={best_b:.4f}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "067f2494-8dfa-414a-9842-97a0ce7ab30f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "'submission_final_4M3E_292680.csv' created successfully! Good luck on the leaderboard!\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>pi_lower</th>\n",
       "      <th>pi_upper</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>200000</td>\n",
       "      <td>818979.243810</td>\n",
       "      <td>1.012118e+06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>200001</td>\n",
       "      <td>578291.681163</td>\n",
       "      <td>7.985706e+05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>200002</td>\n",
       "      <td>450231.469687</td>\n",
       "      <td>6.537853e+05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>200003</td>\n",
       "      <td>294631.580892</td>\n",
       "      <td>4.240563e+05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>200004</td>\n",
       "      <td>347391.215159</td>\n",
       "      <td>7.981286e+05</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       id       pi_lower      pi_upper\n",
       "0  200000  818979.243810  1.012118e+06\n",
       "1  200001  578291.681163  7.985706e+05\n",
       "2  200002  450231.469687  6.537853e+05\n",
       "3  200003  294631.580892  4.240563e+05\n",
       "4  200004  347391.215159  7.981286e+05"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Create the final prediction intervals for the test set\n",
    "final_lower = test_ensemble_mean - np.clip(test_error_final_ensemble, 0, None) * best_a\n",
    "final_upper = test_ensemble_mean + np.clip(test_error_final_ensemble, 0, None) * best_b\n",
    "final_upper = np.maximum(final_lower + 1, final_upper)\n",
    "\n",
    "# Create the submission DataFrame\n",
    "submission_df = pd.DataFrame({'id': pd.read_csv('./test.csv')['id'], \n",
    "                              'pi_lower': final_lower, \n",
    "                              'pi_upper': final_upper})\n",
    "\n",
    "# Save the submission file\n",
    "submission_filename = f'submission_final_4M3E_{int(best_score)}.csv'\n",
    "submission_df.to_csv(submission_filename, index=False)\n",
    "\n",
    "print(f\"\\n'{submission_filename}' created successfully! Good luck on the leaderboard!\")\n",
    "display(submission_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "835379f2-4e26-4000-a45e-45f3ae49ebe7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (Kaggle Comp)",
   "language": "python",
   "name": "kaggle-comp"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d33a4648-2943-40a8-8af7-ae50570d8d81",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Libraries imported successfully.\n",
      "Raw data loaded successfully.\n",
      "Setup complete.\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# BLOCK 1: SETUP, IMPORTS, AND DATA LOADING\n",
    "# =============================================================================\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "import time\n",
    "import os\n",
    "# --- Library Imports ---\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import gc\n",
    "from sklearn.model_selection import StratifiedKFold, train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import xgboost as xgb\n",
    "import catboost as cb\n",
    "import optuna\n",
    "from scipy.optimize import minimize\n",
    "print(\"Libraries imported successfully.\")\n",
    "# --- Helper Function for Winkler Score ---\n",
    "def winkler_score(y_true, lower, upper, alpha=0.1, return_coverage=False):\n",
    "    width = upper - lower\n",
    "    penalty_lower = np.where(y_true < lower, (2 / alpha) * (lower - y_true), 0)\n",
    "    penalty_upper = np.where(y_true > upper, (2 / alpha) * (y_true - upper), 0)\n",
    "    score = width + penalty_lower + penalty_upper\n",
    "    if return_coverage:\n",
    "        coverage = np.mean((y_true >= lower) & (y_true <= upper))\n",
    "        return np.mean(score), coverage\n",
    "    return np.mean(score)\n",
    "# --- Global Constants ---\n",
    "N_SPLITS = 5\n",
    "RANDOM_STATE = 42\n",
    "DATA_PATH = './'\n",
    "N_OPTUNA_TRIALS = 30 # A strong number for a comprehensive search\n",
    "COMPETITION_ALPHA = 0.1\n",
    "\n",
    "# --- Load Raw Data ---\n",
    "try:\n",
    "    # We drop the low-variance columns they identified right away\n",
    "    drop_cols=['id', 'golf', 'view_rainier', 'view_skyline', 'view_lakesamm','view_otherwater', 'view_other']\n",
    "    df_train = pd.read_csv(DATA_PATH + 'dataset.csv').drop(columns=drop_cols)\n",
    "    df_test = pd.read_csv(DATA_PATH + 'test.csv').drop(columns=drop_cols)\n",
    "    print(\"Raw data loaded successfully.\")\n",
    "except FileNotFoundError:\n",
    "    print(\"ERROR: Could not find 'dataset.csv' or 'test.csv'.\")\n",
    "    exit()\n",
    "# --- Prepare Target Variable ---\n",
    "y_true = df_train['sale_price'].copy()\n",
    "grade_for_stratify = df_train['grade'].copy()\n",
    "# The mean-error model works best when predicting the raw price directly\n",
    "# So, we will NOT log-transform the target this time.\n",
    "# df_train.drop('sale_price', axis=1, inplace=True) # We keep sale_price for FE\n",
    "print(\"Setup complete.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3a94b99e-758e-4790-9a36-e704094fb922",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Starting Block 2.5: Executing Feature Engineering Pipeline ---\n",
      "--- Starting Comprehensive Feature Engineering ---\n",
      "Step 1: Creating brute-force numerical interaction features...\n",
      "Step 2: Creating date features...\n",
      "Step 3: Creating TF-IDF features for text columns...\n",
      "Step 4: Creating group-by aggregation features...\n",
      "Step 5: Creating ratio features...\n",
      "Step 6: Creating geospatial clustering features...\n",
      "Step 7: Finalizing feature set...\n",
      "\n",
      "Comprehensive FE complete. Total features: 233\n",
      "Feature engineering complete. X shape: (200000, 233), X_test shape: (200000, 233)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Make sure to have these libraries installed\n",
    "# pip install pandas numpy scikit-learn\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.cluster import KMeans\n",
    "import gc\n",
    "\n",
    "# Define a random state for reproducibility\n",
    "RANDOM_STATE = 42\n",
    "\n",
    "def create_comprehensive_features(df_train, df_test):\n",
    "    \"\"\"\n",
    "    Combines original and new advanced feature engineering steps into a single pipeline.\n",
    "    \"\"\"\n",
    "    print(\"--- Starting Comprehensive Feature Engineering ---\")\n",
    "\n",
    "    # Store original indices and target variable\n",
    "    train_ids = df_train.index\n",
    "    test_ids = df_test.index\n",
    "    y_train = df_train['sale_price'].copy() # Keep the target separate\n",
    "\n",
    "    # Combine for consistent processing\n",
    "    df_train_temp = df_train.drop(columns=['sale_price'])\n",
    "    all_data = pd.concat([df_train_temp, df_test], axis=0, ignore_index=True)\n",
    "\n",
    "    # --- Original Feature Engineering ---\n",
    "\n",
    "    # A) Brute-Force Numerical Interactions\n",
    "    print(\"Step 1: Creating brute-force numerical interaction features...\")\n",
    "    NUMS = ['area', 'land_val', 'imp_val', 'sqft_lot', 'sqft', 'sqft_1', 'grade', 'year_built']\n",
    "    # Ensure all columns exist and are numeric, fill missing with 0 for safety\n",
    "    for col in NUMS:\n",
    "        if col not in all_data.columns:\n",
    "            all_data[col] = 0\n",
    "        else:\n",
    "            all_data[col] = pd.to_numeric(all_data[col], errors='coerce').fillna(0)\n",
    "            \n",
    "    for i in range(len(NUMS)):\n",
    "        for j in range(i + 1, len(NUMS)):\n",
    "            all_data[f'{NUMS[i]}_x_{NUMS[j]}'] = all_data[NUMS[i]] * all_data[NUMS[j]]\n",
    "\n",
    "    # B) Date Features\n",
    "    print(\"Step 2: Creating date features...\")\n",
    "    all_data['sale_date'] = pd.to_datetime(all_data['sale_date'])\n",
    "    all_data['sale_year'] = all_data['sale_date'].dt.year\n",
    "    all_data['sale_month'] = all_data['sale_date'].dt.month\n",
    "    all_data['sale_dayofyear'] = all_data['sale_date'].dt.dayofyear\n",
    "    all_data['age_at_sale'] = all_data['sale_year'] - all_data['year_built']\n",
    "\n",
    "    # C) TF-IDF Text Features\n",
    "    print(\"Step 3: Creating TF-IDF features for text columns...\")\n",
    "    text_cols = ['subdivision', 'zoning', 'city', 'sale_warning', 'join_status', 'submarket']\n",
    "    all_data[text_cols] = all_data[text_cols].fillna('missing').astype(str)\n",
    "    \n",
    "    for col in text_cols:\n",
    "        tfidf = TfidfVectorizer(analyzer='char', ngram_range=(3, 5), max_features=128, binary=True)\n",
    "        svd = TruncatedSVD(n_components=8, random_state=RANDOM_STATE)\n",
    "        \n",
    "        tfidf_matrix = tfidf.fit_transform(all_data[col])\n",
    "        tfidf_svd = svd.fit_transform(tfidf_matrix)\n",
    "        \n",
    "        tfidf_df = pd.DataFrame(tfidf_svd, columns=[f'{col}_tfidf_svd_{i}' for i in range(8)])\n",
    "        all_data = pd.concat([all_data, tfidf_df], axis=1)\n",
    "\n",
    "    # D) Log transform some interaction features\n",
    "    for c in ['land_val_x_imp_val', 'land_val_x_sqft', 'imp_val_x_sqft']:\n",
    "        if c in all_data.columns:\n",
    "            all_data[c] = np.log1p(all_data[c].fillna(0))\n",
    "\n",
    "    # --- New Feature Engineering Ideas ---\n",
    "\n",
    "    # F) Group-By Aggregation Features\n",
    "    print(\"Step 4: Creating group-by aggregation features...\")\n",
    "    group_cols = ['submarket', 'city', 'zoning']\n",
    "    num_cols_for_agg = ['grade', 'sqft', 'imp_val', 'land_val', 'age_at_sale']\n",
    "\n",
    "    for group_col in group_cols:\n",
    "        for num_col in num_cols_for_agg:\n",
    "            agg_stats = all_data.groupby(group_col)[num_col].agg(['mean', 'std', 'max', 'min']).reset_index()\n",
    "            agg_stats.columns = [group_col] + [f'{group_col}_{num_col}_{stat}' for stat in ['mean', 'std', 'max', 'min']]\n",
    "            all_data = pd.merge(all_data, agg_stats, on=group_col, how='left')\n",
    "            all_data[f'{num_col}_minus_{group_col}_mean'] = all_data[num_col] - all_data[f'{group_col}_{num_col}_mean']\n",
    "\n",
    "    # G) Ratio Features\n",
    "    print(\"Step 5: Creating ratio features...\")\n",
    "    # Add a small epsilon to prevent division by zero\n",
    "    epsilon = 1e-6 \n",
    "    all_data['total_val'] = all_data['imp_val'] + all_data['land_val']\n",
    "    all_data['imp_val_to_land_val_ratio'] = all_data['imp_val'] / (all_data['land_val'] + epsilon)\n",
    "    all_data['land_val_ratio'] = all_data['land_val'] / (all_data['total_val'] + epsilon)\n",
    "    all_data['sqft_to_lot_ratio'] = all_data['sqft'] / (all_data['sqft_lot'] + epsilon)\n",
    "    all_data['was_renovated'] = (all_data['year_reno'] > 0).astype(int)\n",
    "    all_data['reno_age_at_sale'] = np.where(all_data['was_renovated'] == 1, all_data['sale_year'] - all_data['year_reno'], -1)\n",
    "\n",
    "    # H) Geospatial Clustering Features\n",
    "    print(\"Step 6: Creating geospatial clustering features...\")\n",
    "    coords = all_data[['latitude', 'longitude']].copy()\n",
    "    coords.fillna(coords.median(), inplace=True) # Simple imputation\n",
    "\n",
    "    # KMeans is sensitive to feature scaling, but for lat/lon it's often okay without it.\n",
    "    kmeans = KMeans(n_clusters=20, random_state=RANDOM_STATE, n_init=10) \n",
    "    all_data['location_cluster'] = kmeans.fit_predict(coords)\n",
    "    \n",
    "    # Calculate distance to each cluster center\n",
    "    cluster_centers = kmeans.cluster_centers_\n",
    "    for i in range(len(cluster_centers)):\n",
    "        center = cluster_centers[i]\n",
    "        all_data[f'dist_to_cluster_{i}'] = np.sqrt((coords['latitude'] - center[0])**2 + (coords['longitude'] - center[1])**2)\n",
    "\n",
    "    # --- Final Cleanup ---\n",
    "    print(\"Step 7: Finalizing feature set...\")\n",
    "    cols_to_drop = ['sale_date', 'subdivision', 'zoning', 'city', 'sale_warning', 'join_status', 'submarket']\n",
    "    all_data = all_data.drop(columns=cols_to_drop)\n",
    "\n",
    "    # One-hot encode the new cluster feature\n",
    "    all_data = pd.get_dummies(all_data, columns=['location_cluster'], prefix='loc_cluster')\n",
    "    \n",
    "    # Final check for any remaining object columns to be safe (besides index)\n",
    "    object_cols = all_data.select_dtypes(include='object').columns\n",
    "    if len(object_cols) > 0:\n",
    "        print(f\"Warning: Found unexpected object columns: {object_cols}. Dropping them.\")\n",
    "        all_data = all_data.drop(columns=object_cols)\n",
    "        \n",
    "    all_data.fillna(0, inplace=True)\n",
    "\n",
    "    # Separate back into train and test sets\n",
    "    train_len = len(train_ids)\n",
    "    X = all_data.iloc[:train_len].copy()\n",
    "    X_test = all_data.iloc[train_len:].copy()\n",
    "    \n",
    "    # Restore original indices\n",
    "    X.index = train_ids\n",
    "    X_test.index = test_ids\n",
    "    \n",
    "    # Align columns - crucial for model prediction\n",
    "    X_test = X_test[X.columns]\n",
    "    \n",
    "    print(f\"\\nComprehensive FE complete. Total features: {X.shape[1]}\")\n",
    "    gc.collect()\n",
    "    \n",
    "    return X, X_test, y_train\n",
    "# =============================================================================\n",
    "# BLOCK 2.5: EXECUTE FEATURE ENGINEERING\n",
    "# =============================================================================\n",
    "print(\"\\n--- Starting Block 2.5: Executing Feature Engineering Pipeline ---\")\n",
    "\n",
    "# This is the crucial step that was missing.\n",
    "# We call the function to create our training and testing dataframes.\n",
    "X, X_test, y_train = create_comprehensive_features(df_train, df_test)\n",
    "\n",
    "# Let's verify the output\n",
    "print(f\"Feature engineering complete. X shape: {X.shape}, X_test shape: {X_test.shape}\")\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1fc25c52-2f59-4ab9-9878-c98a2b97e3de",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Loading all base model predictions from saved .npy files... ---\n",
      "All MEAN AND ERROR models predictions loaded successfully.\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# BLOCK 3: LOAD ALL PRE-TRAINED MODEL PREDICTIONS\n",
    "# =============================================================================\n",
    "\n",
    "# Define paths to your saved prediction files\n",
    "PREDS_SAVE_PATH = './mean_models_v1/' # For XGB and CatBoost preds\n",
    "NN_PREDS_PATH = './NN_model_predictions/' # For NN preds\n",
    "ERR_PATH = './error_models/' # For error preds\n",
    "\n",
    "print(\"--- Loading all base model predictions from saved .npy files... ---\")\n",
    "try:\n",
    "    # Load Mean Model OOF (Out-of-Fold) Predictions\n",
    "    oof_xgb_preds = np.load(os.path.join(PREDS_SAVE_PATH, 'oof_xgb_preds.npy'))\n",
    "    oof_cb_preds = np.load(os.path.join(PREDS_SAVE_PATH, 'oof_cb_preds.npy'))\n",
    "    oof_lgbm_preds = np.load(os.path.join(PREDS_SAVE_PATH, 'oof_lgbm_preds.npy'))\n",
    "    oof_nn_preds = np.load(os.path.join(NN_PREDS_PATH, 'oof_nn_preds.npy'))\n",
    "\n",
    "    oof_error_preds_cb = np.load(os.path.join(ERR_PATH, 'oof_error_preds_cb.npy'))\n",
    "    oof_error_preds_lgbm = np.load(os.path.join(ERR_PATH, 'oof_error_preds_lgbm.npy'))\n",
    "    oof_error_preds_xgb = np.load(os.path.join(ERR_PATH, 'oof_error_preds_xgb.npy'))\n",
    "    \n",
    "    # Load Mean Model Test Predictions\n",
    "    test_xgb_preds = np.load(os.path.join(PREDS_SAVE_PATH, 'test_xgb_preds.npy'))\n",
    "    test_cb_preds = np.load(os.path.join(PREDS_SAVE_PATH, 'test_cb_preds.npy'))\n",
    "    test_lgbm_preds = np.load(os.path.join(PREDS_SAVE_PATH, 'test_lgbm_preds.npy'))\n",
    "    test_nn_preds = np.load(os.path.join(NN_PREDS_PATH, 'test_nn_preds.npy'))\n",
    "\n",
    "    test_error_preds_cb = np.load(os.path.join(ERR_PATH, 'test_error_preds_cb.npy'))\n",
    "    test_error_preds_lgbm = np.load(os.path.join(ERR_PATH, 'test_error_preds_lgbm.npy'))\n",
    "    test_error_preds_xgb = np.load(os.path.join(ERR_PATH, 'test_error_preds_xgb.npy'))\n",
    "\n",
    "     \n",
    "    \n",
    "    print(\"All MEAN AND ERROR models predictions loaded successfully.\")\n",
    "    \n",
    "except FileNotFoundError as e:\n",
    "    print(f\"\\nERROR: Could not find a required prediction file. {e}\")\n",
    "    print(\"Please ensure you have run all training notebooks and saved their predictions first.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e8c81f2b-cce5-4d7b-b3a4-f09551d44b2d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Training a simple model to determine feature importance... ---\n",
      "Feature importance ranking created.\n",
      "\n",
      "--- Building an ELITE feature set with reduced features and stacked predictions ---\n",
      "Selected the top 25 raw features to reduce noise.\n",
      "\n",
      "--- Engineering and adding VOLATILITY features (leakage-proof method) ---\n",
      "\n",
      "Elite feature set for quantile models created successfully.\n",
      "Final Shape: (200000, 34)\n",
      "Total features include: 25 raw + 7 stacked preds + 2 volatility features = 34\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# BLOCK 5.5 (Corrected): ELITE FEATURE SET WITH VOLATILITY FEATURES\n",
    "# =============================================================================\n",
    "# This block creates the definitive feature set for the quantile models.\n",
    "#\n",
    "# NEW IMPROVEMENT: We are now engineering features specifically designed to\n",
    "# capture price VOLATILITY. This gives the quantile models a direct signal\n",
    "# about which groups of houses have a wider or narrower price distribution,\n",
    "# which is exactly what they need to predict the tails accurately.\n",
    "#\n",
    "# We also drastically reduce the number of raw features to N=25 to combat\n",
    "# overfitting, forcing the model to rely on these powerful new signals.\n",
    "\n",
    "# --- Step 1: Generate Feature Importance Ranking (Same as before) ---\n",
    "print(\"\\n--- Training a simple model to determine feature importance... ---\")\n",
    "dtrain_importance = xgb.DMatrix(X, label=y_true)\n",
    "params = {'objective': 'reg:squarederror', 'eval_metric': 'rmse', 'seed': RANDOM_STATE, 'n_jobs': -1}\n",
    "bst_for_importance = xgb.train(params, dtrain_importance, num_boost_round=500, verbose_eval=False)\n",
    "importance_scores = bst_for_importance.get_score(importance_type='gain')\n",
    "feature_importance = pd.DataFrame({\n",
    "    'Feature': importance_scores.keys(),\n",
    "    'Importance': importance_scores.values()\n",
    "}).sort_values(by='Importance', ascending=False).reset_index(drop=True)\n",
    "print(\"Feature importance ranking created.\")\n",
    "\n",
    "# --- Step 2: Build the ELITE Feature Set ---\n",
    "print(\"\\n--- Building an ELITE feature set with reduced features and stacked predictions ---\")\n",
    "\n",
    "# Drastically reduce feature count to combat overfitting.\n",
    "N_TOP_FEATURES = 25\n",
    "elite_raw_features = feature_importance['Feature'].head(N_TOP_FEATURES).tolist()\n",
    "print(f\"Selected the top {N_TOP_FEATURES} raw features to reduce noise.\")\n",
    "\n",
    "# Create the base dataframes\n",
    "X_for_quantile = X[elite_raw_features].copy()\n",
    "X_test_for_quantile = X_test[elite_raw_features].copy()\n",
    "\n",
    "# Add the stacked predictions (mean and error models)\n",
    "# These are still the most powerful features.\n",
    "# (Code to add oof_mean_*, oof_error_*, test_mean_*, test_error_* preds)\n",
    "for pred_name, oof_pred, test_pred in [\n",
    "    ('oof_mean_xgb', oof_xgb_preds, test_xgb_preds), ('oof_mean_cb', oof_cb_preds, test_cb_preds),\n",
    "    ('oof_mean_lgbm', oof_lgbm_preds, test_lgbm_preds), ('oof_mean_nn', oof_nn_preds, test_nn_preds),\n",
    "    ('oof_error_xgb', oof_error_preds_xgb, test_error_preds_xgb), ('oof_error_cb', oof_error_preds_cb, test_error_preds_cb),\n",
    "    ('oof_error_lgbm', oof_error_preds_lgbm, test_error_preds_lgbm)\n",
    "]:\n",
    "    X_for_quantile[pred_name] = oof_pred\n",
    "    X_test_for_quantile[pred_name] = test_pred\n",
    "\n",
    "# --- Step 3: Create and Add Volatility Features (The Leakage-Proof Way) ---\n",
    "# This is the key improvement. We must create these features inside a CV loop.\n",
    "print(\"\\n--- Engineering and adding VOLATILITY features (leakage-proof method) ---\")\n",
    "\n",
    "# We will create OOF features for the train set and a single set for the test set.\n",
    "# Initialize new feature columns with NaNs\n",
    "X_for_quantile['price_std_by_submarket'] = np.nan\n",
    "X_for_quantile['price_range_by_grade'] = np.nan\n",
    "X_test_for_quantile['price_std_by_submarket'] = np.nan\n",
    "X_test_for_quantile['price_range_by_grade'] = np.nan\n",
    "\n",
    "# We will average the test set calculations over the folds\n",
    "test_std_agg = np.zeros(len(X_test_for_quantile))\n",
    "test_range_agg = np.zeros(len(X_test_for_quantile))\n",
    "\n",
    "skf = StratifiedKFold(n_splits=N_SPLITS, shuffle=True, random_state=RANDOM_STATE)\n",
    "\n",
    "for fold, (train_idx, val_idx) in enumerate(skf.split(X, grade_for_stratify)):\n",
    "    # Get the training data for THIS FOLD ONLY\n",
    "    X_train_fold, y_train_fold = df_train.iloc[train_idx], y_true.iloc[train_idx]\n",
    "    \n",
    "    # --- Feature 1: Price Standard Deviation by Submarket ---\n",
    "    # Calculate the std only on the fold's training data\n",
    "    std_map = y_train_fold.groupby(X_train_fold['submarket']).std()\n",
    "    # Map these values to the validation set for this fold\n",
    "    val_stds = df_train.iloc[val_idx]['submarket'].map(std_map)\n",
    "    X_for_quantile.loc[X_for_quantile.index[val_idx], 'price_std_by_submarket'] = val_stds\n",
    "    # Map to the test set and add to the aggregate\n",
    "    test_std_agg += df_test['submarket'].map(std_map) / N_SPLITS\n",
    "\n",
    "    # --- Feature 2: Price Range by Grade ---\n",
    "    # Calculate the range (max-min) only on the fold's training data\n",
    "    range_map = y_train_fold.groupby(X_train_fold['grade']).apply(lambda x: x.max() - x.min())\n",
    "    # Map these values to the validation set for this fold\n",
    "    val_ranges = df_train.iloc[val_idx]['grade'].map(range_map)\n",
    "    X_for_quantile.loc[X_for_quantile.index[val_idx], 'price_range_by_grade'] = val_ranges\n",
    "    # Map to the test set and add to the aggregate\n",
    "    test_range_agg += df_test['grade'].map(range_map) / N_SPLITS\n",
    "\n",
    "# Assign the averaged features to the test set\n",
    "X_test_for_quantile['price_std_by_submarket'] = test_std_agg\n",
    "X_test_for_quantile['price_range_by_grade'] = test_range_agg\n",
    "\n",
    "# --- Step 4: Final Cleanup and Report ---\n",
    "# Fill any NaNs that might have occurred if a category in val/test was not in train\n",
    "# (e.g., using the global median as a fallback)\n",
    "global_std_median = X_for_quantile['price_std_by_submarket'].median()\n",
    "global_range_median = X_for_quantile['price_range_by_grade'].median()\n",
    "\n",
    "X_for_quantile.fillna({\n",
    "    'price_std_by_submarket': global_std_median,\n",
    "    'price_range_by_grade': global_range_median\n",
    "}, inplace=True)\n",
    "X_test_for_quantile.fillna({\n",
    "    'price_std_by_submarket': global_std_median,\n",
    "    'price_range_by_grade': global_range_median\n",
    "}, inplace=True)\n",
    "\n",
    "# Final alignment check\n",
    "X_test_for_quantile = X_test_for_quantile[X_for_quantile.columns]\n",
    "\n",
    "total_features = X_for_quantile.shape[1]\n",
    "print(f\"\\nElite feature set for quantile models created successfully.\")\n",
    "print(f\"Final Shape: {X_for_quantile.shape}\")\n",
    "print(f\"Total features include: {N_TOP_FEATURES} raw + 7 stacked preds + 2 volatility features = {total_features}\")\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "038310b2-0d19-4196-a824-3fbd15d30b9f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-07-25 20:05:30,314] A new study created in memory with name: no-name-f74bb3b3-bf8c-4c51-839e-0b5f2ae45c79\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Preparing data for LightGBM Optuna tuning ---\n",
      "\n",
      "--- Tuning the LightGBM Lower-Bound Model (alpha=0.05)... ---\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bac2e37d09bb4aa186b66ccff742ca42",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/50 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-07-25 20:05:37,948] Trial 0 finished with value: 6860.680883437204 and parameters: {'learning_rate': 0.01624558920804399, 'n_estimators': 1975, 'num_leaves': 81, 'max_depth': 8, 'lambda_l1': 6.149418998196093, 'lambda_l2': 0.06082070250054613, 'feature_fraction': 0.7814762143030334, 'bagging_fraction': 0.7789449320917392, 'bagging_freq': 1}. Best is trial 0 with value: 6860.680883437204.\n",
      "[I 2025-07-25 20:05:39,680] Trial 1 finished with value: 6887.548198043642 and parameters: {'learning_rate': 0.06963590301846011, 'n_estimators': 887, 'num_leaves': 94, 'max_depth': 7, 'lambda_l1': 8.183727050895508, 'lambda_l2': 0.6685165770330787, 'feature_fraction': 0.8921604128851564, 'bagging_fraction': 0.9105833881260805, 'bagging_freq': 1}. Best is trial 0 with value: 6860.680883437204.\n",
      "[I 2025-07-25 20:05:45,934] Trial 2 finished with value: 6876.043811150028 and parameters: {'learning_rate': 0.019354917781911014, 'n_estimators': 1366, 'num_leaves': 137, 'max_depth': 11, 'lambda_l1': 0.22429572671497688, 'lambda_l2': 2.2685340663604214, 'feature_fraction': 0.9168929382376447, 'bagging_fraction': 0.8821270173084359, 'bagging_freq': 6}. Best is trial 0 with value: 6860.680883437204.\n",
      "[I 2025-07-25 20:05:48,724] Trial 3 finished with value: 6872.730671978789 and parameters: {'learning_rate': 0.03602414974461841, 'n_estimators': 929, 'num_leaves': 134, 'max_depth': 8, 'lambda_l1': 9.092913793654235, 'lambda_l2': 0.06127536464177313, 'feature_fraction': 0.69730283205182, 'bagging_fraction': 0.6310884098416332, 'bagging_freq': 7}. Best is trial 0 with value: 6860.680883437204.\n",
      "[I 2025-07-25 20:05:50,763] Trial 4 finished with value: 6890.704184130207 and parameters: {'learning_rate': 0.03763820043266627, 'n_estimators': 1089, 'num_leaves': 146, 'max_depth': 8, 'lambda_l1': 0.046499355139778466, 'lambda_l2': 4.0542260276336854, 'feature_fraction': 0.8234569535055218, 'bagging_fraction': 0.9662909800812267, 'bagging_freq': 4}. Best is trial 0 with value: 6860.680883437204.\n",
      "[I 2025-07-25 20:05:54,899] Trial 5 finished with value: 6856.569639553547 and parameters: {'learning_rate': 0.02776515435258503, 'n_estimators': 926, 'num_leaves': 81, 'max_depth': 7, 'lambda_l1': 36.86126186144685, 'lambda_l2': 0.022854593734153345, 'feature_fraction': 0.7552742573925516, 'bagging_fraction': 0.9245385470818048, 'bagging_freq': 4}. Best is trial 5 with value: 6856.569639553547.\n",
      "[I 2025-07-25 20:05:56,366] Trial 6 finished with value: 6850.78599308275 and parameters: {'learning_rate': 0.05930233056121849, 'n_estimators': 1954, 'num_leaves': 121, 'max_depth': 6, 'lambda_l1': 2.430269988228486, 'lambda_l2': 1.5057276780474842, 'feature_fraction': 0.7945015074738312, 'bagging_fraction': 0.7937928668028309, 'bagging_freq': 4}. Best is trial 6 with value: 6850.78599308275.\n",
      "[I 2025-07-25 20:05:57,720] Trial 7 finished with value: 6872.09543533712 and parameters: {'learning_rate': 0.08414936044299244, 'n_estimators': 2153, 'num_leaves': 42, 'max_depth': 11, 'lambda_l1': 0.09699588310057487, 'lambda_l2': 40.17872119572263, 'feature_fraction': 0.8491312296020351, 'bagging_fraction': 0.8498523338500734, 'bagging_freq': 5}. Best is trial 6 with value: 6850.78599308275.\n",
      "[I 2025-07-25 20:06:00,784] Trial 8 finished with value: 6880.545837847899 and parameters: {'learning_rate': 0.05471127330544703, 'n_estimators': 2048, 'num_leaves': 124, 'max_depth': 7, 'lambda_l1': 31.756690073143186, 'lambda_l2': 0.031680997043393534, 'feature_fraction': 0.8588703536052713, 'bagging_fraction': 0.6722785596421142, 'bagging_freq': 6}. Best is trial 6 with value: 6850.78599308275.\n",
      "[I 2025-07-25 20:06:03,364] Trial 9 finished with value: 6878.3277577287045 and parameters: {'learning_rate': 0.023429267493937398, 'n_estimators': 969, 'num_leaves': 150, 'max_depth': 8, 'lambda_l1': 0.06265722723524983, 'lambda_l2': 2.1904811555804953, 'feature_fraction': 0.7407699522090583, 'bagging_fraction': 0.6030883218345497, 'bagging_freq': 7}. Best is trial 6 with value: 6850.78599308275.\n",
      "[I 2025-07-25 20:06:11,799] Trial 10 finished with value: 6820.448271958186 and parameters: {'learning_rate': 0.01257764315279147, 'n_estimators': 2816, 'num_leaves': 107, 'max_depth': 5, 'lambda_l1': 0.8421824037090525, 'lambda_l2': 98.90956643396686, 'feature_fraction': 0.63352684660382, 'bagging_fraction': 0.7490901865311451, 'bagging_freq': 3}. Best is trial 10 with value: 6820.448271958186.\n",
      "[I 2025-07-25 20:06:17,211] Trial 11 finished with value: 6814.59575679699 and parameters: {'learning_rate': 0.010103740199848906, 'n_estimators': 2897, 'num_leaves': 107, 'max_depth': 5, 'lambda_l1': 0.8475259687131079, 'lambda_l2': 76.9431866613182, 'feature_fraction': 0.6390470225391565, 'bagging_fraction': 0.7471030439334717, 'bagging_freq': 3}. Best is trial 11 with value: 6814.59575679699.\n",
      "[I 2025-07-25 20:06:23,859] Trial 12 finished with value: 6820.500317359446 and parameters: {'learning_rate': 0.010148870688180605, 'n_estimators': 2969, 'num_leaves': 104, 'max_depth': 5, 'lambda_l1': 0.7988291033069364, 'lambda_l2': 79.90759822662756, 'feature_fraction': 0.603612249335137, 'bagging_fraction': 0.7298428502369602, 'bagging_freq': 2}. Best is trial 11 with value: 6814.59575679699.\n",
      "[I 2025-07-25 20:06:31,041] Trial 13 finished with value: 6814.031598672509 and parameters: {'learning_rate': 0.010477538325231845, 'n_estimators': 2998, 'num_leaves': 60, 'max_depth': 5, 'lambda_l1': 0.010673607668945272, 'lambda_l2': 21.28708361966206, 'feature_fraction': 0.6263726745632358, 'bagging_fraction': 0.7223117805978678, 'bagging_freq': 3}. Best is trial 13 with value: 6814.031598672509.\n",
      "[I 2025-07-25 20:06:37,102] Trial 14 finished with value: 6861.554359275517 and parameters: {'learning_rate': 0.013337141331860132, 'n_estimators': 2554, 'num_leaves': 57, 'max_depth': 10, 'lambda_l1': 0.01266111173758206, 'lambda_l2': 14.738004939204112, 'feature_fraction': 0.6747631293341507, 'bagging_fraction': 0.6920441306334213, 'bagging_freq': 3}. Best is trial 13 with value: 6814.031598672509.\n",
      "[I 2025-07-25 20:06:41,860] Trial 15 finished with value: 6840.557669053281 and parameters: {'learning_rate': 0.010419287678486405, 'n_estimators': 2488, 'num_leaves': 22, 'max_depth': 5, 'lambda_l1': 0.011499277815876167, 'lambda_l2': 13.85383248646779, 'feature_fraction': 0.9709251021786187, 'bagging_fraction': 0.8184794099965705, 'bagging_freq': 2}. Best is trial 13 with value: 6814.031598672509.\n",
      "[I 2025-07-25 20:06:45,248] Trial 16 finished with value: 7119.330029111162 and parameters: {'learning_rate': 0.016555580638639156, 'n_estimators': 2605, 'num_leaves': 64, 'max_depth': 6, 'lambda_l1': 95.22659642132248, 'lambda_l2': 0.3986566132174279, 'feature_fraction': 0.6812385345351671, 'bagging_fraction': 0.7101021241503295, 'bagging_freq': 3}. Best is trial 13 with value: 6814.031598672509.\n",
      "[I 2025-07-25 20:06:48,864] Trial 17 finished with value: 6850.0029051191805 and parameters: {'learning_rate': 0.020328197047673684, 'n_estimators': 1528, 'num_leaves': 66, 'max_depth': 6, 'lambda_l1': 0.2615479135195884, 'lambda_l2': 12.789853981291099, 'feature_fraction': 0.6356926624017154, 'bagging_fraction': 0.6581185992045318, 'bagging_freq': 2}. Best is trial 13 with value: 6814.031598672509.\n",
      "[I 2025-07-25 20:07:20,697] Trial 18 finished with value: 6838.397643283213 and parameters: {'learning_rate': 0.014003253998316902, 'n_estimators': 2336, 'num_leaves': 48, 'max_depth': 10, 'lambda_l1': 0.030464375942141037, 'lambda_l2': 34.24944719235582, 'feature_fraction': 0.6063889315352124, 'bagging_fraction': 0.7745019539096849, 'bagging_freq': 5}. Best is trial 13 with value: 6814.031598672509.\n",
      "[I 2025-07-25 20:07:24,772] Trial 19 finished with value: 6836.739675100875 and parameters: {'learning_rate': 0.027468575512813335, 'n_estimators': 2981, 'num_leaves': 29, 'max_depth': 5, 'lambda_l1': 0.2677159434448291, 'lambda_l2': 6.06119012229607, 'feature_fraction': 0.7312852256956304, 'bagging_fraction': 0.8328595004150776, 'bagging_freq': 3}. Best is trial 13 with value: 6814.031598672509.\n",
      "[I 2025-07-25 20:07:26,695] Trial 20 finished with value: 6860.376500341031 and parameters: {'learning_rate': 0.04610294065737008, 'n_estimators': 2745, 'num_leaves': 73, 'max_depth': 12, 'lambda_l1': 1.5517690150739127, 'lambda_l2': 0.2575576521973075, 'feature_fraction': 0.6511834357572667, 'bagging_fraction': 0.7444088395648014, 'bagging_freq': 5}. Best is trial 13 with value: 6814.031598672509.\n",
      "[I 2025-07-25 20:07:32,912] Trial 21 finished with value: 6823.893614135379 and parameters: {'learning_rate': 0.012273156000619127, 'n_estimators': 2771, 'num_leaves': 105, 'max_depth': 5, 'lambda_l1': 0.6043323859673209, 'lambda_l2': 72.79638417842465, 'feature_fraction': 0.639931224183542, 'bagging_fraction': 0.752162483058763, 'bagging_freq': 3}. Best is trial 13 with value: 6814.031598672509.\n",
      "[I 2025-07-25 20:07:41,240] Trial 22 finished with value: 6833.940409176598 and parameters: {'learning_rate': 0.01003768114496887, 'n_estimators': 2806, 'num_leaves': 108, 'max_depth': 6, 'lambda_l1': 1.9314707002128237, 'lambda_l2': 96.81916815212435, 'feature_fraction': 0.7108115744943893, 'bagging_fraction': 0.7123337794654441, 'bagging_freq': 2}. Best is trial 13 with value: 6814.031598672509.\n",
      "[I 2025-07-25 20:07:47,134] Trial 23 finished with value: 6828.913967692216 and parameters: {'learning_rate': 0.012363984461999076, 'n_estimators': 2368, 'num_leaves': 93, 'max_depth': 5, 'lambda_l1': 0.139050349161812, 'lambda_l2': 31.308224478994966, 'feature_fraction': 0.6562353339921054, 'bagging_fraction': 0.7573268225918743, 'bagging_freq': 4}. Best is trial 13 with value: 6814.031598672509.\n",
      "[I 2025-07-25 20:07:51,739] Trial 24 finished with value: 6836.626797103724 and parameters: {'learning_rate': 0.0156751916429454, 'n_estimators': 2984, 'num_leaves': 116, 'max_depth': 6, 'lambda_l1': 0.559186996969805, 'lambda_l2': 25.84650003715137, 'feature_fraction': 0.6164675433031289, 'bagging_fraction': 0.6512385132529986, 'bagging_freq': 3}. Best is trial 13 with value: 6814.031598672509.\n",
      "[I 2025-07-25 20:08:01,016] Trial 25 finished with value: 6847.852146275224 and parameters: {'learning_rate': 0.011652284057712639, 'n_estimators': 1649, 'num_leaves': 94, 'max_depth': 7, 'lambda_l1': 5.1643905267861365, 'lambda_l2': 6.96768383576494, 'feature_fraction': 0.7141999862814894, 'bagging_fraction': 0.8055997275823736, 'bagging_freq': 2}. Best is trial 13 with value: 6814.031598672509.\n",
      "[I 2025-07-25 20:08:06,706] Trial 26 finished with value: 6827.102831424125 and parameters: {'learning_rate': 0.019301179397326004, 'n_estimators': 2691, 'num_leaves': 113, 'max_depth': 5, 'lambda_l1': 3.330259175004067, 'lambda_l2': 53.191939003426, 'feature_fraction': 0.6745670584536392, 'bagging_fraction': 0.6912340404948024, 'bagging_freq': 3}. Best is trial 13 with value: 6814.031598672509.\n",
      "[I 2025-07-25 20:08:10,772] Trial 27 finished with value: 6831.335720720643 and parameters: {'learning_rate': 0.01444392490188725, 'n_estimators': 2273, 'num_leaves': 88, 'max_depth': 6, 'lambda_l1': 0.023257149327027972, 'lambda_l2': 21.502100018745516, 'feature_fraction': 0.6310376425821324, 'bagging_fraction': 0.7261844675268821, 'bagging_freq': 1}. Best is trial 13 with value: 6814.031598672509.\n",
      "[I 2025-07-25 20:08:18,337] Trial 28 finished with value: 6836.583603664586 and parameters: {'learning_rate': 0.011829283915246961, 'n_estimators': 2839, 'num_leaves': 127, 'max_depth': 5, 'lambda_l1': 20.244264821892305, 'lambda_l2': 92.02012309069885, 'feature_fraction': 0.760796344787618, 'bagging_fraction': 0.8468833334933974, 'bagging_freq': 4}. Best is trial 13 with value: 6814.031598672509.\n",
      "[I 2025-07-25 20:08:23,098] Trial 29 finished with value: 6851.972713836382 and parameters: {'learning_rate': 0.017396958897504814, 'n_estimators': 2603, 'num_leaves': 79, 'max_depth': 7, 'lambda_l1': 1.2581827406139912, 'lambda_l2': 8.438579274500041, 'feature_fraction': 0.6613224567991534, 'bagging_fraction': 0.7742449671684118, 'bagging_freq': 1}. Best is trial 13 with value: 6814.031598672509.\n",
      "[I 2025-07-25 20:08:26,830] Trial 30 finished with value: 6850.486905037603 and parameters: {'learning_rate': 0.023961546257368176, 'n_estimators': 1762, 'num_leaves': 100, 'max_depth': 6, 'lambda_l1': 0.453175293026548, 'lambda_l2': 45.56486809587101, 'feature_fraction': 0.6959653109461541, 'bagging_fraction': 0.778772640454155, 'bagging_freq': 2}. Best is trial 13 with value: 6814.031598672509.\n",
      "[I 2025-07-25 20:08:38,273] Trial 31 finished with value: 6823.113398510459 and parameters: {'learning_rate': 0.010445947136460534, 'n_estimators': 2923, 'num_leaves': 102, 'max_depth': 5, 'lambda_l1': 0.9676466550559785, 'lambda_l2': 58.879365673559576, 'feature_fraction': 0.6152045171640148, 'bagging_fraction': 0.7294507685979311, 'bagging_freq': 2}. Best is trial 13 with value: 6814.031598672509.\n",
      "[I 2025-07-25 20:08:43,479] Trial 32 finished with value: 6819.355603359799 and parameters: {'learning_rate': 0.010183825440513501, 'n_estimators': 2874, 'num_leaves': 74, 'max_depth': 5, 'lambda_l1': 1.0159186046583744, 'lambda_l2': 99.57766161977966, 'feature_fraction': 0.6077129831115807, 'bagging_fraction': 0.6894622791681864, 'bagging_freq': 3}. Best is trial 13 with value: 6814.031598672509.\n",
      "[I 2025-07-25 20:08:47,205] Trial 33 finished with value: 6852.939185397709 and parameters: {'learning_rate': 0.014446612480876767, 'n_estimators': 2456, 'num_leaves': 74, 'max_depth': 9, 'lambda_l1': 0.390082660429771, 'lambda_l2': 23.091043452890602, 'feature_fraction': 0.6291578227597774, 'bagging_fraction': 0.6848048608386231, 'bagging_freq': 3}. Best is trial 13 with value: 6814.031598672509.\n",
      "[I 2025-07-25 20:08:52,518] Trial 34 finished with value: 6814.460694723372 and parameters: {'learning_rate': 0.011813580024039648, 'n_estimators': 2843, 'num_leaves': 53, 'max_depth': 5, 'lambda_l1': 4.2881837062649195, 'lambda_l2': 0.11937066952856677, 'feature_fraction': 0.6501598309326987, 'bagging_fraction': 0.6264854140300338, 'bagging_freq': 4}. Best is trial 13 with value: 6814.031598672509.\n",
      "[I 2025-07-25 20:08:56,798] Trial 35 finished with value: 6830.28299743893 and parameters: {'learning_rate': 0.01127746779097225, 'n_estimators': 2664, 'num_leaves': 49, 'max_depth': 6, 'lambda_l1': 9.822459865283625, 'lambda_l2': 0.11124421217557323, 'feature_fraction': 0.6002615887563441, 'bagging_fraction': 0.611740857549566, 'bagging_freq': 5}. Best is trial 13 with value: 6814.031598672509.\n",
      "[I 2025-07-25 20:09:00,001] Trial 36 finished with value: 6829.576573017558 and parameters: {'learning_rate': 0.016492730262590734, 'n_estimators': 2865, 'num_leaves': 40, 'max_depth': 7, 'lambda_l1': 4.780471526808372, 'lambda_l2': 0.740534574959003, 'feature_fraction': 0.6620508147871347, 'bagging_fraction': 0.6289604730868397, 'bagging_freq': 4}. Best is trial 13 with value: 6814.031598672509.\n",
      "[I 2025-07-25 20:09:04,868] Trial 37 finished with value: 6866.200217092018 and parameters: {'learning_rate': 0.013527687877388509, 'n_estimators': 1338, 'num_leaves': 56, 'max_depth': 9, 'lambda_l1': 14.26635317685523, 'lambda_l2': 0.20639666448121788, 'feature_fraction': 0.9500546268806798, 'bagging_fraction': 0.6443295580070798, 'bagging_freq': 4}. Best is trial 13 with value: 6814.031598672509.\n",
      "[I 2025-07-25 20:09:06,499] Trial 38 finished with value: 6842.247182505228 and parameters: {'learning_rate': 0.03463143051075681, 'n_estimators': 2721, 'num_leaves': 64, 'max_depth': 6, 'lambda_l1': 0.17568915898421728, 'lambda_l2': 0.013699466589627176, 'feature_fraction': 0.7093628598602554, 'bagging_fraction': 0.6721824191909582, 'bagging_freq': 4}. Best is trial 13 with value: 6814.031598672509.\n",
      "[I 2025-07-25 20:09:09,243] Trial 39 finished with value: 6849.396637815671 and parameters: {'learning_rate': 0.018923665917467792, 'n_estimators': 2210, 'num_leaves': 37, 'max_depth': 5, 'lambda_l1': 2.959616727243227, 'lambda_l2': 0.102494477561817, 'feature_fraction': 0.7819594451996825, 'bagging_fraction': 0.6190636327912253, 'bagging_freq': 4}. Best is trial 13 with value: 6814.031598672509.\n",
      "[I 2025-07-25 20:09:15,229] Trial 40 finished with value: 6867.643028199967 and parameters: {'learning_rate': 0.011214140171564791, 'n_estimators': 2457, 'num_leaves': 85, 'max_depth': 8, 'lambda_l1': 0.09545802041120044, 'lambda_l2': 3.338367962778644, 'feature_fraction': 0.9022958582262071, 'bagging_fraction': 0.9803125455271594, 'bagging_freq': 6}. Best is trial 13 with value: 6814.031598672509.\n",
      "[I 2025-07-25 20:09:18,875] Trial 41 finished with value: 6824.604263457134 and parameters: {'learning_rate': 0.012935914493344544, 'n_estimators': 2868, 'num_leaves': 137, 'max_depth': 5, 'lambda_l1': 1.3123864411521602, 'lambda_l2': 49.55276608798525, 'feature_fraction': 0.6425575702984334, 'bagging_fraction': 0.6988165233465518, 'bagging_freq': 3}. Best is trial 13 with value: 6814.031598672509.\n",
      "[I 2025-07-25 20:09:23,125] Trial 42 finished with value: 6827.511114679289 and parameters: {'learning_rate': 0.011340257545554462, 'n_estimators': 2859, 'num_leaves': 72, 'max_depth': 5, 'lambda_l1': 3.410789201308582, 'lambda_l2': 1.0384769279560073, 'feature_fraction': 0.6868847411069594, 'bagging_fraction': 0.6708907535623708, 'bagging_freq': 3}. Best is trial 13 with value: 6814.031598672509.\n",
      "[I 2025-07-25 20:09:29,879] Trial 43 finished with value: 6827.811092325636 and parameters: {'learning_rate': 0.010033138580022732, 'n_estimators': 2730, 'num_leaves': 60, 'max_depth': 5, 'lambda_l1': 8.814571461162556, 'lambda_l2': 90.83706380122146, 'feature_fraction': 0.6244729728456765, 'bagging_fraction': 0.8808765273328796, 'bagging_freq': 3}. Best is trial 13 with value: 6814.031598672509.\n",
      "[I 2025-07-25 20:09:33,196] Trial 44 finished with value: 6838.880353652772 and parameters: {'learning_rate': 0.015164596548333806, 'n_estimators': 3000, 'num_leaves': 54, 'max_depth': 6, 'lambda_l1': 0.7896171178179756, 'lambda_l2': 19.111792435334472, 'feature_fraction': 0.6661890560337255, 'bagging_fraction': 0.7446601559236171, 'bagging_freq': 4}. Best is trial 13 with value: 6814.031598672509.\n",
      "[I 2025-07-25 20:09:37,060] Trial 45 finished with value: 6828.088182887453 and parameters: {'learning_rate': 0.01296364503725869, 'n_estimators': 2557, 'num_leaves': 79, 'max_depth': 5, 'lambda_l1': 0.346602152692225, 'lambda_l2': 0.055692216747128535, 'feature_fraction': 0.8251143331895046, 'bagging_fraction': 0.7968375606695735, 'bagging_freq': 3}. Best is trial 13 with value: 6814.031598672509.\n",
      "[I 2025-07-25 20:09:37,624] Trial 46 finished with value: 6978.6163712154985 and parameters: {'learning_rate': 0.09161317689652668, 'n_estimators': 2878, 'num_leaves': 47, 'max_depth': 6, 'lambda_l1': 59.806275371106175, 'lambda_l2': 39.64067084397121, 'feature_fraction': 0.6440504077843178, 'bagging_fraction': 0.7122961406725, 'bagging_freq': 5}. Best is trial 13 with value: 6814.031598672509.\n",
      "[I 2025-07-25 20:09:42,066] Trial 47 finished with value: 6849.3229739349845 and parameters: {'learning_rate': 0.011201502902223912, 'n_estimators': 2104, 'num_leaves': 89, 'max_depth': 7, 'lambda_l1': 0.06521335893418077, 'lambda_l2': 59.16063095734642, 'feature_fraction': 0.6200939802733683, 'bagging_fraction': 0.641733971336223, 'bagging_freq': 2}. Best is trial 13 with value: 6814.031598672509.\n",
      "[I 2025-07-25 20:09:44,088] Trial 48 finished with value: 6831.562164566883 and parameters: {'learning_rate': 0.02184065348763525, 'n_estimators': 2635, 'num_leaves': 69, 'max_depth': 5, 'lambda_l1': 1.85317274916924, 'lambda_l2': 1.5465488388607436, 'feature_fraction': 0.729307559126104, 'bagging_fraction': 0.763595277061224, 'bagging_freq': 3}. Best is trial 13 with value: 6814.031598672509.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-07-25 20:09:45,455] A new study created in memory with name: no-name-d3e34789-05af-48d0-959b-5bb57dfa32c9\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-07-25 20:09:45,451] Trial 49 finished with value: 6863.72616726848 and parameters: {'learning_rate': 0.0685723473523539, 'n_estimators': 1198, 'num_leaves': 34, 'max_depth': 6, 'lambda_l1': 0.019057023129960535, 'lambda_l2': 0.40673492882398404, 'feature_fraction': 0.6022319951342885, 'bagging_fraction': 0.9403733325884754, 'bagging_freq': 4}. Best is trial 13 with value: 6814.031598672509.\n",
      "\n",
      "--- Tuning the LightGBM Upper-Bound Model (alpha=0.95)... ---\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "29c34b200683417197b60a212c2d4e74",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/50 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-07-25 20:09:47,223] Trial 0 finished with value: 8028.4156578229995 and parameters: {'learning_rate': 0.038592412943763636, 'n_estimators': 1587, 'num_leaves': 115, 'max_depth': 8, 'lambda_l1': 0.8060047255179139, 'lambda_l2': 0.8012965778625967, 'feature_fraction': 0.9470730995025582, 'bagging_fraction': 0.6521226608958565, 'bagging_freq': 6}. Best is trial 0 with value: 8028.4156578229995.\n",
      "[I 2025-07-25 20:09:48,119] Trial 1 finished with value: 7880.496285951066 and parameters: {'learning_rate': 0.07456663541923571, 'n_estimators': 2685, 'num_leaves': 147, 'max_depth': 5, 'lambda_l1': 0.19236414389785744, 'lambda_l2': 6.603550825847633, 'feature_fraction': 0.6215737952134176, 'bagging_fraction': 0.7775101063945684, 'bagging_freq': 3}. Best is trial 1 with value: 7880.496285951066.\n",
      "[I 2025-07-25 20:09:49,550] Trial 2 finished with value: 7893.880492306065 and parameters: {'learning_rate': 0.039476909874639865, 'n_estimators': 2237, 'num_leaves': 32, 'max_depth': 12, 'lambda_l1': 0.12468667968210034, 'lambda_l2': 50.56435162210422, 'feature_fraction': 0.9906617759920259, 'bagging_fraction': 0.6290781997397703, 'bagging_freq': 4}. Best is trial 1 with value: 7880.496285951066.\n",
      "[I 2025-07-25 20:09:50,533] Trial 3 finished with value: 7931.34021231941 and parameters: {'learning_rate': 0.07921469222309901, 'n_estimators': 1110, 'num_leaves': 107, 'max_depth': 8, 'lambda_l1': 13.50949013846117, 'lambda_l2': 0.011923397539505493, 'feature_fraction': 0.9005288004159472, 'bagging_fraction': 0.6814423212161637, 'bagging_freq': 5}. Best is trial 1 with value: 7880.496285951066.\n",
      "[I 2025-07-25 20:09:52,010] Trial 4 finished with value: 7893.982738634126 and parameters: {'learning_rate': 0.05624353546081926, 'n_estimators': 1010, 'num_leaves': 84, 'max_depth': 7, 'lambda_l1': 4.594126677341751, 'lambda_l2': 0.18414989042375837, 'feature_fraction': 0.9923382533346804, 'bagging_fraction': 0.7785535937639375, 'bagging_freq': 3}. Best is trial 1 with value: 7880.496285951066.\n",
      "[I 2025-07-25 20:09:53,502] Trial 5 finished with value: 7890.991478525191 and parameters: {'learning_rate': 0.05653083118775304, 'n_estimators': 1785, 'num_leaves': 51, 'max_depth': 7, 'lambda_l1': 10.394775102251094, 'lambda_l2': 0.8886409591627905, 'feature_fraction': 0.8025820429613738, 'bagging_fraction': 0.6810056810389558, 'bagging_freq': 2}. Best is trial 1 with value: 7880.496285951066.\n",
      "[I 2025-07-25 20:09:54,463] Trial 6 finished with value: 7905.511709787953 and parameters: {'learning_rate': 0.05134968770381619, 'n_estimators': 1890, 'num_leaves': 92, 'max_depth': 5, 'lambda_l1': 77.05273028303029, 'lambda_l2': 4.362121903826083, 'feature_fraction': 0.8266365929873427, 'bagging_fraction': 0.8406955815532035, 'bagging_freq': 5}. Best is trial 1 with value: 7880.496285951066.\n",
      "[I 2025-07-25 20:09:57,142] Trial 7 finished with value: 7917.792120889044 and parameters: {'learning_rate': 0.022777592811738953, 'n_estimators': 1418, 'num_leaves': 80, 'max_depth': 8, 'lambda_l1': 0.8229852101938607, 'lambda_l2': 61.88525155717451, 'feature_fraction': 0.7950958227522371, 'bagging_fraction': 0.6952989162226166, 'bagging_freq': 4}. Best is trial 1 with value: 7880.496285951066.\n",
      "[I 2025-07-25 20:09:59,411] Trial 8 finished with value: 7851.212277942592 and parameters: {'learning_rate': 0.03101130460854385, 'n_estimators': 1529, 'num_leaves': 40, 'max_depth': 6, 'lambda_l1': 13.014627629228043, 'lambda_l2': 7.220837867858201, 'feature_fraction': 0.8476244090501832, 'bagging_fraction': 0.9243714870638264, 'bagging_freq': 4}. Best is trial 8 with value: 7851.212277942592.\n",
      "[I 2025-07-25 20:10:04,443] Trial 9 finished with value: 7924.914601044043 and parameters: {'learning_rate': 0.01229117952553528, 'n_estimators': 2614, 'num_leaves': 77, 'max_depth': 10, 'lambda_l1': 4.695082268184229, 'lambda_l2': 3.1984503104939037, 'feature_fraction': 0.8070273989768841, 'bagging_fraction': 0.8394999469392466, 'bagging_freq': 2}. Best is trial 8 with value: 7851.212277942592.\n",
      "[I 2025-07-25 20:10:07,687] Trial 10 finished with value: 7851.595272372965 and parameters: {'learning_rate': 0.021313953922729358, 'n_estimators': 2241, 'num_leaves': 22, 'max_depth': 10, 'lambda_l1': 0.012499024594719636, 'lambda_l2': 0.03360226562113953, 'feature_fraction': 0.6843574029448825, 'bagging_fraction': 0.9722405474875511, 'bagging_freq': 7}. Best is trial 8 with value: 7851.212277942592.\n",
      "[I 2025-07-25 20:10:10,623] Trial 11 finished with value: 7859.0109260077 and parameters: {'learning_rate': 0.02206524511274627, 'n_estimators': 2262, 'num_leaves': 20, 'max_depth': 10, 'lambda_l1': 0.03201514948294704, 'lambda_l2': 0.04949512674562738, 'feature_fraction': 0.6636412782424328, 'bagging_fraction': 0.9951772004340853, 'bagging_freq': 7}. Best is trial 8 with value: 7851.212277942592.\n",
      "[I 2025-07-25 20:10:14,307] Trial 12 finished with value: 7907.416842202932 and parameters: {'learning_rate': 0.021578761965010435, 'n_estimators': 2266, 'num_leaves': 48, 'max_depth': 10, 'lambda_l1': 0.01083515488041883, 'lambda_l2': 0.10704332105299602, 'feature_fraction': 0.707757675981831, 'bagging_fraction': 0.976756109328315, 'bagging_freq': 7}. Best is trial 8 with value: 7851.212277942592.\n",
      "[I 2025-07-25 20:10:17,555] Trial 13 finished with value: 7939.544265166632 and parameters: {'learning_rate': 0.013740550690111276, 'n_estimators': 1400, 'num_leaves': 50, 'max_depth': 12, 'lambda_l1': 96.94534673915035, 'lambda_l2': 20.215030622655522, 'feature_fraction': 0.7323060429570238, 'bagging_fraction': 0.9231343945569941, 'bagging_freq': 5}. Best is trial 8 with value: 7851.212277942592.\n",
      "[I 2025-07-25 20:10:19,281] Trial 14 finished with value: 7866.839030410035 and parameters: {'learning_rate': 0.028698570731694514, 'n_estimators': 2961, 'num_leaves': 21, 'max_depth': 6, 'lambda_l1': 0.1432612595290057, 'lambda_l2': 0.018819216286709173, 'feature_fraction': 0.8748447134464007, 'bagging_fraction': 0.9189194062457214, 'bagging_freq': 1}. Best is trial 8 with value: 7851.212277942592.\n",
      "[I 2025-07-25 20:10:23,812] Trial 15 finished with value: 7871.980419423629 and parameters: {'learning_rate': 0.016405992704633113, 'n_estimators': 2074, 'num_leaves': 62, 'max_depth': 11, 'lambda_l1': 23.048434660360055, 'lambda_l2': 0.41682544854100834, 'feature_fraction': 0.7243662564112039, 'bagging_fraction': 0.9241848854720058, 'bagging_freq': 6}. Best is trial 8 with value: 7851.212277942592.\n",
      "[I 2025-07-25 20:10:29,087] Trial 16 finished with value: 7874.5877456988155 and parameters: {'learning_rate': 0.010070258588255583, 'n_estimators': 1626, 'num_leaves': 35, 'max_depth': 9, 'lambda_l1': 2.233646285321886, 'lambda_l2': 11.69350931431465, 'feature_fraction': 0.7606983873422579, 'bagging_fraction': 0.8708923771163797, 'bagging_freq': 6}. Best is trial 8 with value: 7851.212277942592.\n",
      "[I 2025-07-25 20:10:31,155] Trial 17 finished with value: 7864.216436085683 and parameters: {'learning_rate': 0.03310076505722956, 'n_estimators': 820, 'num_leaves': 64, 'max_depth': 6, 'lambda_l1': 0.010914963114251625, 'lambda_l2': 2.02410655671045, 'feature_fraction': 0.6067878665981418, 'bagging_fraction': 0.96249401170697, 'bagging_freq': 3}. Best is trial 8 with value: 7851.212277942592.\n",
      "[I 2025-07-25 20:10:33,683] Trial 18 finished with value: 7873.938200608269 and parameters: {'learning_rate': 0.017918956797983113, 'n_estimators': 2542, 'num_leaves': 36, 'max_depth': 9, 'lambda_l1': 0.3920123534030741, 'lambda_l2': 0.04195471597372317, 'feature_fraction': 0.8551759241863777, 'bagging_fraction': 0.866614516456059, 'bagging_freq': 4}. Best is trial 8 with value: 7851.212277942592.\n",
      "[I 2025-07-25 20:10:36,639] Trial 19 finished with value: 8112.788452085555 and parameters: {'learning_rate': 0.027868499369996264, 'n_estimators': 2036, 'num_leaves': 150, 'max_depth': 11, 'lambda_l1': 0.041746228418567975, 'lambda_l2': 0.18429691618917643, 'feature_fraction': 0.6654267613834923, 'bagging_fraction': 0.7396503059846573, 'bagging_freq': 5}. Best is trial 8 with value: 7851.212277942592.\n",
      "[I 2025-07-25 20:10:38,570] Trial 20 finished with value: 7904.019084133173 and parameters: {'learning_rate': 0.04034329883742508, 'n_estimators': 1298, 'num_leaves': 62, 'max_depth': 7, 'lambda_l1': 1.6566328768309002, 'lambda_l2': 21.64156588743822, 'feature_fraction': 0.9295426809502194, 'bagging_fraction': 0.9448826759495168, 'bagging_freq': 7}. Best is trial 8 with value: 7851.212277942592.\n",
      "[I 2025-07-25 20:10:41,511] Trial 21 finished with value: 7852.1275950193885 and parameters: {'learning_rate': 0.023835847895384534, 'n_estimators': 2380, 'num_leaves': 20, 'max_depth': 10, 'lambda_l1': 0.0278765706651792, 'lambda_l2': 0.04844974995873282, 'feature_fraction': 0.6681063190090136, 'bagging_fraction': 0.9936315186713298, 'bagging_freq': 7}. Best is trial 8 with value: 7851.212277942592.\n",
      "[I 2025-07-25 20:10:43,834] Trial 22 finished with value: 7867.034560471626 and parameters: {'learning_rate': 0.024765199030430722, 'n_estimators': 2427, 'num_leaves': 30, 'max_depth': 11, 'lambda_l1': 0.043279618872067355, 'lambda_l2': 0.0462785723149878, 'feature_fraction': 0.6722742235800352, 'bagging_fraction': 0.9952905276541258, 'bagging_freq': 6}. Best is trial 8 with value: 7851.212277942592.\n",
      "[I 2025-07-25 20:10:47,411] Trial 23 finished with value: 7891.464996244261 and parameters: {'learning_rate': 0.017476549481932488, 'n_estimators': 1715, 'num_leaves': 42, 'max_depth': 9, 'lambda_l1': 0.021137411311324114, 'lambda_l2': 0.38544677503482855, 'feature_fraction': 0.764382603706564, 'bagging_fraction': 0.8955915545050601, 'bagging_freq': 7}. Best is trial 8 with value: 7851.212277942592.\n",
      "[I 2025-07-25 20:10:49,442] Trial 24 finished with value: 7856.25698674829 and parameters: {'learning_rate': 0.03242647283909736, 'n_estimators': 2908, 'num_leaves': 25, 'max_depth': 10, 'lambda_l1': 0.08030447774475144, 'lambda_l2': 0.024452286078593545, 'feature_fraction': 0.6420335501434512, 'bagging_fraction': 0.9599109442463728, 'bagging_freq': 6}. Best is trial 8 with value: 7851.212277942592.\n",
      "[I 2025-07-25 20:10:57,713] Trial 25 finished with value: 7862.62840975799 and parameters: {'learning_rate': 0.01870217958989891, 'n_estimators': 2052, 'num_leaves': 42, 'max_depth': 11, 'lambda_l1': 36.00516137507225, 'lambda_l2': 0.010148414706697143, 'feature_fraction': 0.688291431636738, 'bagging_fraction': 0.8943290234120531, 'bagging_freq': 4}. Best is trial 8 with value: 7851.212277942592.\n",
      "[I 2025-07-25 20:11:00,050] Trial 26 finished with value: 7871.662421606836 and parameters: {'learning_rate': 0.027672635211571865, 'n_estimators': 2433, 'num_leaves': 20, 'max_depth': 9, 'lambda_l1': 0.34961694328888104, 'lambda_l2': 0.10649753747974455, 'feature_fraction': 0.7743299636629302, 'bagging_fraction': 0.99904679380786, 'bagging_freq': 7}. Best is trial 8 with value: 7851.212277942592.\n",
      "[I 2025-07-25 20:11:01,018] Trial 27 finished with value: 7912.353959005344 and parameters: {'learning_rate': 0.09826271021173255, 'n_estimators': 1922, 'num_leaves': 54, 'max_depth': 6, 'lambda_l1': 0.020804688611444765, 'lambda_l2': 1.8428383343364376, 'feature_fraction': 0.8419972481112639, 'bagging_fraction': 0.9484482780045991, 'bagging_freq': 5}. Best is trial 8 with value: 7851.212277942592.\n",
      "[I 2025-07-25 20:11:05,227] Trial 28 finished with value: 7882.113036941793 and parameters: {'learning_rate': 0.014254300505446777, 'n_estimators': 2803, 'num_leaves': 40, 'max_depth': 10, 'lambda_l1': 0.06824494756454048, 'lambda_l2': 0.09488185876786497, 'feature_fraction': 0.6352018600906311, 'bagging_fraction': 0.8133974685727035, 'bagging_freq': 6}. Best is trial 8 with value: 7851.212277942592.\n",
      "[I 2025-07-25 20:11:06,830] Trial 29 finished with value: 7987.857763714528 and parameters: {'learning_rate': 0.036656731477667355, 'n_estimators': 1565, 'num_leaves': 123, 'max_depth': 8, 'lambda_l1': 0.3879063908691382, 'lambda_l2': 0.48584249795450773, 'feature_fraction': 0.7429390374252314, 'bagging_fraction': 0.900248383539069, 'bagging_freq': 7}. Best is trial 8 with value: 7851.212277942592.\n",
      "[I 2025-07-25 20:11:09,990] Trial 30 finished with value: 7872.427741958639 and parameters: {'learning_rate': 0.020102901338893753, 'n_estimators': 2273, 'num_leaves': 29, 'max_depth': 12, 'lambda_l1': 0.017734959929630083, 'lambda_l2': 1.4907884197491683, 'feature_fraction': 0.8932895673143723, 'bagging_fraction': 0.9703419838581269, 'bagging_freq': 2}. Best is trial 8 with value: 7851.212277942592.\n",
      "[I 2025-07-25 20:11:12,334] Trial 31 finished with value: 7875.3583403031525 and parameters: {'learning_rate': 0.04569204749916188, 'n_estimators': 2771, 'num_leaves': 27, 'max_depth': 10, 'lambda_l1': 0.08574624175531587, 'lambda_l2': 0.0209709195936616, 'feature_fraction': 0.6342620791565833, 'bagging_fraction': 0.9525394930921999, 'bagging_freq': 6}. Best is trial 8 with value: 7851.212277942592.\n",
      "[I 2025-07-25 20:11:14,442] Trial 32 finished with value: 7852.270884108816 and parameters: {'learning_rate': 0.032684589351763785, 'n_estimators': 2975, 'num_leaves': 26, 'max_depth': 10, 'lambda_l1': 0.05993325991131815, 'lambda_l2': 0.023272715344394963, 'feature_fraction': 0.6470591724433035, 'bagging_fraction': 0.9328875816566609, 'bagging_freq': 6}. Best is trial 8 with value: 7851.212277942592.\n",
      "[I 2025-07-25 20:11:17,762] Trial 33 finished with value: 7886.225265537813 and parameters: {'learning_rate': 0.025236354151827012, 'n_estimators': 2432, 'num_leaves': 34, 'max_depth': 11, 'lambda_l1': 0.03295972688957199, 'lambda_l2': 0.0410945335394838, 'feature_fraction': 0.7007179305967384, 'bagging_fraction': 0.8699297825175967, 'bagging_freq': 7}. Best is trial 8 with value: 7851.212277942592.\n",
      "[I 2025-07-25 20:11:20,449] Trial 34 finished with value: 8015.85645420467 and parameters: {'learning_rate': 0.03507647404174047, 'n_estimators': 2720, 'num_leaves': 131, 'max_depth': 9, 'lambda_l1': 0.21314598843773702, 'lambda_l2': 0.2114112669826085, 'feature_fraction': 0.6016151496649575, 'bagging_fraction': 0.9283089101772494, 'bagging_freq': 6}. Best is trial 8 with value: 7851.212277942592.\n",
      "[I 2025-07-25 20:11:21,833] Trial 35 finished with value: 7865.231354322606 and parameters: {'learning_rate': 0.04068890290794125, 'n_estimators': 2136, 'num_leaves': 100, 'max_depth': 5, 'lambda_l1': 0.014664753755624855, 'lambda_l2': 0.06902790357783928, 'feature_fraction': 0.6792773333466154, 'bagging_fraction': 0.9786600304702495, 'bagging_freq': 4}. Best is trial 8 with value: 7851.212277942592.\n",
      "[I 2025-07-25 20:11:24,253] Trial 36 finished with value: 7902.786316195279 and parameters: {'learning_rate': 0.025806198346451058, 'n_estimators': 2547, 'num_leaves': 43, 'max_depth': 8, 'lambda_l1': 0.055430698633150664, 'lambda_l2': 0.017039709978371266, 'feature_fraction': 0.9547640747481718, 'bagging_fraction': 0.9353095218508441, 'bagging_freq': 3}. Best is trial 8 with value: 7851.212277942592.\n",
      "[I 2025-07-25 20:11:26,648] Trial 37 finished with value: 7856.362730428737 and parameters: {'learning_rate': 0.03085388131052796, 'n_estimators': 1901, 'num_leaves': 28, 'max_depth': 11, 'lambda_l1': 4.686425358879387, 'lambda_l2': 0.032187611700900856, 'feature_fraction': 0.642420396631633, 'bagging_fraction': 0.6131878798020112, 'bagging_freq': 7}. Best is trial 8 with value: 7851.212277942592.\n",
      "[I 2025-07-25 20:11:28,292] Trial 38 finished with value: 8029.873153168872 and parameters: {'learning_rate': 0.04331361803966008, 'n_estimators': 1104, 'num_leaves': 71, 'max_depth': 10, 'lambda_l1': 0.025639278985994468, 'lambda_l2': 7.63061453010697, 'feature_fraction': 0.712446392305206, 'bagging_fraction': 0.903488127541824, 'bagging_freq': 5}. Best is trial 8 with value: 7851.212277942592.\n",
      "[I 2025-07-25 20:11:29,374] Trial 39 finished with value: 7868.230802452783 and parameters: {'learning_rate': 0.06519027366759214, 'n_estimators': 1803, 'num_leaves': 53, 'max_depth': 7, 'lambda_l1': 13.22352595316135, 'lambda_l2': 0.014109505888400257, 'feature_fraction': 0.6552697180367938, 'bagging_fraction': 0.8030387115427975, 'bagging_freq': 4}. Best is trial 8 with value: 7851.212277942592.\n",
      "[I 2025-07-25 20:11:30,868] Trial 40 finished with value: 7875.09358455948 and parameters: {'learning_rate': 0.048858583921390454, 'n_estimators': 1522, 'num_leaves': 35, 'max_depth': 9, 'lambda_l1': 1.3608852702133296, 'lambda_l2': 79.22997235204926, 'feature_fraction': 0.6204911648610747, 'bagging_fraction': 0.7633715831231923, 'bagging_freq': 7}. Best is trial 8 with value: 7851.212277942592.\n",
      "[I 2025-07-25 20:11:33,126] Trial 41 finished with value: 7863.651873794848 and parameters: {'learning_rate': 0.031681048441838416, 'n_estimators': 2923, 'num_leaves': 25, 'max_depth': 10, 'lambda_l1': 0.09181991513679293, 'lambda_l2': 0.08098374333411218, 'feature_fraction': 0.6819134203741531, 'bagging_fraction': 0.9618042697402656, 'bagging_freq': 6}. Best is trial 8 with value: 7851.212277942592.\n",
      "[I 2025-07-25 20:11:36,661] Trial 42 finished with value: 7837.5066587340625 and parameters: {'learning_rate': 0.02262989251718083, 'n_estimators': 2936, 'num_leaves': 20, 'max_depth': 10, 'lambda_l1': 0.20122090634568718, 'lambda_l2': 0.0273012154299323, 'feature_fraction': 0.6307505196149259, 'bagging_fraction': 0.9813415340135895, 'bagging_freq': 5}. Best is trial 42 with value: 7837.5066587340625.\n",
      "[I 2025-07-25 20:11:38,966] Trial 43 finished with value: 7840.710959523486 and parameters: {'learning_rate': 0.02251809689199252, 'n_estimators': 2841, 'num_leaves': 20, 'max_depth': 10, 'lambda_l1': 0.18012869127725512, 'lambda_l2': 0.027200631310691475, 'feature_fraction': 0.6146638700032586, 'bagging_fraction': 0.9813392428519383, 'bagging_freq': 5}. Best is trial 42 with value: 7837.5066587340625.\n",
      "[I 2025-07-25 20:11:41,821] Trial 44 finished with value: 7839.887551860325 and parameters: {'learning_rate': 0.021379960971063226, 'n_estimators': 2832, 'num_leaves': 20, 'max_depth': 12, 'lambda_l1': 0.1835304240259056, 'lambda_l2': 0.05937221965352143, 'feature_fraction': 0.6117935301689824, 'bagging_fraction': 0.9848364668187453, 'bagging_freq': 5}. Best is trial 42 with value: 7837.5066587340625.\n",
      "[I 2025-07-25 20:11:44,754] Trial 45 finished with value: 7873.746507576076 and parameters: {'learning_rate': 0.02043947372053886, 'n_estimators': 2822, 'num_leaves': 33, 'max_depth': 12, 'lambda_l1': 0.567793361782667, 'lambda_l2': 32.085479817600635, 'feature_fraction': 0.6197889620838158, 'bagging_fraction': 0.984448146107947, 'bagging_freq': 5}. Best is trial 42 with value: 7837.5066587340625.\n",
      "[I 2025-07-25 20:11:48,957] Trial 46 finished with value: 7891.392263128945 and parameters: {'learning_rate': 0.014954947070556801, 'n_estimators': 2637, 'num_leaves': 46, 'max_depth': 12, 'lambda_l1': 0.15302532736297014, 'lambda_l2': 0.13501275336445476, 'feature_fraction': 0.6008637917894991, 'bagging_fraction': 0.9738513546559746, 'bagging_freq': 4}. Best is trial 42 with value: 7837.5066587340625.\n",
      "[I 2025-07-25 20:11:52,582] Trial 47 finished with value: 8024.835373712757 and parameters: {'learning_rate': 0.022736143277111185, 'n_estimators': 2831, 'num_leaves': 93, 'max_depth': 11, 'lambda_l1': 0.2676777276853471, 'lambda_l2': 0.03191958362786265, 'feature_fraction': 0.8175325664166814, 'bagging_fraction': 0.9494712198888335, 'bagging_freq': 5}. Best is trial 42 with value: 7837.5066587340625.\n",
      "[I 2025-07-25 20:11:55,434] Trial 48 finished with value: 7840.827542111149 and parameters: {'learning_rate': 0.016624410713194017, 'n_estimators': 2685, 'num_leaves': 38, 'max_depth': 5, 'lambda_l1': 0.6081782986957462, 'lambda_l2': 0.2666179398693815, 'feature_fraction': 0.6219460863251971, 'bagging_fraction': 0.6525766693773847, 'bagging_freq': 3}. Best is trial 42 with value: 7837.5066587340625.\n",
      "[I 2025-07-25 20:11:58,368] Trial 49 finished with value: 7835.817135144207 and parameters: {'learning_rate': 0.015151036760775017, 'n_estimators': 2678, 'num_leaves': 57, 'max_depth': 5, 'lambda_l1': 0.5575209712586338, 'lambda_l2': 0.6395934672400004, 'feature_fraction': 0.6173604150177827, 'bagging_fraction': 0.7077726647570459, 'bagging_freq': 3}. Best is trial 49 with value: 7835.817135144207.\n",
      "\n",
      "LightGBM Tuning Complete.\n",
      "Best Lower Params: {'learning_rate': 0.010477538325231845, 'n_estimators': 2998, 'num_leaves': 60, 'max_depth': 5, 'lambda_l1': 0.010673607668945272, 'lambda_l2': 21.28708361966206, 'feature_fraction': 0.6263726745632358, 'bagging_fraction': 0.7223117805978678, 'bagging_freq': 3}\n",
      "Best Upper Params: {'learning_rate': 0.015151036760775017, 'n_estimators': 2678, 'num_leaves': 57, 'max_depth': 5, 'lambda_l1': 0.5575209712586338, 'lambda_l2': 0.6395934672400004, 'feature_fraction': 0.6173604150177827, 'bagging_fraction': 0.7077726647570459, 'bagging_freq': 3}\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# BLOCK 10: TUNE LIGHTGBM QUANTILE MODELS WITH OPTUNA\n",
    "# =============================================================================\n",
    "import lightgbm as lgb\n",
    "import optuna\n",
    "\n",
    "# Use the same elite feature set with volatility features\n",
    "# X_for_quantile, X_test_for_quantile, y_true should be available\n",
    "\n",
    "print(\"\\n--- Preparing data for LightGBM Optuna tuning ---\")\n",
    "X_train_opt, X_val_opt, y_train_opt, y_val_opt = train_test_split(\n",
    "    X_for_quantile, y_true, test_size=0.20, random_state=RANDOM_STATE\n",
    ")\n",
    "\n",
    "def objective_lgbm(trial, alpha_value):\n",
    "    \"\"\"Unified Optuna objective for LightGBM quantile models.\"\"\"\n",
    "    params = {\n",
    "        'objective': 'quantile',\n",
    "        'alpha': alpha_value,\n",
    "        'metric': 'quantile', # Pinball loss\n",
    "        'random_state': RANDOM_STATE,\n",
    "        'n_jobs': -1,\n",
    "        'verbosity': -1,\n",
    "        'boosting_type': 'gbdt',\n",
    "        \n",
    "        # --- Hyperparameters to Tune for LightGBM ---\n",
    "        'learning_rate': trial.suggest_float('learning_rate', 0.01, 0.1, log=True),\n",
    "        'n_estimators': trial.suggest_int('n_estimators', 800, 3000),\n",
    "        'num_leaves': trial.suggest_int('num_leaves', 20, 150),\n",
    "        'max_depth': trial.suggest_int('max_depth', 5, 12),\n",
    "        'lambda_l1': trial.suggest_float('lambda_l1', 1e-2, 100.0, log=True),\n",
    "        'lambda_l2': trial.suggest_float('lambda_l2', 1e-2, 100.0, log=True),\n",
    "        'feature_fraction': trial.suggest_float('feature_fraction', 0.6, 1.0),\n",
    "        'bagging_fraction': trial.suggest_float('bagging_fraction', 0.6, 1.0),\n",
    "        'bagging_freq': trial.suggest_int('bagging_freq', 1, 7),\n",
    "    }\n",
    "\n",
    "    model = lgb.LGBMRegressor(**params)\n",
    "    model.fit(\n",
    "        X_train_opt, y_train_opt,\n",
    "        eval_set=[(X_val_opt, y_val_opt)],\n",
    "        eval_metric='quantile',\n",
    "        callbacks=[lgb.early_stopping(50, verbose=False)]\n",
    "    )\n",
    "    \n",
    "    preds = model.predict(X_val_opt)\n",
    "    pinball_loss = np.mean(np.where(y_val_opt >= preds, \n",
    "                                    (y_val_opt - preds) * alpha_value, \n",
    "                                    (preds - y_val_opt) * (1 - alpha_value)))\n",
    "    return pinball_loss\n",
    "\n",
    "# --- Tune Lower-Bound Model (alpha=0.05) ---\n",
    "print(\"\\n--- Tuning the LightGBM Lower-Bound Model (alpha=0.05)... ---\")\n",
    "study_lower_lgbm = optuna.create_study(direction='minimize')\n",
    "study_lower_lgbm.optimize(lambda trial: objective_lgbm(trial, 0.05), n_trials=50, show_progress_bar=True)\n",
    "best_params_lower_lgbm = study_lower_lgbm.best_params\n",
    "\n",
    "# --- Tune Upper-Bound Model (alpha=0.95) ---\n",
    "print(\"\\n--- Tuning the LightGBM Upper-Bound Model (alpha=0.95)... ---\")\n",
    "study_upper_lgbm = optuna.create_study(direction='minimize')\n",
    "study_upper_lgbm.optimize(lambda trial: objective_lgbm(trial, 0.95), n_trials=50, show_progress_bar=True)\n",
    "best_params_upper_lgbm = study_upper_lgbm.best_params\n",
    "\n",
    "print(\"\\nLightGBM Tuning Complete.\")\n",
    "print(f\"Best Lower Params: {best_params_lower_lgbm}\")\n",
    "print(f\"Best Upper Params: {best_params_upper_lgbm}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "59e26397-9fc6-4e8c-82aa-7cbefe906e4f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Starting K-Fold training for LightGBM Quantile Models ---\n",
      "LGBM K-Fold 1/5...\n",
      "LGBM K-Fold 2/5...\n",
      "LGBM K-Fold 3/5...\n",
      "LGBM K-Fold 4/5...\n",
      "LGBM K-Fold 5/5...\n",
      "\n",
      "--- Saving LightGBM quantile predictions... ---\n",
      "LightGBM quantile predictions saved successfully.\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# BLOCK 11 (Corrected): K-FOLD TRAIN LIGHTGBM QUANTILE MODELS\n",
    "# =============================================================================\n",
    "# This block performs the final K-Fold training for the tuned LightGBM quantile\n",
    "# models.\n",
    "#\n",
    "# CORRECTION: Using correct NumPy array indexing for `y_true` instead of the\n",
    "# pandas `.iloc` method to prevent the AttributeError.\n",
    "\n",
    "import os\n",
    "import numpy as np\n",
    "import lightgbm as lgb\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "import gc\n",
    "\n",
    "# Define path for saving predictions\n",
    "META_LGBM_QUANTILE_PATH = './meta_lgbm_quantile_models/'\n",
    "os.makedirs(META_LGBM_QUANTILE_PATH, exist_ok=True)\n",
    "\n",
    "# Initialize prediction arrays\n",
    "oof_lower_preds_lgbm = np.zeros(len(X_for_quantile))\n",
    "test_lower_preds_lgbm = np.zeros(len(X_test_for_quantile))\n",
    "oof_upper_preds_lgbm = np.zeros(len(X_for_quantile))\n",
    "test_upper_preds_lgbm = np.zeros(len(X_test_for_quantile))\n",
    "\n",
    "# Initialize the K-Fold splitter\n",
    "skf = StratifiedKFold(n_splits=N_SPLITS, shuffle=True, random_state=RANDOM_STATE)\n",
    "\n",
    "print(\"\\n--- Starting K-Fold training for LightGBM Quantile Models ---\")\n",
    "for fold, (train_idx, val_idx) in enumerate(skf.split(X_for_quantile, grade_for_stratify)):\n",
    "    print(f\"LGBM K-Fold {fold+1}/{N_SPLITS}...\")\n",
    "    \n",
    "    # Split features (pandas .iloc is correct here)\n",
    "    X_train, X_val = X_for_quantile.iloc[train_idx], X_for_quantile.iloc[val_idx]\n",
    "    \n",
    "    # Split target variable (NumPy indexing is correct here)\n",
    "    y_train_fold, y_val_fold = y_true[train_idx], y_true[val_idx]\n",
    "    \n",
    "    # --- Train and predict lower-bound model ---\n",
    "    lower_model_lgbm = lgb.LGBMRegressor(objective='quantile', alpha=0.05, **best_params_lower_lgbm)\n",
    "    lower_model_lgbm.fit(\n",
    "        X_train, y_train_fold, \n",
    "        eval_set=[(X_val, y_val_fold)], \n",
    "        callbacks=[lgb.early_stopping(50, verbose=False)]\n",
    "    )\n",
    "    oof_lower_preds_lgbm[val_idx] = lower_model_lgbm.predict(X_val)\n",
    "    test_lower_preds_lgbm += lower_model_lgbm.predict(X_test_for_quantile) / N_SPLITS\n",
    "\n",
    "    # --- Train and predict upper-bound model ---\n",
    "    upper_model_lgbm = lgb.LGBMRegressor(objective='quantile', alpha=0.95, **best_params_upper_lgbm)\n",
    "    upper_model_lgbm.fit(\n",
    "        X_train, y_train_fold, \n",
    "        eval_set=[(X_val, y_val_fold)], \n",
    "        callbacks=[lgb.early_stopping(50, verbose=False)]\n",
    "    )\n",
    "    oof_upper_preds_lgbm[val_idx] = upper_model_lgbm.predict(X_val)\n",
    "    test_upper_preds_lgbm += upper_model_lgbm.predict(X_test_for_quantile) / N_SPLITS\n",
    "    \n",
    "    gc.collect()\n",
    "\n",
    "# --- Save the prediction artifacts ---\n",
    "print(\"\\n--- Saving LightGBM quantile predictions... ---\")\n",
    "np.save(os.path.join(META_LGBM_QUANTILE_PATH, 'oof_lower_preds_lgbm.npy'), oof_lower_preds_lgbm)\n",
    "np.save(os.path.join(META_LGBM_QUANTILE_PATH, 'test_lower_preds_lgbm.npy'), test_lower_preds_lgbm)\n",
    "np.save(os.path.join(META_LGBM_QUANTILE_PATH, 'oof_upper_preds_lgbm.npy'), oof_upper_preds_lgbm)\n",
    "np.save(os.path.join(META_LGBM_QUANTILE_PATH, 'test_upper_preds_lgbm.npy'), test_upper_preds_lgbm)\n",
    "print(\"LightGBM quantile predictions saved successfully.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e6a1b2ed-e304-4a00-ab91-fa2d3bb84334",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Loading all necessary prediction files from all pipelines ---\n",
      "All prediction artifacts loaded successfully.\n",
      "\n",
      "--- Recreating/Calibrating final OOF and Test bounds for each pipeline ---\n",
      "\n",
      "--- Optimizing the blend weight for the THREE interval models ---\n",
      "\n",
      "============================================================\n",
      "FINAL 3-MODEL ENSEMBLE RESULTS\n",
      "============================================================\n",
      "  A) Mean+Error Model OOF Score:     $295,017.11\n",
      "  B) XGB Quantile Model OOF Score:   $349,311.64\n",
      "  C) LGBM Quantile Model OOF Score:  $293,068.04\n",
      "------------------------------------------------------------\n",
      "Final BLENDED OOF Winkler Score:   $291,785.50\n",
      "------------------------------------------------------------\n",
      "Optimal Blend Weights:\n",
      "  -> Model A (Mean+Error):   37.30%\n",
      "  -> Model B (XGB Quantile): 0.00%\n",
      "  -> Model C (LGBM Quantile):62.70%\n",
      "\n",
      "--- Creating final blended submission file... ---\n",
      "\n",
      "'submission_FINAL_3M_BLEND_291785.csv' created successfully! This should be your best submission yet.\n",
      "\n",
      "Final Submission Head:\n",
      "       id       pi_lower      pi_upper\n",
      "0  200000  812014.599932  1.019174e+06\n",
      "1  200001  587682.397367  8.087494e+05\n",
      "2  200002  455659.289136  6.540434e+05\n",
      "3  200003  293065.786785  4.257511e+05\n",
      "4  200004  387947.222069  8.337936e+05\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "187"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# BLOCK 12 (Complete): FINAL 3-MODEL INTERVAL ENSEMBLE\n",
    "# =============================================================================\n",
    "# This is the new definitive final step. We are creating a master ensemble by\n",
    "# blending the calibrated interval bounds from our three distinct pipelines:\n",
    "# 1. The \"Mean+Error\" Model (Our Champion)\n",
    "# 2. The XGBoost \"Direct Quantile\" Model (Challenger 1)\n",
    "# 3. The LightGBM \"Direct Quantile\" Model (Challenger 2)\n",
    "#\n",
    "# The optimizer will find the best possible weights to combine them, leveraging\n",
    "# the unique strengths of each approach.\n",
    "\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy.optimize import minimize\n",
    "import gc\n",
    "\n",
    "# --- Helper Function for Winkler Score ---\n",
    "def winkler_score(y_true, lower, upper, alpha=0.1):\n",
    "    width = upper - lower\n",
    "    penalty_lower = np.where(y_true < lower, (2 / alpha) * (lower - y_true), 0)\n",
    "    penalty_upper = np.where(y_true > upper, (2 / alpha) * (y_true - upper), 0)\n",
    "    score = width + penalty_lower + penalty_upper\n",
    "    return np.mean(score)\n",
    "\n",
    "# --- Step 1: Load All Prediction Artifacts from All Pipelines ---\n",
    "print(\"\\n--- Loading all necessary prediction files from all pipelines ---\")\n",
    "\n",
    "# --- Define Paths ---\n",
    "DATA_PATH = './'\n",
    "PREDS_SAVE_PATH = './mean_models_v1/'\n",
    "NN_PREDS_PATH = './NN_model_predictions/'\n",
    "ERROR_MODELS_PATH = './error_models/'\n",
    "META_XGB_QUANTILE_PATH = './meta_quantile_models/'\n",
    "META_LGBM_QUANTILE_PATH = './meta_lgbm_quantile_models/' # Path for new LGBM preds\n",
    "\n",
    "try:\n",
    "    # --- Load Test Set Submission Files (for IDs and final bounds) ---\n",
    "    df_error_model_sub = pd.read_csv('submission_final_OptimalEoE_292680.csv')\n",
    "    df_xgb_quantile_sub = pd.read_csv('submission_direct_quantile_robust_349061.csv')\n",
    "    # We will generate the LGBM quantile submission bounds after calibration\n",
    "\n",
    "    # --- Load all OOF and Test predictions needed to recreate the intervals ---\n",
    "    df_train = pd.read_csv(os.path.join(DATA_PATH, 'dataset.csv'))\n",
    "    y_true = df_train['sale_price'].values\n",
    "\n",
    "    # Mean Model OOF & Test\n",
    "    oof_xgb_preds = np.load(os.path.join(PREDS_SAVE_PATH, 'oof_xgb_preds.npy'))\n",
    "    oof_cb_preds = np.load(os.path.join(PREDS_SAVE_PATH, 'oof_cb_preds.npy'))\n",
    "    oof_lgbm_preds = np.load(os.path.join(PREDS_SAVE_PATH, 'oof_lgbm_preds.npy'))\n",
    "    oof_nn_preds = np.load(os.path.join(NN_PREDS_PATH, 'oof_nn_preds.npy'))\n",
    "    test_xgb_preds = np.load(os.path.join(PREDS_SAVE_PATH, 'test_xgb_preds.npy'))\n",
    "    test_cb_preds = np.load(os.path.join(PREDS_SAVE_PATH, 'test_cb_preds.npy'))\n",
    "    test_lgbm_preds = np.load(os.path.join(PREDS_SAVE_PATH, 'test_lgbm_preds.npy'))\n",
    "    test_nn_preds = np.load(os.path.join(NN_PREDS_PATH, 'test_nn_preds.npy'))\n",
    "\n",
    "    # Error Model OOF\n",
    "    oof_error_preds_xgb = np.load(os.path.join(ERROR_MODELS_PATH, 'oof_error_preds_xgb.npy'))\n",
    "    oof_error_preds_cb = np.load(os.path.join(ERROR_MODELS_PATH, 'oof_error_preds_cb.npy'))\n",
    "\n",
    "    # XGB Quantile Model OOF & Test\n",
    "    oof_lower_xgb = np.load(os.path.join(META_XGB_QUANTILE_PATH, 'oof_lower_preds.npy'))\n",
    "    oof_upper_xgb = np.load(os.path.join(META_XGB_QUANTILE_PATH, 'oof_upper_preds.npy'))\n",
    "    \n",
    "    # LGBM Quantile Model OOF & Test\n",
    "    oof_lower_lgbm = np.load(os.path.join(META_LGBM_QUANTILE_PATH, 'oof_lower_preds_lgbm.npy'))\n",
    "    oof_upper_lgbm = np.load(os.path.join(META_LGBM_QUANTILE_PATH, 'oof_upper_preds_lgbm.npy'))\n",
    "    test_lower_lgbm = np.load(os.path.join(META_LGBM_QUANTILE_PATH, 'test_lower_preds_lgbm.npy'))\n",
    "    test_upper_lgbm = np.load(os.path.join(META_LGBM_QUANTILE_PATH, 'test_upper_preds_lgbm.npy'))\n",
    "\n",
    "    print(\"All prediction artifacts loaded successfully.\")\n",
    "\n",
    "except FileNotFoundError as e:\n",
    "    print(f\"\\nERROR: Could not find a required file. {e}\")\n",
    "    print(\"Please ensure ALL previous training and tuning notebooks have been run successfully.\")\n",
    "    # exit()\n",
    "\n",
    "# --- Step 2: Calibrate and Recreate Final Bounds for Each Pipeline ---\n",
    "print(\"\\n--- Recreating/Calibrating final OOF and Test bounds for each pipeline ---\")\n",
    "\n",
    "# --- Pipeline A: Mean+Error Model Bounds ---\n",
    "oof_ensemble_mean = (oof_xgb_preds + oof_cb_preds + oof_lgbm_preds + oof_nn_preds) / 4\n",
    "a_err, b_err = 1.9799, 2.1755\n",
    "oof_error_final = np.clip((oof_error_preds_xgb * 0.60 + oof_error_preds_cb * 0.40), 0, None)\n",
    "oof_lower_A = oof_ensemble_mean - oof_error_final * a_err\n",
    "oof_upper_A = oof_ensemble_mean + oof_error_final * b_err\n",
    "test_lower_A, test_upper_A = df_error_model_sub['pi_lower'].values, df_error_model_sub['pi_upper'].values\n",
    "score_A = winkler_score(y_true, oof_lower_A, oof_upper_A)\n",
    "\n",
    "# --- Pipeline B: XGB Quantile Model Bounds ---\n",
    "# These are already calibrated in the submission file\n",
    "oof_lower_B, oof_upper_B = df_xgb_quantile_sub['pi_lower'].values, df_xgb_quantile_sub['pi_upper'].values # This is an error, we need OOF\n",
    "test_lower_B, test_upper_B = df_xgb_quantile_sub['pi_lower'].values, df_xgb_quantile_sub['pi_upper'].values\n",
    "# Recreate OOF for XGB Quantile\n",
    "a_xgb, b_xgb = 0.8118, 1.1960\n",
    "oof_lower_xgb_raw = np.minimum(oof_lower_xgb, oof_upper_xgb)\n",
    "oof_upper_xgb_raw = np.maximum(oof_lower_xgb, oof_upper_xgb)\n",
    "oof_lower_B = oof_lower_xgb_raw * a_xgb\n",
    "oof_upper_B = oof_upper_xgb_raw * b_xgb\n",
    "score_B = winkler_score(y_true, oof_lower_B, oof_upper_B)\n",
    "\n",
    "\n",
    "# --- Pipeline C: LGBM Quantile Model Bounds (Needs Calibration) ---\n",
    "def get_robust_calibrated_winkler(multipliers, y_true_oof, lower_oof, upper_oof):\n",
    "    a, b = multipliers\n",
    "    lower_raw = np.minimum(lower_oof, upper_oof)\n",
    "    upper_raw = np.maximum(lower_oof, upper_oof)\n",
    "    return winkler_score(y_true_oof, lower_raw * a, upper_raw * b)\n",
    "\n",
    "res_lgbm = minimize(fun=get_robust_calibrated_winkler, x0=[0.95, 1.05], args=(y_true, oof_lower_lgbm, oof_upper_lgbm), method='L-BFGS-B', bounds=[(0.8, 1.2), (0.8, 1.2)])\n",
    "a_lgbm, b_lgbm = res_lgbm.x\n",
    "score_C = res_lgbm.fun\n",
    "\n",
    "# Create final calibrated bounds for LGBM model\n",
    "oof_lower_lgbm_raw, oof_upper_lgbm_raw = np.minimum(oof_lower_lgbm, oof_upper_lgbm), np.maximum(oof_lower_lgbm, oof_upper_lgbm)\n",
    "test_lower_lgbm_raw, test_upper_lgbm_raw = np.minimum(test_lower_lgbm, test_upper_lgbm), np.maximum(test_lower_lgbm, test_upper_lgbm)\n",
    "oof_lower_C = oof_lower_lgbm_raw * a_lgbm\n",
    "oof_upper_C = oof_upper_lgbm_raw * b_lgbm\n",
    "test_lower_C = test_lower_lgbm_raw * a_lgbm\n",
    "test_upper_C = test_upper_lgbm_raw * b_lgbm\n",
    "\n",
    "# --- Step 3: Find Optimal 3-Model Blend Weights ---\n",
    "print(\"\\n--- Optimizing the blend weight for the THREE interval models ---\")\n",
    "\n",
    "def get_3_model_blended_winkler(weights, y_true_oof, bounds_a, bounds_b, bounds_c):\n",
    "    w_a, w_b = weights[0], weights[1]\n",
    "    w_c = 1 - w_a - w_b\n",
    "    if w_c < 0: return 1e9\n",
    "    \n",
    "    final_lower = (bounds_a[0] * w_a) + (bounds_b[0] * w_b) + (bounds_c[0] * w_c)\n",
    "    final_upper = (bounds_a[1] * w_a) + (bounds_b[1] * w_b) + (bounds_c[1] * w_c)\n",
    "    return winkler_score(y_true_oof, final_lower, final_upper)\n",
    "\n",
    "result_blend_3 = minimize(\n",
    "    fun=get_3_model_blended_winkler,\n",
    "    x0=[0.8, 0.1], # Start with a guess favouring the champion\n",
    "    args=(y_true, (oof_lower_A, oof_upper_A), (oof_lower_B, oof_upper_B), (oof_lower_C, oof_upper_C)),\n",
    "    method='L-BFGS-B', bounds=[(0, 1), (0, 1)],\n",
    "    constraints={'type': 'ineq', 'fun': lambda w: 1 - np.sum(w)} # w[0]+w[1] <= 1\n",
    ")\n",
    "\n",
    "w_A, w_B = result_blend_3.x\n",
    "w_C = 1 - w_A - w_B\n",
    "best_blended_score = result_blend_3.fun\n",
    "\n",
    "# --- Step 4: Display Final Results ---\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"FINAL 3-MODEL ENSEMBLE RESULTS\")\n",
    "print(\"=\"*60)\n",
    "print(f\"  A) Mean+Error Model OOF Score:     ${score_A:,.2f}\")\n",
    "print(f\"  B) XGB Quantile Model OOF Score:   ${score_B:,.2f}\")\n",
    "print(f\"  C) LGBM Quantile Model OOF Score:  ${score_C:,.2f}\")\n",
    "print(\"-\" * 60)\n",
    "print(f\"Final BLENDED OOF Winkler Score:   ${best_blended_score:,.2f}\")\n",
    "print(\"-\" * 60)\n",
    "print(f\"Optimal Blend Weights:\")\n",
    "print(f\"  -> Model A (Mean+Error):   {w_A:.2%}\")\n",
    "print(f\"  -> Model B (XGB Quantile): {w_B:.2%}\")\n",
    "print(f\"  -> Model C (LGBM Quantile):{w_C:.2%}\")\n",
    "\n",
    "# --- Step 5: Create and Save the Final Blended Submission ---\n",
    "print(\"\\n--- Creating final blended submission file... ---\")\n",
    "\n",
    "final_test_lower = (test_lower_A * w_A) + (test_lower_B * w_B) + (test_lower_C * w_C)\n",
    "final_test_upper = (test_upper_A * w_A) + (test_upper_B * w_B) + (test_upper_C * w_C)\n",
    "final_test_upper = np.maximum(final_test_lower + 1, final_test_upper)\n",
    "\n",
    "submission_df_final = pd.DataFrame({\n",
    "    'id': df_error_model_sub['id'],\n",
    "    'pi_lower': final_test_lower,\n",
    "    'pi_upper': final_test_upper\n",
    "})\n",
    "\n",
    "submission_filename = f'submission_FINAL_3M_BLEND_{int(best_blended_score)}.csv'\n",
    "submission_df_final.to_csv(submission_filename, index=False)\n",
    "\n",
    "print(f\"\\n'{submission_filename}' created successfully! This should be your best submission yet.\")\n",
    "print(\"\\nFinal Submission Head:\")\n",
    "print(submission_df_final.head())\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "335fddf3-c89a-46ff-8cb3-bb30507f8cf2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (Kaggle Comp)",
   "language": "python",
   "name": "kaggle-comp"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
